# é™„å½• Bï¼šè°ƒè¯•æŠ€å·§

## ğŸ“– æ¦‚è¿°

æœ¬é™„å½•ä»‹ç» PyTorch å¼€å‘ä¸­å¸¸è§é—®é¢˜çš„è¯Šæ–­å’Œè°ƒè¯•æ–¹æ³•ï¼Œå¸®åŠ©ä½ å¿«é€Ÿå®šä½å’Œè§£å†³é—®é¢˜ã€‚

---

## B.1 å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

### å½¢çŠ¶ä¸åŒ¹é…

**é”™è¯¯ä¿¡æ¯**ï¼š
```
RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x64 and 128x10)
```

**è°ƒè¯•æ–¹æ³•**ï¼š

```python
def debug_shapes(model, x):
    """æ‰“å°æ¯å±‚çš„è¾“å…¥è¾“å‡ºå½¢çŠ¶"""
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    for name, layer in model.named_children():
        x = layer(x)
        print(f"{name}: {x.shape}")
    
    return x


# ä½¿ç”¨é’©å­å‡½æ•°
def register_hooks(model):
    """æ³¨å†Œé’©å­æ‰“å°ä¸­é—´å½¢çŠ¶"""
    def hook(module, input, output):
        print(f"{module.__class__.__name__}: {input[0].shape} -> {output.shape}")
    
    for layer in model.children():
        layer.register_forward_hook(hook)


# é€å±‚æ£€æŸ¥
x = torch.randn(32, 3, 224, 224)
print(f"è¾“å…¥: {x.shape}")

x = model.conv1(x)
print(f"conv1 å: {x.shape}")

x = model.pool(x)
print(f"pool å: {x.shape}")

# æ‰¾åˆ°é—®é¢˜å±‚
x = x.view(x.size(0), -1)
print(f"å±•å¹³å: {x.shape}")
print(f"fc1 æœŸæœ›è¾“å…¥: {model.fc1.in_features}")
```

### è®¾å¤‡ä¸åŒ¹é…

**é”™è¯¯ä¿¡æ¯**ï¼š
```
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
```

**è°ƒè¯•æ–¹æ³•**ï¼š

```python
def check_device(model, data):
    """æ£€æŸ¥è®¾å¤‡ä¸€è‡´æ€§"""
    # æ¨¡å‹è®¾å¤‡
    model_device = next(model.parameters()).device
    print(f"æ¨¡å‹è®¾å¤‡: {model_device}")
    
    # æ•°æ®è®¾å¤‡
    if isinstance(data, dict):
        for k, v in data.items():
            if isinstance(v, torch.Tensor):
                print(f"æ•°æ® '{k}' è®¾å¤‡: {v.device}")
    else:
        print(f"æ•°æ®è®¾å¤‡: {data.device}")
    
    # æ£€æŸ¥ä¸€è‡´æ€§
    if isinstance(data, torch.Tensor):
        assert data.device == model_device, "è®¾å¤‡ä¸åŒ¹é…ï¼"


# ç¡®ä¿ä¸€è‡´æ€§
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
inputs = inputs.to(device)
targets = targets.to(device)
```

### æ¢¯åº¦é—®é¢˜

**é”™è¯¯ä¿¡æ¯**ï¼š
```
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
```

**è°ƒè¯•æ–¹æ³•**ï¼š

```python
def check_gradients(model):
    """æ£€æŸ¥æ¢¯åº¦çŠ¶æ€"""
    for name, param in model.named_parameters():
        print(f"{name}:")
        print(f"  requires_grad: {param.requires_grad}")
        print(f"  grad: {param.grad is not None}")
        if param.grad is not None:
            print(f"  grad norm: {param.grad.norm().item():.6f}")


# æ£€æŸ¥è®¡ç®—å›¾
def check_computation_graph(tensor):
    """æ£€æŸ¥å¼ é‡çš„è®¡ç®—å›¾"""
    print(f"requires_grad: {tensor.requires_grad}")
    print(f"grad_fn: {tensor.grad_fn}")
    print(f"is_leaf: {tensor.is_leaf}")


# ç¡®ä¿æ¢¯åº¦æµåŠ¨
x = torch.randn(10, requires_grad=True)
y = model(x)
print(f"è¾“å‡º requires_grad: {y.requires_grad}")

# æ£€æŸ¥æ˜¯å¦æœ‰æ¢¯åº¦
loss = criterion(y, target)
loss.backward()

for name, param in model.named_parameters():
    if param.grad is None:
        print(f"è­¦å‘Š: {name} æ²¡æœ‰æ¢¯åº¦ï¼")
    elif param.grad.abs().sum() == 0:
        print(f"è­¦å‘Š: {name} æ¢¯åº¦ä¸ºé›¶ï¼")
```

---

## B.2 è°ƒè¯•å·¥å…·

### ä½¿ç”¨ print è°ƒè¯•

```python
class DebugModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, 3)
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        print(f"[DEBUG] è¾“å…¥: {x.shape}, device={x.device}")
        
        x = self.conv(x)
        print(f"[DEBUG] conv å: {x.shape}")
        
        x = F.adaptive_avg_pool2d(x, 1)
        print(f"[DEBUG] pool å: {x.shape}")
        
        x = x.view(x.size(0), -1)
        print(f"[DEBUG] flatten å: {x.shape}")
        
        x = self.fc(x)
        print(f"[DEBUG] fc å: {x.shape}")
        
        return x
```

### ä½¿ç”¨ PyTorch hooks

```python
class ActivationLogger:
    """è®°å½•æ¿€æ´»å€¼ç”¨äºè°ƒè¯•"""
    
    def __init__(self, model):
        self.activations = {}
        self._register_hooks(model)
    
    def _register_hooks(self, model):
        def get_hook(name):
            def hook(module, input, output):
                self.activations[name] = {
                    'input': input[0].detach() if input else None,
                    'output': output.detach() if isinstance(output, torch.Tensor) else None
                }
            return hook
        
        for name, layer in model.named_modules():
            if name:
                layer.register_forward_hook(get_hook(name))
    
    def print_stats(self):
        for name, act in self.activations.items():
            if act['output'] is not None:
                out = act['output']
                print(f"{name}:")
                print(f"  shape: {out.shape}")
                print(f"  mean: {out.mean():.4f}, std: {out.std():.4f}")
                print(f"  min: {out.min():.4f}, max: {out.max():.4f}")


# ä½¿ç”¨
logger = ActivationLogger(model)
output = model(input)
logger.print_stats()
```

### ä½¿ç”¨ torch.autograd.detect_anomaly

```python
# æ£€æµ‹æ¢¯åº¦å¼‚å¸¸
with torch.autograd.detect_anomaly():
    output = model(input)
    loss = criterion(output, target)
    loss.backward()  # å¦‚æœæœ‰ NaN/Inf ä¼šæŠ›å‡ºè¯¦ç»†é”™è¯¯
```

### ä½¿ç”¨ torch.autograd.gradcheck

```python
# æ£€æŸ¥è‡ªå®šä¹‰å‡½æ•°çš„æ¢¯åº¦æ˜¯å¦æ­£ç¡®
from torch.autograd import gradcheck

def my_function(x):
    return x ** 2 + 2 * x

x = torch.randn(10, requires_grad=True, dtype=torch.float64)
result = gradcheck(my_function, x, eps=1e-6, atol=1e-4, rtol=1e-3)
print(f"æ¢¯åº¦æ£€æŸ¥é€šè¿‡: {result}")
```

---

## B.3 å†…å­˜è°ƒè¯•

### æ˜¾å­˜ç›‘æ§

```python
def print_gpu_memory():
    """æ‰“å° GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        print(f"GPU æ˜¾å­˜: å·²åˆ†é… {allocated:.2f} GB, å·²ç¼“å­˜ {cached:.2f} GB")


def gpu_memory_tracker(func):
    """æ˜¾å­˜è¿½è¸ªè£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.synchronize()
        
        result = func(*args, **kwargs)
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            peak = torch.cuda.max_memory_allocated() / 1024**3
            print(f"å³°å€¼æ˜¾å­˜: {peak:.2f} GB")
        
        return result
    return wrapper


# ä½¿ç”¨
@gpu_memory_tracker
def train_step(model, inputs, targets):
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    loss.backward()
    return loss
```

### æ˜¾å­˜æ³„æ¼æ£€æµ‹

```python
def detect_memory_leak(model, dataloader, n_iterations=10):
    """æ£€æµ‹æ˜¾å­˜æ³„æ¼"""
    print("å¼€å§‹æ£€æµ‹æ˜¾å­˜æ³„æ¼...")
    
    memories = []
    
    for i, (inputs, targets) in enumerate(dataloader):
        if i >= n_iterations:
            break
        
        inputs = inputs.cuda()
        targets = targets.cuda()
        
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        torch.cuda.synchronize()
        memories.append(torch.cuda.memory_allocated())
        
        print(f"Iteration {i}: {memories[-1] / 1024**2:.2f} MB")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æŒç»­å¢é•¿
    if memories[-1] > memories[0] * 1.5:
        print("âš ï¸ è­¦å‘Š: å¯èƒ½å­˜åœ¨æ˜¾å­˜æ³„æ¼ï¼")
    else:
        print("âœ“ æ˜¾å­˜ä½¿ç”¨æ­£å¸¸")
```

### å¸¸è§æ˜¾å­˜æ³„æ¼åŸå› 

```python
# âŒ é”™è¯¯ï¼šåœ¨å¾ªç¯ä¸­ç´¯ç§¯å¼ é‡
losses = []
for batch in dataloader:
    loss = model(batch)
    losses.append(loss)  # ä¿ç•™äº†è®¡ç®—å›¾ï¼

# âœ“ æ­£ç¡®ï¼šä½¿ç”¨ .item() æˆ– .detach()
losses = []
for batch in dataloader:
    loss = model(batch)
    losses.append(loss.item())  # åªä¿ç•™æ•°å€¼


# âŒ é”™è¯¯ï¼šæ²¡æœ‰ torch.no_grad() è¿›è¡Œè¯„ä¼°
def evaluate(model, dataloader):
    model.eval()
    for batch in dataloader:
        output = model(batch)  # ä»ç„¶åœ¨æ„å»ºè®¡ç®—å›¾ï¼

# âœ“ æ­£ç¡®
def evaluate(model, dataloader):
    model.eval()
    with torch.no_grad():
        for batch in dataloader:
            output = model(batch)
```

---

## B.4 æ•°å€¼ç¨³å®šæ€§

### æ£€æµ‹ NaN/Inf

```python
def check_tensor_health(tensor, name="tensor"):
    """æ£€æŸ¥å¼ é‡æ˜¯å¦åŒ…å« NaN æˆ– Inf"""
    if torch.isnan(tensor).any():
        print(f"âš ï¸ {name} åŒ…å« NaNï¼")
        return False
    if torch.isinf(tensor).any():
        print(f"âš ï¸ {name} åŒ…å« Infï¼")
        return False
    return True


def check_model_health(model):
    """æ£€æŸ¥æ¨¡å‹å‚æ•°å¥åº·çŠ¶å†µ"""
    for name, param in model.named_parameters():
        if not check_tensor_health(param, f"å‚æ•° {name}"):
            return False
        if param.grad is not None:
            if not check_tensor_health(param.grad, f"æ¢¯åº¦ {name}"):
                return False
    return True


# åœ¨è®­ç»ƒä¸­ä½¿ç”¨
for epoch in range(epochs):
    for batch in dataloader:
        loss = train_step(batch)
        
        if not check_model_health(model):
            print(f"æ¨¡å‹åœ¨ epoch {epoch} å‡ºç°æ•°å€¼é—®é¢˜ï¼")
            break
```

### æ¢¯åº¦è£å‰ª

```python
# æŒ‰èŒƒæ•°è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# æŒ‰å€¼è£å‰ª
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)

# ç›‘æ§æ¢¯åº¦èŒƒæ•°
def get_gradient_norm(model):
    total_norm = 0
    for p in model.parameters():
        if p.grad is not None:
            total_norm += p.grad.data.norm(2).item() ** 2
    return total_norm ** 0.5

print(f"æ¢¯åº¦èŒƒæ•°: {get_gradient_norm(model):.4f}")
```

---

## B.5 æ€§èƒ½è°ƒè¯•

### è¯†åˆ«ç“¶é¢ˆ

```python
import time

class Timer:
    """ç®€å•è®¡æ—¶å™¨"""
    
    def __init__(self, name=""):
        self.name = name
    
    def __enter__(self):
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        self.start = time.time()
        return self
    
    def __exit__(self, *args):
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        self.elapsed = time.time() - self.start
        print(f"{self.name}: {self.elapsed*1000:.2f} ms")


# ä½¿ç”¨
with Timer("æ•°æ®åŠ è½½"):
    batch = next(iter(dataloader))

with Timer("å‰å‘ä¼ æ’­"):
    output = model(input)

with Timer("åå‘ä¼ æ’­"):
    loss.backward()

with Timer("ä¼˜åŒ–å™¨æ­¥éª¤"):
    optimizer.step()
```

### PyTorch Profiler

```python
from torch.profiler import profile, record_function, ProfilerActivity

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    with_stack=True
) as prof:
    with record_function("model_inference"):
        output = model(input)

# æ‰“å°ç»“æœ
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))

# å¯¼å‡º Chrome trace
prof.export_chrome_trace("trace.json")
```

### æ•°æ®åŠ è½½æ€§èƒ½

```python
def benchmark_dataloader(dataloader, n_batches=100):
    """æµ‹è¯• DataLoader æ€§èƒ½"""
    start = time.time()
    
    for i, batch in enumerate(dataloader):
        if i >= n_batches:
            break
    
    elapsed = time.time() - start
    print(f"åŠ è½½ {n_batches} æ‰¹æ¬¡ç”¨æ—¶: {elapsed:.2f}s")
    print(f"æ¯æ‰¹æ¬¡: {elapsed/n_batches*1000:.2f}ms")
    
    # å»ºè®®
    batch_time = elapsed / n_batches
    if batch_time > 0.1:  # è¶…è¿‡ 100ms
        print("å»ºè®®: è€ƒè™‘å¢åŠ  num_workers")


# æµ‹è¯•ä¸åŒ num_workers
for num_workers in [0, 2, 4, 8]:
    loader = DataLoader(dataset, batch_size=32, num_workers=num_workers)
    print(f"\nnum_workers={num_workers}:")
    benchmark_dataloader(loader)
```

---

## B.6 è°ƒè¯•æŠ€å·§æ±‡æ€»

### è°ƒè¯•æ¸…å•

1. **å½¢çŠ¶é—®é¢˜**
   - [ ] æ‰“å°æ¯ä¸€æ­¥çš„å¼ é‡å½¢çŠ¶
   - [ ] æ£€æŸ¥ batch ç»´åº¦æ˜¯å¦æ­£ç¡®
   - [ ] éªŒè¯å·ç§¯å±‚çš„è¾“å‡ºå°ºå¯¸

2. **è®¾å¤‡é—®é¢˜**
   - [ ] ç¡®è®¤æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
   - [ ] ç¡®è®¤æ‰€æœ‰è¾“å…¥æ•°æ®åœ¨åŒä¸€è®¾å¤‡
   - [ ] ä½¿ç”¨ `.to(device)` ç»Ÿä¸€ç®¡ç†

3. **æ¢¯åº¦é—®é¢˜**
   - [ ] æ£€æŸ¥ `requires_grad` è®¾ç½®
   - [ ] éªŒè¯æŸå¤±å‡½æ•°æ˜¯å¦å¯å¾®
   - [ ] æ£€æŸ¥æ˜¯å¦æœ‰ `detach()` åˆ‡æ–­æ¢¯åº¦

4. **å†…å­˜é—®é¢˜**
   - [ ] ä½¿ç”¨ `torch.no_grad()` è¿›è¡Œè¯„ä¼°
   - [ ] ä½¿ç”¨ `.item()` è·å–æ ‡é‡å€¼
   - [ ] å®šæœŸæ¸…ç† GPU ç¼“å­˜

5. **æ•°å€¼é—®é¢˜**
   - [ ] æ£€æŸ¥æ˜¯å¦æœ‰ NaN/Inf
   - [ ] ä½¿ç”¨æ¢¯åº¦è£å‰ª
   - [ ] é™ä½å­¦ä¹ ç‡

### å¿«é€Ÿè°ƒè¯•å‘½ä»¤

```python
# å¿«é€Ÿå½¢çŠ¶æ£€æŸ¥
print({name: p.shape for name, p in model.named_parameters()})

# å¿«é€Ÿæ¢¯åº¦æ£€æŸ¥
print({name: p.grad is not None for name, p in model.named_parameters()})

# å¿«é€Ÿè®¾å¤‡æ£€æŸ¥
print(next(model.parameters()).device)

# å¿«é€Ÿå†…å­˜æ£€æŸ¥
print(f"GPU: {torch.cuda.memory_allocated()/1e9:.2f}GB")
```

---

## B.7 å¸¸è§é—®é¢˜é€ŸæŸ¥

| ç—‡çŠ¶ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|-----|---------|---------|
| æŸå¤±ä¸º NaN | å­¦ä¹ ç‡è¿‡å¤§/æ•°å€¼æº¢å‡º | é™ä½å­¦ä¹ ç‡ï¼Œä½¿ç”¨æ¢¯åº¦è£å‰ª |
| æŸå¤±ä¸ä¸‹é™ | å­¦ä¹ ç‡è¿‡å°/æ¨¡å‹é—®é¢˜ | å¢å¤§å­¦ä¹ ç‡ï¼Œæ£€æŸ¥æ¨¡å‹ç»“æ„ |
| æ˜¾å­˜çˆ†ç‚¸ | batch size è¿‡å¤§ | å‡å° batch sizeï¼Œä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ |
| è®­ç»ƒé€Ÿåº¦æ…¢ | æ•°æ®åŠ è½½ç“¶é¢ˆ | å¢åŠ  num_workersï¼Œä½¿ç”¨ pin_memory |
| ç²¾åº¦ä¸æå‡ | è¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆ | è°ƒæ•´æ¨¡å‹å®¹é‡ï¼Œæ·»åŠ æ­£åˆ™åŒ– |

