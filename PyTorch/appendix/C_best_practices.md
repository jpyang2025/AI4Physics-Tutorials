# é™„å½• Cï¼šæœ€ä½³å®è·µ

## ğŸ“– æ¦‚è¿°

æœ¬é™„å½•æ€»ç»“äº† PyTorch å¼€å‘ä¸­çš„æœ€ä½³å®è·µï¼Œå¸®åŠ©ä½ ç¼–å†™æ›´é«˜æ•ˆã€æ›´å¯ç»´æŠ¤çš„ä»£ç ã€‚

---

## C.1 ä»£ç ç»„ç»‡

### é¡¹ç›®ç»“æ„

```
project/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ default.yaml          # é»˜è®¤é…ç½®
â”‚   â””â”€â”€ experiment.yaml       # å®éªŒé…ç½®
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataset.py            # æ•°æ®é›†å®šä¹‰
â”‚   â”œâ”€â”€ transforms.py         # æ•°æ®è½¬æ¢
â”‚   â””â”€â”€ dataloader.py         # æ•°æ®åŠ è½½å™¨
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py               # åŸºç¡€æ¨¡å‹ç±»
â”‚   â”œâ”€â”€ layers.py             # è‡ªå®šä¹‰å±‚
â”‚   â””â”€â”€ networks.py           # ç½‘ç»œæ¶æ„
â”œâ”€â”€ trainers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_trainer.py       # åŸºç¡€è®­ç»ƒå™¨
â”‚   â””â”€â”€ trainer.py            # å…·ä½“è®­ç»ƒé€»è¾‘
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py            # è¯„ä¼°æŒ‡æ ‡
â”‚   â”œâ”€â”€ visualization.py      # å¯è§†åŒ–
â”‚   â””â”€â”€ checkpoint.py         # æ£€æŸ¥ç‚¹ç®¡ç†
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py              # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ evaluate.py           # è¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ inference.py          # æ¨ç†è„šæœ¬
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb        # åˆ†æç¬”è®°æœ¬
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_model.py         # å•å…ƒæµ‹è¯•
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### æ¨¡å‹å®šä¹‰æ¨¡æ¿

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class MyModel(nn.Module):
    """æ¨¡å‹æè¿°
    
    Args:
        input_dim: è¾“å…¥ç»´åº¦
        hidden_dim: éšè—å±‚ç»´åº¦
        output_dim: è¾“å‡ºç»´åº¦
        dropout: dropout æ¯”ä¾‹
    """
    
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int = 256,
        output_dim: int = 10,
        dropout: float = 0.1
    ):
        super().__init__()
        
        # ä¿å­˜è¶…å‚æ•°
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        
        # å®šä¹‰å±‚
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥å¼ é‡ [batch_size, input_dim]
            
        Returns:
            è¾“å‡ºå¼ é‡ [batch_size, output_dim]
        """
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x
    
    def get_num_params(self) -> int:
        """è·å–å‚æ•°æ•°é‡"""
        return sum(p.numel() for p in self.parameters())
```

### è®­ç»ƒå™¨æ¨¡æ¿

```python
class Trainer:
    """è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        optimizer: torch.optim.Optimizer,
        criterion: nn.Module,
        device: torch.device,
        config: dict
    ):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
        self.config = config
        
        # çŠ¶æ€
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.history = {'train_loss': [], 'val_loss': []}
    
    def train_epoch(self) -> float:
        """è®­ç»ƒä¸€ä¸ª epoch"""
        self.model.train()
        total_loss = 0
        
        for batch in self.train_loader:
            inputs, targets = batch
            inputs = inputs.to(self.device)
            targets = targets.to(self.device)
            
            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
        
        return total_loss / len(self.train_loader)
    
    @torch.no_grad()
    def validate(self) -> float:
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0
        
        for batch in self.val_loader:
            inputs, targets = batch
            inputs = inputs.to(self.device)
            targets = targets.to(self.device)
            
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)
            total_loss += loss.item()
        
        return total_loss / len(self.val_loader)
    
    def train(self, epochs: int):
        """å®Œæ•´è®­ç»ƒ"""
        for epoch in range(epochs):
            train_loss = self.train_epoch()
            val_loss = self.validate()
            
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            
            print(f"Epoch {epoch+1}/{epochs}: "
                  f"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}")
            
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_checkpoint('best.pt')
            
            self.current_epoch += 1
    
    def save_checkpoint(self, filename: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }
        torch.save(checkpoint, filename)
```

---

## C.2 æ€§èƒ½ä¼˜åŒ–

### æ•°æ®åŠ è½½ä¼˜åŒ–

```python
# âœ“ æ¨èçš„ DataLoader é…ç½®
train_loader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,           # å¤šè¿›ç¨‹åŠ è½½
    pin_memory=True,         # å›ºå®šå†…å­˜åŠ é€Ÿä¼ è¾“
    prefetch_factor=2,       # é¢„å–æ‰¹æ¬¡æ•°
    persistent_workers=True  # ä¿æŒ worker è¿›ç¨‹
)

# å¯¹äºå°æ•°æ®é›†ï¼Œå¯ä»¥é¢„åŠ è½½åˆ° GPU
class PreloadedDataset:
    def __init__(self, dataset, device):
        self.data = []
        for x, y in dataset:
            self.data.append((x.to(device), y.to(device)))
    
    def __getitem__(self, idx):
        return self.data[idx]
    
    def __len__(self):
        return len(self.data)
```

### GPU ä¼˜åŒ–

```python
# å¯ç”¨ cuDNN benchmarkï¼ˆè¾“å…¥å°ºå¯¸å›ºå®šæ—¶ï¼‰
torch.backends.cudnn.benchmark = True

# ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for inputs, targets in train_loader:
    optimizer.zero_grad()
    
    with autocast():
        outputs = model(inputs)
        loss = criterion(outputs, targets)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()


# ä½¿ç”¨ torch.compileï¼ˆPyTorch 2.0+ï¼‰
model = torch.compile(model)
```

### å†…å­˜ä¼˜åŒ–

```python
# æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿå¤§ batch sizeï¼‰
accumulation_steps = 4

for i, (inputs, targets) in enumerate(train_loader):
    outputs = model(inputs)
    loss = criterion(outputs, targets) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()


# æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆç”¨æ—¶é—´æ¢å†…å­˜ï¼‰
from torch.utils.checkpoint import checkpoint

class MemoryEfficientModel(nn.Module):
    def forward(self, x):
        x = checkpoint(self.layer1, x)
        x = checkpoint(self.layer2, x)
        return self.output(x)


# åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¼ é‡
del intermediate_tensor
torch.cuda.empty_cache()
```

---

## C.3 å¯å¤ç°æ€§

### è®¾ç½®éšæœºç§å­

```python
import random
import numpy as np
import torch


def set_seed(seed: int = 42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # ç¡®å®šæ€§ç®—æ³•ï¼ˆå¯èƒ½é™ä½æ€§èƒ½ï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# åœ¨è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨
set_seed(42)
```

### é…ç½®ç®¡ç†

```python
from dataclasses import dataclass
from typing import Optional
import yaml


@dataclass
class TrainConfig:
    # æ•°æ®
    data_path: str = "./data"
    batch_size: int = 32
    num_workers: int = 4
    
    # æ¨¡å‹
    model_name: str = "resnet18"
    hidden_dim: int = 256
    dropout: float = 0.1
    
    # è®­ç»ƒ
    epochs: int = 100
    lr: float = 1e-3
    weight_decay: float = 1e-4
    
    # å…¶ä»–
    seed: int = 42
    device: str = "cuda"
    save_dir: str = "./checkpoints"
    
    @classmethod
    def from_yaml(cls, path: str):
        with open(path, 'r') as f:
            config_dict = yaml.safe_load(f)
        return cls(**config_dict)
    
    def to_yaml(self, path: str):
        with open(path, 'w') as f:
            yaml.dump(self.__dict__, f)


# ä½¿ç”¨
config = TrainConfig(lr=1e-4, epochs=50)
config.to_yaml('config.yaml')
```

---

## C.4 å®éªŒç®¡ç†

### æ—¥å¿—è®°å½•

```python
import logging
from datetime import datetime


def setup_logging(log_dir: str):
    """è®¾ç½®æ—¥å¿—"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = f"{log_dir}/train_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


logger = setup_logging("./logs")
logger.info(f"å¼€å§‹è®­ç»ƒï¼Œé…ç½®: {config}")
```

### TensorBoard é›†æˆ

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/experiment_1')

# è®°å½•æ ‡é‡
writer.add_scalar('Loss/train', train_loss, epoch)
writer.add_scalar('Loss/val', val_loss, epoch)
writer.add_scalar('Accuracy/val', accuracy, epoch)

# è®°å½•è¶…å‚æ•°
writer.add_hparams(
    {'lr': config.lr, 'batch_size': config.batch_size},
    {'final_accuracy': best_accuracy}
)

# è®°å½•æ¨¡å‹ç»“æ„
writer.add_graph(model, sample_input)

# è®°å½•å›¾åƒ
writer.add_images('predictions', images, epoch)

# è®°å½•ç›´æ–¹å›¾
for name, param in model.named_parameters():
    writer.add_histogram(name, param, epoch)

writer.close()
```

### å®éªŒè·Ÿè¸ªæ¡†æ¶

```python
# ä½¿ç”¨ Weights & Biases
import wandb

wandb.init(
    project="my-project",
    config={
        "learning_rate": 1e-3,
        "epochs": 100,
        "batch_size": 32
    }
)

for epoch in range(epochs):
    train_loss = train_epoch()
    val_loss = validate()
    
    wandb.log({
        "train_loss": train_loss,
        "val_loss": val_loss,
        "epoch": epoch
    })

wandb.finish()
```

---

## C.5 æ¨¡å‹éªŒè¯

### å•å…ƒæµ‹è¯•

```python
import unittest
import torch


class TestModel(unittest.TestCase):
    
    def setUp(self):
        self.model = MyModel(input_dim=10, output_dim=5)
        self.model.eval()
    
    def test_forward_shape(self):
        """æµ‹è¯•è¾“å‡ºå½¢çŠ¶"""
        x = torch.randn(32, 10)
        y = self.model(x)
        self.assertEqual(y.shape, (32, 5))
    
    def test_forward_batch_sizes(self):
        """æµ‹è¯•ä¸åŒ batch size"""
        for batch_size in [1, 16, 64]:
            x = torch.randn(batch_size, 10)
            y = self.model(x)
            self.assertEqual(y.shape[0], batch_size)
    
    def test_gradient_flow(self):
        """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
        self.model.train()
        x = torch.randn(4, 10, requires_grad=True)
        y = self.model(x)
        loss = y.sum()
        loss.backward()
        
        # æ£€æŸ¥æ‰€æœ‰å‚æ•°éƒ½æœ‰æ¢¯åº¦
        for name, param in self.model.named_parameters():
            self.assertIsNotNone(param.grad, f"{name} æ²¡æœ‰æ¢¯åº¦")
    
    def test_save_load(self):
        """æµ‹è¯•ä¿å­˜å’ŒåŠ è½½"""
        x = torch.randn(4, 10)
        y1 = self.model(x)
        
        # ä¿å­˜å’ŒåŠ è½½
        torch.save(self.model.state_dict(), 'test_model.pt')
        new_model = MyModel(input_dim=10, output_dim=5)
        new_model.load_state_dict(torch.load('test_model.pt'))
        new_model.eval()
        
        y2 = new_model(x)
        self.assertTrue(torch.allclose(y1, y2))


if __name__ == '__main__':
    unittest.main()
```

### è¿‡æ‹Ÿåˆæµ‹è¯•

```python
def overfit_single_batch(model, batch, epochs=100):
    """æµ‹è¯•æ¨¡å‹èƒ½å¦è¿‡æ‹Ÿåˆå•ä¸ªæ‰¹æ¬¡"""
    inputs, targets = batch
    inputs = inputs.to(device)
    targets = targets.to(device)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()
    
    model.train()
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        
        if epoch % 20 == 0:
            acc = (outputs.argmax(1) == targets).float().mean()
            print(f"Epoch {epoch}: loss={loss.item():.4f}, acc={acc.item():.4f}")
    
    # æœ€ç»ˆåº”è¯¥æ¥è¿‘ 100% å‡†ç¡®ç‡
    final_acc = (model(inputs).argmax(1) == targets).float().mean()
    print(f"æœ€ç»ˆå‡†ç¡®ç‡: {final_acc.item():.4f}")
    
    if final_acc > 0.99:
        print("âœ“ æ¨¡å‹å¯ä»¥æ­£ç¡®å­¦ä¹ ")
    else:
        print("âš ï¸ æ¨¡å‹å¯èƒ½æœ‰é—®é¢˜")
```

---

## C.6 éƒ¨ç½²å‡†å¤‡

### æ¨¡å‹å¯¼å‡º

```python
# TorchScript å¯¼å‡º
model.eval()
scripted_model = torch.jit.script(model)
scripted_model.save('model_scripted.pt')

# æˆ–ä½¿ç”¨ trace
traced_model = torch.jit.trace(model, example_input)
traced_model.save('model_traced.pt')


# ONNX å¯¼å‡º
torch.onnx.export(
    model,
    example_input,
    'model.onnx',
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={
        'input': {0: 'batch_size'},
        'output': {0: 'batch_size'}
    },
    opset_version=11
)
```

### æ¨ç†ä¼˜åŒ–

```python
# ä½¿ç”¨ torch.inference_mode
@torch.inference_mode()
def predict(model, inputs):
    return model(inputs)


# æ‰¹é‡æ¨ç†
def batch_predict(model, data, batch_size=32):
    model.eval()
    results = []
    
    with torch.inference_mode():
        for i in range(0, len(data), batch_size):
            batch = data[i:i+batch_size]
            output = model(batch)
            results.append(output)
    
    return torch.cat(results)


# ä½¿ç”¨ torch.compile
compiled_model = torch.compile(model, mode='reduce-overhead')
```

---

## C.7 æ£€æŸ¥æ¸…å•

### è®­ç»ƒå‰

- [ ] è®¾ç½®éšæœºç§å­
- [ ] éªŒè¯æ•°æ®åŠ è½½æ­£ç¡®
- [ ] æµ‹è¯•æ¨¡å‹å¯ä»¥è¿‡æ‹Ÿåˆå°æ•°æ®
- [ ] æ£€æŸ¥æŸå¤±å‡½æ•°è®¡ç®—æ­£ç¡®
- [ ] éªŒè¯æ¢¯åº¦æ­£å¸¸æµåŠ¨

### è®­ç»ƒä¸­

- [ ] ç›‘æ§æŸå¤±å’ŒæŒ‡æ ‡
- [ ] å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
- [ ] æ£€æŸ¥æ¢¯åº¦èŒƒæ•°
- [ ] ç›‘æ§ GPU ä½¿ç”¨ç‡

### è®­ç»ƒå

- [ ] è¯„ä¼°æµ‹è¯•é›†æ€§èƒ½
- [ ] å¯è§†åŒ–ç»“æœ
- [ ] ä¿å­˜æœ€ä½³æ¨¡å‹
- [ ] è®°å½•å®éªŒé…ç½®

### éƒ¨ç½²å‰

- [ ] å¯¼å‡ºæ¨¡å‹ï¼ˆTorchScript/ONNXï¼‰
- [ ] éªŒè¯å¯¼å‡ºæ¨¡å‹æ­£ç¡®æ€§
- [ ] æµ‹è¯•æ¨ç†æ€§èƒ½
- [ ] ç¼–å†™æ¨ç†æ–‡æ¡£

