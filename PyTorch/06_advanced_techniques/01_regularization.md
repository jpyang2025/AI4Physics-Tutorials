# 6.1 æ­£åˆ™åŒ–æŠ€æœ¯

## ğŸ“– æ¦‚è¿°

æ­£åˆ™åŒ–æ˜¯é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆçš„æ ¸å¿ƒæŠ€æœ¯ã€‚ä»ç‰©ç†è§’åº¦çœ‹ï¼Œæ­£åˆ™åŒ–ç›¸å½“äºåœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ "çº¦æŸåŠ¿èƒ½"ï¼Œé™åˆ¶æ¨¡å‹å‚æ•°çš„è‡ªç”±åº¦ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£è¿‡æ‹Ÿåˆçš„æœ¬è´¨
- æŒæ¡ L1/L2 æ­£åˆ™åŒ–
- ä½¿ç”¨ Dropout å’Œ Batch Normalization
- äº†è§£æ•°æ®å¢å¼ºä½œä¸ºæ­£åˆ™åŒ–

---

## 6.1.1 è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ

### ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Ÿ

```python
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt

def demonstrate_overfitting():
    """æ¼”ç¤ºè¿‡æ‹Ÿåˆç°è±¡"""
    # çœŸå®å‡½æ•°
    def true_function(x):
        return np.sin(2 * np.pi * x)
    
    # ç”Ÿæˆå¸¦å™ªå£°çš„è®­ç»ƒæ•°æ®
    np.random.seed(42)
    x_train = np.random.uniform(0, 1, 20)
    y_train = true_function(x_train) + 0.3 * np.random.randn(20)
    
    # è½¬æ¢ä¸ºå¼ é‡
    X = torch.from_numpy(x_train).float().reshape(-1, 1)
    Y = torch.from_numpy(y_train).float().reshape(-1, 1)
    
    # åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾
    def polynomial_features(x, degree):
        return torch.cat([x**i for i in range(degree + 1)], dim=1)
    
    # ä¸åŒå¤æ‚åº¦çš„æ¨¡å‹
    degrees = [1, 4, 15]  # æ¬ æ‹Ÿåˆã€é€‚å½“ã€è¿‡æ‹Ÿåˆ
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    x_test = torch.linspace(0, 1, 100).reshape(-1, 1)
    
    for ax, degree in zip(axes, degrees):
        # å¤šé¡¹å¼ç‰¹å¾
        X_poly = polynomial_features(X, degree)
        X_test_poly = polynomial_features(x_test, degree)
        
        # çº¿æ€§å›å½’
        model = nn.Linear(degree + 1, 1, bias=False)
        optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
        
        for _ in range(1000):
            optimizer.zero_grad()
            loss = nn.MSELoss()(model(X_poly), Y)
            loss.backward()
            optimizer.step()
        
        # é¢„æµ‹
        with torch.no_grad():
            y_pred = model(X_test_poly)
        
        ax.scatter(x_train, y_train, c='blue', label='è®­ç»ƒæ•°æ®')
        ax.plot(x_test.numpy(), true_function(x_test.numpy()), 
                'g--', label='çœŸå®å‡½æ•°')
        ax.plot(x_test.numpy(), y_pred.numpy(), 'r-', label='æ‹Ÿåˆç»“æœ')
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_title(f'å¤šé¡¹å¼é˜¶æ•° = {degree}')
        ax.legend()
        ax.set_ylim(-2, 2)
    
    axes[0].set_title('æ¬ æ‹Ÿåˆ (degree=1)')
    axes[1].set_title('é€‚å½“æ‹Ÿåˆ (degree=4)')
    axes[2].set_title('è¿‡æ‹Ÿåˆ (degree=15)')
    
    plt.tight_layout()
    plt.show()
```

### ç‰©ç†ç›´è§‰ï¼šè‡ªç”±åº¦ä¸çº¦æŸ

ä»ç»Ÿè®¡åŠ›å­¦è§’åº¦ï¼š

- **æ¨¡å‹å‚æ•°æ•°**ï¼šç³»ç»Ÿè‡ªç”±åº¦ $N_f$
- **è®­ç»ƒæ ·æœ¬æ•°**ï¼šå¯¹ç³»ç»Ÿçš„çº¦æŸ $N_c$
- **æœ‰æ•ˆè‡ªç”±åº¦**ï¼š$N_{\text{eff}} = N_f - N_c$

å½“ $N_{\text{eff}} \gg 0$ æ—¶ï¼Œç³»ç»Ÿæœ‰å¤ªå¤šæœªè¢«çº¦æŸçš„è‡ªç”±åº¦ï¼Œå¯¼è‡´è¿‡æ‹Ÿåˆã€‚

---

## 6.1.2 L2 æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰

### ç‰©ç†ç±»æ¯”ï¼šè°æŒ¯å­åŠ¿èƒ½

L2 æ­£åˆ™åŒ–åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒé‡çš„å¹³æ–¹å’Œï¼š

$$L_{\text{reg}} = L + \frac{\lambda}{2}\|\mathbf{w}\|_2^2$$

è¿™ç›¸å½“äºç»™æ¯ä¸ªæƒé‡æ·»åŠ ä¸€ä¸ª**è°æŒ¯å­åŠ¿èƒ½**ï¼Œå€¾å‘äºä½¿æƒé‡ä¿æŒåœ¨åŸç‚¹é™„è¿‘ã€‚

### PyTorch å®ç°

```python
# æ–¹æ³•1ï¼šåœ¨ä¼˜åŒ–å™¨ä¸­è®¾ç½® weight_decay
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=0.001,
    weight_decay=1e-4  # L2 æ­£åˆ™åŒ–ç³»æ•°
)

# æ–¹æ³•2ï¼šæ‰‹åŠ¨æ·»åŠ æ­£åˆ™åŒ–é¡¹
def train_with_l2_regularization(model, train_loader, criterion, 
                                  optimizer, l2_lambda=1e-4):
    """å¸¦ L2 æ­£åˆ™åŒ–çš„è®­ç»ƒ"""
    model.train()
    
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        
        # æ·»åŠ  L2 æ­£åˆ™åŒ–é¡¹
        l2_reg = torch.tensor(0.)
        for param in model.parameters():
            l2_reg += torch.norm(param, 2)**2
        
        loss = loss + l2_lambda * l2_reg
        
        loss.backward()
        optimizer.step()
```

### L2 æ­£åˆ™åŒ–çš„æ•ˆæœ

```python
def compare_l2_regularization():
    """æ¯”è¾ƒä¸åŒ L2 æ­£åˆ™åŒ–å¼ºåº¦çš„æ•ˆæœ"""
    
    # ç”Ÿæˆæ•°æ®
    torch.manual_seed(42)
    X = torch.randn(100, 20)  # 100 ä¸ªæ ·æœ¬ï¼Œ20 ä¸ªç‰¹å¾
    w_true = torch.zeros(20, 1)
    w_true[:5] = torch.randn(5, 1)  # åªæœ‰å‰5ä¸ªç‰¹å¾æœ‰ç”¨
    Y = X @ w_true + 0.1 * torch.randn(100, 1)
    
    l2_lambdas = [0, 1e-3, 1e-2, 1e-1, 1.0]
    
    fig, axes = plt.subplots(1, len(l2_lambdas), figsize=(15, 3))
    
    for ax, l2_lambda in zip(axes, l2_lambdas):
        model = nn.Linear(20, 1, bias=False)
        
        # ä½¿ç”¨ weight_decay å®ç° L2 æ­£åˆ™åŒ–
        optimizer = torch.optim.SGD(model.parameters(), lr=0.1, 
                                    weight_decay=l2_lambda)
        
        for _ in range(1000):
            optimizer.zero_grad()
            loss = nn.MSELoss()(model(X), Y)
            loss.backward()
            optimizer.step()
        
        # å¯è§†åŒ–æƒé‡
        weights = model.weight.data.numpy().flatten()
        ax.bar(range(20), weights)
        ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)
        ax.set_xlabel('ç‰¹å¾ç´¢å¼•')
        ax.set_ylabel('æƒé‡å€¼')
        ax.set_title(f'Î» = {l2_lambda}')
        ax.set_ylim(-1.5, 1.5)
    
    plt.suptitle('L2 æ­£åˆ™åŒ–å¯¹æƒé‡çš„å½±å“')
    plt.tight_layout()
    plt.show()
```

---

## 6.1.3 L1 æ­£åˆ™åŒ–

### ç‰©ç†ç±»æ¯”ï¼šå„å‘å¼‚æ€§åŠ¿èƒ½

L1 æ­£åˆ™åŒ–æ·»åŠ æƒé‡çš„ç»å¯¹å€¼å’Œï¼š

$$L_{\text{reg}} = L + \lambda\|\mathbf{w}\|_1$$

è¿™å€¾å‘äºäº§ç”Ÿ**ç¨€ç–è§£**ï¼ˆè®¸å¤šæƒé‡ä¸ºé›¶ï¼‰ï¼Œç±»ä¼¼äºå„å‘å¼‚æ€§æ™¶ä½“åŠ¿èƒ½ã€‚

### PyTorch å®ç°

```python
def train_with_l1_regularization(model, train_loader, criterion, 
                                  optimizer, l1_lambda=1e-4):
    """å¸¦ L1 æ­£åˆ™åŒ–çš„è®­ç»ƒ"""
    model.train()
    
    for inputs, targets in train_loader:
        optimizer.zero_grad()
        
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        
        # æ·»åŠ  L1 æ­£åˆ™åŒ–é¡¹
        l1_reg = torch.tensor(0.)
        for param in model.parameters():
            l1_reg += torch.norm(param, 1)
        
        loss = loss + l1_lambda * l1_reg
        
        loss.backward()
        optimizer.step()
```

### L1 vs L2ï¼šç¨€ç–æ€§æ¯”è¾ƒ

```python
def compare_l1_l2():
    """æ¯”è¾ƒ L1 å’Œ L2 æ­£åˆ™åŒ–"""
    
    torch.manual_seed(42)
    X = torch.randn(100, 50)
    w_true = torch.zeros(50, 1)
    w_true[:10] = torch.randn(10, 1)  # åªæœ‰å‰10ä¸ªç‰¹å¾æœ‰ç”¨
    Y = X @ w_true + 0.1 * torch.randn(100, 1)
    
    results = {}
    
    # æ— æ­£åˆ™åŒ–
    model_none = nn.Linear(50, 1, bias=False)
    opt_none = torch.optim.Adam(model_none.parameters(), lr=0.01)
    for _ in range(1000):
        opt_none.zero_grad()
        loss = nn.MSELoss()(model_none(X), Y)
        loss.backward()
        opt_none.step()
    results['æ— æ­£åˆ™åŒ–'] = model_none.weight.data.numpy().flatten()
    
    # L2 æ­£åˆ™åŒ–
    model_l2 = nn.Linear(50, 1, bias=False)
    opt_l2 = torch.optim.Adam(model_l2.parameters(), lr=0.01, weight_decay=0.1)
    for _ in range(1000):
        opt_l2.zero_grad()
        loss = nn.MSELoss()(model_l2(X), Y)
        loss.backward()
        opt_l2.step()
    results['L2'] = model_l2.weight.data.numpy().flatten()
    
    # L1 æ­£åˆ™åŒ–
    model_l1 = nn.Linear(50, 1, bias=False)
    opt_l1 = torch.optim.Adam(model_l1.parameters(), lr=0.01)
    for _ in range(1000):
        opt_l1.zero_grad()
        loss = nn.MSELoss()(model_l1(X), Y)
        l1_reg = 0.1 * sum(p.abs().sum() for p in model_l1.parameters())
        (loss + l1_reg).backward()
        opt_l1.step()
    results['L1'] = model_l1.weight.data.numpy().flatten()
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    for ax, (name, weights) in zip(axes, results.items()):
        ax.bar(range(50), weights, color='steelblue')
        ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)
        ax.set_xlabel('ç‰¹å¾ç´¢å¼•')
        ax.set_ylabel('æƒé‡å€¼')
        ax.set_title(f'{name}\néé›¶æƒé‡æ•°: {np.sum(np.abs(weights) > 0.01)}')
    
    plt.tight_layout()
    plt.show()
```

---

## 6.1.4 Dropout

### ç‰©ç†ç±»æ¯”ï¼šéšæœºç¨€é‡Š

Dropout åœ¨è®­ç»ƒæ—¶éšæœº"å…³é—­"ä¸€éƒ¨åˆ†ç¥ç»å…ƒï¼Œç±»ä¼¼äº**æ ¼ç‚¹æ¨¡å‹ä¸­çš„éšæœºç¨€é‡Š**ã€‚

```python
import torch.nn.functional as F

class NetworkWithDropout(nn.Module):
    """å¸¦ Dropout çš„ç½‘ç»œ"""
    
    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.5):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout_rate)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)  # è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒ
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x


# Dropout çš„å·¥ä½œåŸç†
def dropout_demonstration():
    """æ¼”ç¤º Dropout çš„å·¥ä½œåŸç†"""
    
    dropout = nn.Dropout(p=0.5)
    
    x = torch.ones(1, 10)
    
    # è®­ç»ƒæ¨¡å¼
    dropout.train()
    print("è®­ç»ƒæ¨¡å¼ (éšæœºä¸¢å¼ƒ):")
    for i in range(3):
        out = dropout(x)
        print(f"  å°è¯• {i+1}: {out}")
    
    # è¯„ä¼°æ¨¡å¼
    dropout.eval()
    print("\nè¯„ä¼°æ¨¡å¼ (æ— ä¸¢å¼ƒ):")
    out = dropout(x)
    print(f"  è¾“å‡º: {out}")
```

### Dropout å˜ä½“

```python
# æ ‡å‡† Dropout
dropout = nn.Dropout(p=0.5)

# 2D Dropoutï¼ˆç”¨äº CNNï¼‰
dropout2d = nn.Dropout2d(p=0.5)  # ä¸¢å¼ƒæ•´ä¸ªé€šé“

# Alpha Dropoutï¼ˆç”¨äº SELU æ¿€æ´»ï¼‰
alpha_dropout = nn.AlphaDropout(p=0.5)

# ä½¿ç”¨ç¤ºä¾‹
class CNNWithDropout(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.dropout2d = nn.Dropout2d(0.25)  # å·ç§¯å±‚åä½¿ç”¨ 2D dropout
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.dropout = nn.Dropout(0.5)       # å…¨è¿æ¥å±‚åä½¿ç”¨æ ‡å‡† dropout
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2)
        x = self.dropout2d(x)
        
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2)
        x = self.dropout2d(x)
        
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x
```

### Dropout çš„ç‰©ç†æ„ä¹‰

ä»ç»Ÿè®¡åŠ›å­¦è§’åº¦ï¼ŒDropout å¯ä»¥ç†è§£ä¸ºï¼š

1. **ç³»ç»¼å¹³å‡**ï¼šæ¯æ¬¡å‰å‘ä¼ æ’­ä½¿ç”¨ä¸åŒçš„å­ç½‘ç»œï¼Œæœ€ç»ˆç»“æœæ˜¯å¯¹æ‰€æœ‰å¯èƒ½å­ç½‘ç»œçš„å¹³å‡
2. **å™ªå£°æ³¨å…¥**ï¼šç±»ä¼¼äºæœ‰é™æ¸©åº¦ä¸‹çš„çƒ­æ¶¨è½
3. **æ¨¡å‹é›†æˆ**ï¼šéšå¼åœ°è®­ç»ƒäº† $2^N$ ä¸ªå­æ¨¡å‹ï¼ˆN æ˜¯ç¥ç»å…ƒæ•°ï¼‰

---

## 6.1.5 Batch Normalization

### ç‰©ç†ç±»æ¯”ï¼šé‡æ•´åŒ–

Batch Normalization å°†æ¯å±‚çš„æ¿€æ´»å€¼æ ‡å‡†åŒ–ï¼Œç±»ä¼¼äº**é‡æ•´åŒ–ç¾¤å˜æ¢**ã€‚

```python
class NetworkWithBatchNorm(nn.Module):
    """å¸¦ Batch Normalization çš„ç½‘ç»œ"""
    
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.bn1 = nn.BatchNorm1d(hidden_dim)  # æ‰¹å½’ä¸€åŒ–
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)      # å½’ä¸€åŒ–
        x = F.relu(x)
        
        x = self.fc2(x)
        x = self.bn2(x)
        x = F.relu(x)
        
        x = self.fc3(x)
        return x


# BatchNorm çš„å·¥ä½œåŸç†
def batchnorm_demonstration():
    """æ¼”ç¤º BatchNorm çš„å·¥ä½œåŸç†"""
    
    bn = nn.BatchNorm1d(5)
    
    # è®­ç»ƒæ¨¡å¼
    bn.train()
    x = torch.randn(32, 5) * 10 + 5  # å‡å€¼çº¦5ï¼Œæ ‡å‡†å·®çº¦10
    
    print(f"è¾“å…¥ç»Ÿè®¡: å‡å€¼={x.mean().item():.2f}, æ ‡å‡†å·®={x.std().item():.2f}")
    
    y = bn(x)
    print(f"è¾“å‡ºç»Ÿè®¡: å‡å€¼={y.mean().item():.2f}, æ ‡å‡†å·®={y.std().item():.2f}")
    
    # è¯„ä¼°æ¨¡å¼ä½¿ç”¨è¿è¡Œæ—¶ç»Ÿè®¡é‡
    bn.eval()
    x_test = torch.randn(8, 5) * 10 + 5
    y_test = bn(x_test)
    print(f"\nè¯„ä¼°æ¨¡å¼è¾“å‡º: å‡å€¼={y_test.mean().item():.2f}, æ ‡å‡†å·®={y_test.std().item():.2f}")
```

### å½’ä¸€åŒ–å˜ä½“

```python
# ä¸åŒçš„å½’ä¸€åŒ–æ–¹æ³•
batch_norm = nn.BatchNorm1d(num_features)   # å¯¹ batch ç»´åº¦å½’ä¸€åŒ–
layer_norm = nn.LayerNorm(normalized_shape)  # å¯¹ç‰¹å¾ç»´åº¦å½’ä¸€åŒ–
instance_norm = nn.InstanceNorm2d(num_features)  # å¯¹æ¯ä¸ªæ ·æœ¬çš„ç©ºé—´ç»´åº¦å½’ä¸€åŒ–
group_norm = nn.GroupNorm(num_groups, num_channels)  # å¯¹é€šé“åˆ†ç»„å½’ä¸€åŒ–

# ä½¿ç”¨åœºæ™¯
# - BatchNorm: CNNã€å¤§æ‰¹é‡è®­ç»ƒ
# - LayerNorm: Transformerã€RNNã€å°æ‰¹é‡
# - InstanceNorm: é£æ ¼è¿ç§»
# - GroupNorm: å°æ‰¹é‡ CNN
```

### Layer Normalizationï¼ˆé€‚åˆ Transformerï¼‰

```python
class TransformerBlock(nn.Module):
    """Transformer å—ä½¿ç”¨ LayerNorm"""
    
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # è‡ªæ³¨æ„åŠ› + æ®‹å·® + LayerNorm
        attn_out, _ = self.attention(x, x, x)
        x = self.norm1(x + self.dropout(attn_out))
        
        # å‰é¦ˆ + æ®‹å·® + LayerNorm
        ff_out = self.ff(x)
        x = self.norm2(x + self.dropout(ff_out))
        
        return x
```

---

## 6.1.6 æ•°æ®å¢å¼º

### ç‰©ç†ç±»æ¯”ï¼šå¯¹ç§°æ€§çº¦æŸ

æ•°æ®å¢å¼ºåˆ©ç”¨é—®é¢˜çš„**å¯¹ç§°æ€§**æ¥æ‰©å……æ•°æ®ï¼Œç±»ä¼¼äºç‰©ç†ä¸­åˆ©ç”¨å¯¹ç§°æ€§å‡å°‘é—®é¢˜å¤æ‚åº¦ã€‚

```python
import torchvision.transforms as T

# å›¾åƒæ•°æ®å¢å¼º
train_transform = T.Compose([
    T.RandomHorizontalFlip(p=0.5),
    T.RandomRotation(15),
    T.RandomCrop(32, padding=4),
    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# å¼ºæ•°æ®å¢å¼ºï¼ˆå¦‚ RandAugment é£æ ¼ï¼‰
strong_transform = T.Compose([
    T.RandomResizedCrop(224, scale=(0.08, 1.0)),
    T.RandomHorizontalFlip(),
    T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
```

### ç‰©ç†æ•°æ®å¢å¼º

```python
class PhysicsDataAugmentation:
    """ç‰©ç†æ•°æ®çš„å¢å¼ºç­–ç•¥"""
    
    def __init__(self):
        pass
    
    def add_gaussian_noise(self, x, sigma=0.1):
        """æ·»åŠ é«˜æ–¯å™ªå£° - æ¨¡æ‹Ÿæµ‹é‡è¯¯å·®"""
        return x + sigma * torch.randn_like(x)
    
    def scale_invariance(self, x, scale_range=(0.8, 1.2)):
        """ç¼©æ”¾ä¸å˜æ€§ - ç‰©ç†é‡çš„é‡çº²å˜æ¢"""
        scale = torch.empty(1).uniform_(*scale_range).item()
        return x * scale
    
    def time_reversal(self, trajectory):
        """æ—¶é—´åæ¼” - å¯¹äºå¯é€†ç³»ç»Ÿ"""
        return torch.flip(trajectory, dims=[0])
    
    def rotation(self, coords, angle=None):
        """
        æ—‹è½¬å˜æ¢ - åˆ©ç”¨æ—‹è½¬å¯¹ç§°æ€§
        coords: [N, 2] æˆ– [N, 3]
        """
        if angle is None:
            angle = torch.empty(1).uniform_(0, 2 * np.pi).item()
        
        if coords.shape[1] == 2:
            # 2D æ—‹è½¬
            cos_a, sin_a = np.cos(angle), np.sin(angle)
            R = torch.tensor([[cos_a, -sin_a], [sin_a, cos_a]])
            return coords @ R.T
        else:
            # 3D æ—‹è½¬ï¼ˆç»• z è½´ï¼‰
            cos_a, sin_a = np.cos(angle), np.sin(angle)
            R = torch.tensor([
                [cos_a, -sin_a, 0],
                [sin_a, cos_a, 0],
                [0, 0, 1]
            ])
            return coords @ R.T
    
    def translation(self, coords, max_shift=0.1):
        """å¹³ç§»å˜æ¢ - åˆ©ç”¨å¹³ç§»å¯¹ç§°æ€§"""
        shift = torch.empty(coords.shape[1]).uniform_(-max_shift, max_shift)
        return coords + shift


# ä½¿ç”¨ç¤ºä¾‹
augmenter = PhysicsDataAugmentation()

# ç²’å­åæ ‡å¢å¼º
coords = torch.randn(10, 3)  # 10ä¸ªç²’å­çš„3Dåæ ‡
coords_aug = augmenter.rotation(coords)
coords_aug = augmenter.add_gaussian_noise(coords_aug, sigma=0.05)
```

---

## 6.1.7 å…¶ä»–æ­£åˆ™åŒ–æŠ€æœ¯

### æ—©åœï¼ˆEarly Stoppingï¼‰

```python
class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    
    def __init__(self, patience=10, min_delta=0, restore_best=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best = restore_best
        self.counter = 0
        self.best_loss = float('inf')
        self.best_weights = None
        self.should_stop = False
    
    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            if self.restore_best:
                self.best_weights = {k: v.cpu().clone() 
                                     for k, v in model.state_dict().items()}
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.should_stop = True
                if self.restore_best and self.best_weights:
                    model.load_state_dict(self.best_weights)
        
        return self.should_stop
```

### æ ‡ç­¾å¹³æ»‘ï¼ˆLabel Smoothingï¼‰

```python
class LabelSmoothingCrossEntropy(nn.Module):
    """æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µ"""
    
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
    
    def forward(self, pred, target):
        n_classes = pred.size(-1)
        
        # åˆ›å»ºå¹³æ»‘æ ‡ç­¾
        with torch.no_grad():
            smooth_target = torch.zeros_like(pred)
            smooth_target.fill_(self.smoothing / (n_classes - 1))
            smooth_target.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)
        
        # è®¡ç®—äº¤å‰ç†µ
        log_probs = F.log_softmax(pred, dim=-1)
        loss = (-smooth_target * log_probs).sum(dim=-1).mean()
        
        return loss


# ä½¿ç”¨
criterion = LabelSmoothingCrossEntropy(smoothing=0.1)
```

### Mixup æ•°æ®å¢å¼º

```python
def mixup_data(x, y, alpha=0.2):
    """
    Mixup æ•°æ®å¢å¼º
    
    å°†ä¸¤ä¸ªæ ·æœ¬çº¿æ€§æ··åˆ
    """
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    
    batch_size = x.size(0)
    index = torch.randperm(batch_size)
    
    mixed_x = lam * x + (1 - lam) * x[index]
    y_a, y_b = y, y[index]
    
    return mixed_x, y_a, y_b, lam


def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """Mixup æŸå¤±å‡½æ•°"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


# è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
for inputs, targets in train_loader:
    inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha=0.2)
    
    outputs = model(inputs)
    loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

---

## 6.1.8 æ­£åˆ™åŒ–å¼ºåº¦é€‰æ‹©

### è¶…å‚æ•°æœç´¢

```python
from sklearn.model_selection import ParameterGrid

def regularization_search(model_fn, train_loader, val_loader, device):
    """ç½‘æ ¼æœç´¢æ­£åˆ™åŒ–è¶…å‚æ•°"""
    
    param_grid = {
        'weight_decay': [0, 1e-5, 1e-4, 1e-3, 1e-2],
        'dropout': [0, 0.1, 0.2, 0.3, 0.5]
    }
    
    best_val_loss = float('inf')
    best_params = None
    
    for params in ParameterGrid(param_grid):
        print(f"æµ‹è¯•å‚æ•°: {params}")
        
        # åˆ›å»ºæ¨¡å‹
        model = model_fn(dropout=params['dropout']).to(device)
        optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=0.001, 
            weight_decay=params['weight_decay']
        )
        
        # è®­ç»ƒ
        for epoch in range(20):
            model.train()
            for inputs, targets in train_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                optimizer.zero_grad()
                loss = F.cross_entropy(model(inputs), targets)
                loss.backward()
                optimizer.step()
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                val_loss += F.cross_entropy(model(inputs), targets).item()
        val_loss /= len(val_loader)
        
        print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_params = params
    
    print(f"\næœ€ä½³å‚æ•°: {best_params}")
    return best_params
```

---

## ğŸ”¬ ç‰©ç†è§†è§’æ€»ç»“

### æ­£åˆ™åŒ–çš„ç»Ÿè®¡åŠ›å­¦è§£é‡Š

| æ­£åˆ™åŒ– | ç‰©ç†å¯¹åº” | æ•ˆæœ |
|--------|---------|------|
| L2 | è°æŒ¯å­åŠ¿ | æƒé‡è¶‹å‘å°å€¼ |
| L1 | å„å‘å¼‚æ€§åŠ¿ | äº§ç”Ÿç¨€ç–è§£ |
| Dropout | éšæœºç¨€é‡Š | é›†æˆæ•ˆåº” |
| BatchNorm | é‡æ•´åŒ– | ç¨³å®šè®­ç»ƒ |
| æ—©åœ | æœ‰é™æ—¶é—´ | é¿å…è¿‡åº¦å¼›è±« |

### é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èæ­£åˆ™åŒ– |
|------|-----------|
| å°æ•°æ®é›† | å¼ºæ­£åˆ™åŒ–ï¼ˆé«˜ dropoutã€é«˜ weight decayï¼‰ |
| å¤§æ•°æ®é›† | è½»æ­£åˆ™åŒ– + æ•°æ®å¢å¼º |
| CNN | Dropout2D + BatchNorm + æ•°æ®å¢å¼º |
| Transformer | LayerNorm + Dropout + Label Smoothing |
| ç‰©ç†é—®é¢˜ | åˆ©ç”¨å¯¹ç§°æ€§çš„æ•°æ®å¢å¼º |

---

## ğŸ“ ç»ƒä¹ 

1. æ¯”è¾ƒ L1 å’Œ L2 æ­£åˆ™åŒ–åœ¨ç‰¹å¾é€‰æ‹©ä¸Šçš„æ•ˆæœ
2. å®ç° Mixup æ•°æ®å¢å¼ºå¹¶è§‚å¯Ÿå…¶æ•ˆæœ
3. ä¸ºä½ çš„ç‰©ç†æ•°æ®è®¾è®¡åˆé€‚çš„æ•°æ®å¢å¼ºç­–ç•¥

---

## â­ï¸ ä¸‹ä¸€èŠ‚

ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†å­¦ä¹  [å­¦ä¹ ç‡è°ƒåº¦](./02_learning_rate_scheduling.md)ï¼Œäº†è§£å¦‚ä½•åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ä»¥è·å¾—æ›´å¥½çš„è®­ç»ƒæ•ˆæœã€‚

