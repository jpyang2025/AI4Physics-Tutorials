# 6.2 å­¦ä¹ ç‡è°ƒåº¦

## ğŸ“– æ¦‚è¿°

å­¦ä¹ ç‡æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€é‡è¦çš„è¶…å‚æ•°ä¹‹ä¸€ã€‚åˆé€‚çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½å’Œè®­ç»ƒç¨³å®šæ€§ã€‚

ä»ç‰©ç†è§’åº¦çœ‹ï¼Œå­¦ä¹ ç‡è°ƒåº¦ç±»ä¼¼äº**æ¨¡æ‹Ÿé€€ç«**â€”â€”é€šè¿‡é€æ­¥é™ä½"æ¸©åº¦"æ¥æ‰¾åˆ°æ›´å¥½çš„è§£ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£å­¦ä¹ ç‡è°ƒåº¦çš„åŸç†
- æŒæ¡å¸¸ç”¨çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
- å®ç°è‡ªå®šä¹‰è°ƒåº¦ç­–ç•¥
- å­¦ä¼šä½¿ç”¨å­¦ä¹ ç‡é¢„çƒ­

---

## 6.2.1 å­¦ä¹ ç‡çš„ç‰©ç†æ„ä¹‰

### æ¢¯åº¦ä¸‹é™çš„åŠ¨åŠ›å­¦

æ¢¯åº¦ä¸‹é™å¯ä»¥å†™æˆï¼š

$$\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t)$$

å°†å…¶è§†ä¸ºè¿ç»­æ—¶é—´åŠ¨åŠ›å­¦ï¼š

$$\frac{d\theta}{dt} = -\eta \nabla L(\theta)$$

å­¦ä¹ ç‡ $\eta$ ç›¸å½“äºï¼š
- **æ—¶é—´æ­¥é•¿**ï¼š$\eta = \Delta t$
- **ç³»ç»Ÿæ¸©åº¦**ï¼šå½±å“åœ¨èƒ½é‡æ™¯è§‚ä¸­çš„æ¢ç´¢èƒ½åŠ›
- **é˜»å°¼ç³»æ•°çš„å€’æ•°**ï¼šåœ¨è¿‡é˜»å°¼ç³»ç»Ÿä¸­ $\eta \sim 1/\gamma$

### å­¦ä¹ ç‡è°ƒåº¦çš„ç‰©ç†å›¾åƒ

```
                    é«˜å­¦ä¹ ç‡                      ä½å­¦ä¹ ç‡
                    (é«˜æ¸©)                        (ä½æ¸©)
                    
èƒ½é‡              â•­â”€â•®                           â•­â”€â•®
æ™¯è§‚            â•­â”€â•¯ â•°â”€â•®                       â•­â”€â•¯ â•°â”€â•®
               â•¯     â•°â”€â•®                     â•¯     â•°â”€â•®
              â•¯       â•°â”€â•®   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>    â•¯       â•°â”€â•®
             â•¯          â•°                  â•¯          â•°
            â—â†â”€â”€â†’â—â†â”€â†’â—    æ¢ç´¢            â—            ç¨³å®š
              å¤§å¹…æŒ¯è¡                     æ”¶æ•›åˆ°æå°å€¼
```

---

## 6.2.2 PyTorch å­¦ä¹ ç‡è°ƒåº¦å™¨

### åŸºæœ¬ç”¨æ³•

```python
import torch
import torch.optim as optim
from torch.optim.lr_scheduler import (
    StepLR, MultiStepLR, ExponentialLR, CosineAnnealingLR,
    ReduceLROnPlateau, OneCycleLR, CyclicLR, LambdaLR
)

# åˆ›å»ºä¼˜åŒ–å™¨
model = torch.nn.Linear(10, 1)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# åˆ›å»ºè°ƒåº¦å™¨
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)

# è®­ç»ƒå¾ªç¯
for epoch in range(100):
    # è®­ç»ƒä»£ç ...
    train_one_epoch()
    
    # æ›´æ–°å­¦ä¹ ç‡ï¼ˆæ¯ä¸ª epoch ç»“æŸæ—¶ï¼‰
    scheduler.step()
    
    print(f"Epoch {epoch}, LR: {scheduler.get_last_lr()[0]:.6f}")
```

### å¸¸ç”¨è°ƒåº¦å™¨ä¸€è§ˆ

```python
def visualize_schedulers():
    """å¯è§†åŒ–å„ç§å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    import matplotlib.pyplot as plt
    
    epochs = 100
    initial_lr = 0.1
    
    schedulers = {}
    
    # 1. StepLR - é˜¶æ¢¯ä¸‹é™
    opt = optim.SGD([torch.zeros(1, requires_grad=True)], lr=initial_lr)
    schedulers['StepLR'] = (opt, StepLR(opt, step_size=30, gamma=0.1))
    
    # 2. MultiStepLR - å¤šé˜¶æ®µé˜¶æ¢¯ä¸‹é™
    opt = optim.SGD([torch.zeros(1, requires_grad=True)], lr=initial_lr)
    schedulers['MultiStepLR'] = (opt, MultiStepLR(opt, milestones=[30, 60, 80], gamma=0.1))
    
    # 3. ExponentialLR - æŒ‡æ•°è¡°å‡
    opt = optim.SGD([torch.zeros(1, requires_grad=True)], lr=initial_lr)
    schedulers['ExponentialLR'] = (opt, ExponentialLR(opt, gamma=0.95))
    
    # 4. CosineAnnealingLR - ä½™å¼¦é€€ç«
    opt = optim.SGD([torch.zeros(1, requires_grad=True)], lr=initial_lr)
    schedulers['CosineAnnealingLR'] = (opt, CosineAnnealingLR(opt, T_max=epochs))
    
    # æ”¶é›†å­¦ä¹ ç‡
    lrs = {name: [] for name in schedulers}
    
    for epoch in range(epochs):
        for name, (opt, sched) in schedulers.items():
            lrs[name].append(opt.param_groups[0]['lr'])
            sched.step()
    
    # ç»˜å›¾
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    for ax, (name, lr_values) in zip(axes.flat, lrs.items()):
        ax.plot(lr_values, linewidth=2)
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Learning Rate')
        ax.set_title(name)
        ax.grid(True, alpha=0.3)
        ax.set_yscale('log')
    
    plt.tight_layout()
    plt.show()
```

---

## 6.2.3 StepLR å’Œ MultiStepLR

### StepLR - å›ºå®šæ­¥é•¿è¡°å‡

```python
# æ¯ step_size ä¸ª epochï¼Œå­¦ä¹ ç‡ä¹˜ä»¥ gamma
scheduler = StepLR(
    optimizer,
    step_size=30,    # æ¯30ä¸ªepochè¡°å‡ä¸€æ¬¡
    gamma=0.1        # è¡°å‡ç³»æ•°
)

# å­¦ä¹ ç‡å˜åŒ–ï¼š0.1 â†’ 0.01 â†’ 0.001 â†’ ...
# åœ¨ epoch 30, 60, 90, ... è¡°å‡
```

### MultiStepLR - æŒ‡å®šé‡Œç¨‹ç¢‘è¡°å‡

```python
# åœ¨æŒ‡å®šçš„ epoch è¡°å‡
scheduler = MultiStepLR(
    optimizer,
    milestones=[50, 75, 90],  # åœ¨è¿™äº›epochè¡°å‡
    gamma=0.1
)

# å­¦ä¹ ç‡å˜åŒ–ï¼š
# epoch 0-49: 0.1
# epoch 50-74: 0.01
# epoch 75-89: 0.001
# epoch 90+: 0.0001
```

---

## 6.2.4 ExponentialLR - æŒ‡æ•°è¡°å‡

```python
# æ¯ä¸ª epoch å­¦ä¹ ç‡ä¹˜ä»¥ gamma
scheduler = ExponentialLR(
    optimizer,
    gamma=0.95  # æ¯ epoch è¡°å‡ 5%
)

# lr(t) = lr(0) * gamma^t
```

### ç‰©ç†ç±»æ¯”ï¼šæ”¾å°„æ€§è¡°å˜

æŒ‡æ•°è¡°å‡å¯¹åº”ç‰©ç†ä¸­çš„æ”¾å°„æ€§è¡°å˜ï¼š

$$\text{lr}(t) = \text{lr}_0 \cdot e^{-\lambda t}$$

å…¶ä¸­ $\gamma = e^{-\lambda}$ã€‚

---

## 6.2.5 CosineAnnealingLR - ä½™å¼¦é€€ç«

### åŸºæœ¬ä½™å¼¦é€€ç«

```python
# å­¦ä¹ ç‡æŒ‰ä½™å¼¦å‡½æ•°ä»åˆå§‹å€¼è¡°å‡åˆ°æœ€å°å€¼
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=100,      # å‘¨æœŸé•¿åº¦
    eta_min=1e-6    # æœ€å°å­¦ä¹ ç‡
)

# lr(t) = eta_min + 0.5 * (lr_0 - eta_min) * (1 + cos(Ï€ * t / T_max))
```

### å¸¦é‡å¯çš„ä½™å¼¦é€€ç«

```python
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts

# å‘¨æœŸæ€§é‡å¯
scheduler = CosineAnnealingWarmRestarts(
    optimizer,
    T_0=10,         # åˆå§‹å‘¨æœŸé•¿åº¦
    T_mult=2,       # æ¯æ¬¡é‡å¯åå‘¨æœŸå€å¢
    eta_min=1e-6
)
```

### ç‰©ç†ç±»æ¯”ï¼šæŒ¯è¡é™æ¸©

ä½™å¼¦é€€ç«ç±»ä¼¼äº**å‘¨æœŸæ€§æŒ¯è¡**å åŠ **æ•´ä½“é™æ¸©**ï¼š

```
LR
^
â”‚ â•­â•®    â•­â”€â•®      â•­â”€â”€â”€â•®          â•­â”€â”€â”€â”€â”€â”€â”€â•®
â”‚â•­â•¯ â•°â”€â”€â•®â•¯  â•°â”€â”€â”€â”€â•®â•¯    â•°â”€â”€â”€â”€â”€â”€â”€â”€â•®â•¯        â•°â”€â”€â”€â”€â”€â”€â”€â”€
â”‚                                              
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Epoch
```

---

## 6.2.6 ReduceLROnPlateau - è‡ªé€‚åº”è¡°å‡

æ ¹æ®éªŒè¯æŸå¤±è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ã€‚

```python
scheduler = ReduceLROnPlateau(
    optimizer,
    mode='min',        # ç›‘æ§æœ€å°å€¼
    factor=0.1,        # è¡°å‡å› å­
    patience=10,       # ç­‰å¾…æ”¹å–„çš„epochæ•°
    threshold=0.0001,  # åˆ¤æ–­æ”¹å–„çš„é˜ˆå€¼
    min_lr=1e-6        # æœ€å°å­¦ä¹ ç‡
)

# è®­ç»ƒå¾ªç¯
for epoch in range(100):
    train_loss = train_one_epoch()
    val_loss = validate()
    
    # æ ¹æ®éªŒè¯æŸå¤±è°ƒæ•´å­¦ä¹ ç‡
    scheduler.step(val_loss)  # æ³¨æ„ï¼šä¼ å…¥ç›‘æ§æŒ‡æ ‡
```

### ç‰©ç†ç±»æ¯”ï¼šåé¦ˆæ§åˆ¶

ReduceLROnPlateau ç±»ä¼¼äº**åé¦ˆæ§åˆ¶ç³»ç»Ÿ**ï¼š

- ç›‘æµ‹ç³»ç»ŸçŠ¶æ€ï¼ˆéªŒè¯æŸå¤±ï¼‰
- å½“ç³»ç»Ÿä¸å†æ”¹å–„æ—¶ï¼Œè°ƒæ•´æ§åˆ¶å‚æ•°ï¼ˆå­¦ä¹ ç‡ï¼‰

---

## 6.2.7 OneCycleLR - å•å‘¨æœŸç­–ç•¥

ç°ä»£æœ€æœ‰æ•ˆçš„å­¦ä¹ ç‡ç­–ç•¥ä¹‹ä¸€ã€‚

```python
# å­¦ä¹ ç‡å…ˆå‡åé™ï¼Œå®Œæˆä¸€ä¸ªå®Œæ•´å‘¨æœŸ
scheduler = OneCycleLR(
    optimizer,
    max_lr=0.1,              # æœ€å¤§å­¦ä¹ ç‡
    epochs=100,              # æ€»epochæ•°
    steps_per_epoch=len(train_loader),  # æ¯epochçš„stepæ•°
    pct_start=0.3,           # ä¸Šå‡é˜¶æ®µå æ¯”
    anneal_strategy='cos',   # ä¸‹é™ç­–ç•¥
    div_factor=25,           # åˆå§‹lr = max_lr / div_factor
    final_div_factor=10000   # æœ€ç»ˆlr = max_lr / final_div_factor
)

# æ³¨æ„ï¼šOneCycleLR éœ€è¦åœ¨æ¯ä¸ª batch åè°ƒç”¨
for epoch in range(100):
    for batch in train_loader:
        train_step(batch)
        scheduler.step()  # æ¯ä¸ª batch åæ›´æ–°
```

### å­¦ä¹ ç‡æ›²çº¿

```
LR
^
â”‚       â•­â”€â”€â”€â”€â”€â•®
â”‚     â•­â”€â•¯     â•°â”€â•®
â”‚   â•­â”€â•¯         â•°â”€â•®
â”‚ â•­â”€â•¯             â•°â”€â•®
â”‚â”€â•¯                 â•°â”€â”€â”€â”€â”€â”€â”€
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Steps
 â†‘               â†‘
 warmup         anneal
```

---

## 6.2.8 å­¦ä¹ ç‡é¢„çƒ­ï¼ˆWarmupï¼‰

### ä¸ºä»€ä¹ˆéœ€è¦é¢„çƒ­ï¼Ÿ

è®­ç»ƒåˆæœŸï¼Œç½‘ç»œå‚æ•°éšæœºåˆå§‹åŒ–ï¼Œæ¢¯åº¦å¯èƒ½å¾ˆå¤§æˆ–æ–¹å‘æ··ä¹±ã€‚é¢„çƒ­é˜¶æ®µä½¿ç”¨å°å­¦ä¹ ç‡ï¼Œè®©ç½‘ç»œ"ç¨³å®šä¸‹æ¥"ã€‚

### çº¿æ€§é¢„çƒ­

```python
def get_linear_warmup_scheduler(optimizer, warmup_epochs, total_epochs, 
                                 after_scheduler):
    """
    çº¿æ€§é¢„çƒ­ + åç»­è°ƒåº¦å™¨
    """
    def lr_lambda(epoch):
        if epoch < warmup_epochs:
            return (epoch + 1) / warmup_epochs
        return 1.0
    
    warmup_scheduler = LambdaLR(optimizer, lr_lambda)
    return warmup_scheduler


class WarmupScheduler:
    """é¢„çƒ­è°ƒåº¦å™¨åŒ…è£…å™¨"""
    
    def __init__(self, optimizer, warmup_epochs, after_scheduler):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.after_scheduler = after_scheduler
        self.current_epoch = 0
        self.base_lrs = [pg['lr'] for pg in optimizer.param_groups]
    
    def step(self):
        if self.current_epoch < self.warmup_epochs:
            # çº¿æ€§é¢„çƒ­
            warmup_factor = (self.current_epoch + 1) / self.warmup_epochs
            for pg, base_lr in zip(self.optimizer.param_groups, self.base_lrs):
                pg['lr'] = base_lr * warmup_factor
        else:
            # ä½¿ç”¨åç»­è°ƒåº¦å™¨
            self.after_scheduler.step()
        
        self.current_epoch += 1
    
    def get_last_lr(self):
        return [pg['lr'] for pg in self.optimizer.param_groups]


# ä½¿ç”¨ç¤ºä¾‹
optimizer = optim.Adam(model.parameters(), lr=0.001)
cosine_scheduler = CosineAnnealingLR(optimizer, T_max=95)
scheduler = WarmupScheduler(optimizer, warmup_epochs=5, 
                            after_scheduler=cosine_scheduler)
```

### Transformer é£æ ¼é¢„çƒ­

```python
class TransformerLRScheduler:
    """
    Transformer è®ºæ–‡ä¸­çš„å­¦ä¹ ç‡è°ƒåº¦
    
    lr = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))
    """
    
    def __init__(self, optimizer, d_model, warmup_steps=4000):
        self.optimizer = optimizer
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.current_step = 0
    
    def step(self):
        self.current_step += 1
        lr = self._compute_lr()
        for pg in self.optimizer.param_groups:
            pg['lr'] = lr
    
    def _compute_lr(self):
        step = self.current_step
        return self.d_model ** (-0.5) * min(
            step ** (-0.5),
            step * self.warmup_steps ** (-1.5)
        )


# ä½¿ç”¨
scheduler = TransformerLRScheduler(optimizer, d_model=512, warmup_steps=4000)
```

---

## 6.2.9 Cyclic Learning Rate

### CyclicLR

```python
# å­¦ä¹ ç‡åœ¨ä¸¤ä¸ªè¾¹ç•Œä¹‹é—´å‘¨æœŸæ€§å˜åŒ–
scheduler = CyclicLR(
    optimizer,
    base_lr=1e-4,         # ä¸‹ç•Œ
    max_lr=1e-2,          # ä¸Šç•Œ
    step_size_up=2000,    # ä¸Šå‡é˜¶æ®µçš„stepæ•°
    step_size_down=2000,  # ä¸‹é™é˜¶æ®µçš„stepæ•°
    mode='triangular',    # æ¨¡å¼ï¼štriangular, triangular2, exp_range
    cycle_momentum=True   # åŒæ­¥è°ƒæ•´åŠ¨é‡
)
```

### ç‰©ç†ç±»æ¯”ï¼šå‘¨æœŸæ€§é©±åŠ¨

å‘¨æœŸæ€§å­¦ä¹ ç‡ç±»ä¼¼äº**å‘¨æœŸæ€§å¤–åŠ›é©±åŠ¨**ï¼Œå¯ä»¥å¸®åŠ©ç³»ç»Ÿè·³å‡ºå±€éƒ¨æå°ï¼š

$$\eta(t) = \eta_0 + \Delta\eta \cdot \sin(\omega t)$$

---

## 6.2.10 è‡ªå®šä¹‰è°ƒåº¦å™¨

### LambdaLR

```python
# ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°
def custom_lr_lambda(epoch):
    """è‡ªå®šä¹‰å­¦ä¹ ç‡å‡½æ•°"""
    if epoch < 10:
        return epoch / 10  # é¢„çƒ­
    elif epoch < 50:
        return 1.0  # ä¿æŒ
    else:
        return 0.1 ** ((epoch - 50) / 50)  # è¡°å‡

scheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)
```

### å®Œå…¨è‡ªå®šä¹‰è°ƒåº¦å™¨

```python
class CustomScheduler:
    """å®Œå…¨è‡ªå®šä¹‰çš„å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    
    def __init__(self, optimizer, schedule_fn):
        """
        Args:
            optimizer: ä¼˜åŒ–å™¨
            schedule_fn: å‡½æ•° f(epoch) -> lr_multiplier
        """
        self.optimizer = optimizer
        self.schedule_fn = schedule_fn
        self.base_lrs = [pg['lr'] for pg in optimizer.param_groups]
        self.current_epoch = 0
    
    def step(self):
        multiplier = self.schedule_fn(self.current_epoch)
        for pg, base_lr in zip(self.optimizer.param_groups, self.base_lrs):
            pg['lr'] = base_lr * multiplier
        self.current_epoch += 1
    
    def get_last_lr(self):
        return [pg['lr'] for pg in self.optimizer.param_groups]


# ç‰©ç†å¯å‘çš„è°ƒåº¦ï¼šæ¨¡æ‹Ÿé€€ç«
def simulated_annealing_schedule(epoch, T0=1.0, T_min=0.01, alpha=0.99):
    """
    æ¨¡æ‹Ÿé€€ç«è°ƒåº¦
    T(n) = max(T_min, T0 * alpha^n)
    """
    return max(T_min, T0 * (alpha ** epoch))

scheduler = CustomScheduler(optimizer, simulated_annealing_schedule)
```

---

## 6.2.11 è°ƒåº¦å™¨é“¾

### é¡ºåºè°ƒåº¦å™¨

```python
from torch.optim.lr_scheduler import SequentialLR

# ç»„åˆå¤šä¸ªè°ƒåº¦å™¨
scheduler1 = LinearLR(optimizer, start_factor=0.1, total_iters=5)  # é¢„çƒ­
scheduler2 = CosineAnnealingLR(optimizer, T_max=95)  # ä½™å¼¦é€€ç«

scheduler = SequentialLR(
    optimizer,
    schedulers=[scheduler1, scheduler2],
    milestones=[5]  # åœ¨ç¬¬5ä¸ªepochåˆ‡æ¢
)
```

### é“¾å¼è°ƒåº¦å™¨

```python
from torch.optim.lr_scheduler import ChainedScheduler

# åŒæ—¶åº”ç”¨å¤šä¸ªè°ƒåº¦å™¨ï¼ˆæ•ˆæœç›¸ä¹˜ï¼‰
scheduler = ChainedScheduler([
    ExponentialLR(optimizer, gamma=0.99),  # åŸºç¡€è¡°å‡
    CyclicLR(optimizer, base_lr=0.001, max_lr=0.01)  # å‘¨æœŸæ€§å˜åŒ–
])
```

---

## 6.2.12 å­¦ä¹ ç‡æŸ¥æ‰¾

### è‡ªåŠ¨æ‰¾åˆ°æœ€ä½³å­¦ä¹ ç‡

```python
def find_lr(model, train_loader, criterion, optimizer, 
            init_lr=1e-8, final_lr=10, num_steps=100, device='cpu'):
    """
    å­¦ä¹ ç‡èŒƒå›´æµ‹è¯•
    
    å‚è€ƒï¼šLeslie Smith çš„è®ºæ–‡ "Cyclical Learning Rates for Training Neural Networks"
    """
    model.train()
    model = model.to(device)
    
    # ä¿å­˜åˆå§‹çŠ¶æ€
    initial_state = {k: v.clone() for k, v in model.state_dict().items()}
    
    # å­¦ä¹ ç‡ä¹˜æ•°
    lr_mult = (final_lr / init_lr) ** (1 / num_steps)
    
    lrs = []
    losses = []
    best_loss = float('inf')
    
    # è®¾ç½®åˆå§‹å­¦ä¹ ç‡
    for pg in optimizer.param_groups:
        pg['lr'] = init_lr
    
    data_iter = iter(train_loader)
    
    for step in range(num_steps):
        try:
            inputs, targets = next(data_iter)
        except StopIteration:
            data_iter = iter(train_loader)
            inputs, targets = next(data_iter)
        
        inputs, targets = inputs.to(device), targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        
        # è®°å½•
        current_lr = optimizer.param_groups[0]['lr']
        lrs.append(current_lr)
        losses.append(loss.item())
        
        # æ£€æŸ¥æ˜¯å¦å‘æ•£
        if loss.item() < best_loss:
            best_loss = loss.item()
        if loss.item() > 4 * best_loss:
            print("æŸå¤±å‘æ•£ï¼Œåœæ­¢æœç´¢")
            break
        
        loss.backward()
        optimizer.step()
        
        # å¢åŠ å­¦ä¹ ç‡
        for pg in optimizer.param_groups:
            pg['lr'] *= lr_mult
    
    # æ¢å¤åˆå§‹çŠ¶æ€
    model.load_state_dict(initial_state)
    
    # æ‰¾åˆ°æœ€ä½³å­¦ä¹ ç‡ï¼ˆæŸå¤±ä¸‹é™æœ€å¿«çš„ç‚¹ï¼‰
    gradients = np.gradient(losses)
    best_idx = np.argmin(gradients)
    suggested_lr = lrs[best_idx]
    
    print(f"å»ºè®®å­¦ä¹ ç‡: {suggested_lr:.2e}")
    
    # ç»˜å›¾
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.semilogx(lrs, losses)
    plt.axvline(x=suggested_lr, color='r', linestyle='--', label=f'å»ºè®®: {suggested_lr:.2e}')
    plt.xlabel('Learning Rate')
    plt.ylabel('Loss')
    plt.title('Learning Rate Finder')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.semilogx(lrs[:len(gradients)], gradients)
    plt.xlabel('Learning Rate')
    plt.ylabel('Loss Gradient')
    plt.title('Loss Gradient')
    
    plt.tight_layout()
    plt.show()
    
    return suggested_lr, lrs, losses
```

---

## 6.2.13 å®è·µå»ºè®®

### è°ƒåº¦å™¨é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èè°ƒåº¦å™¨ |
|------|-----------|
| åŸºçº¿å®éªŒ | StepLR æˆ– MultiStepLR |
| è¿½æ±‚æœ€ä½³æ€§èƒ½ | CosineAnnealingLR æˆ– OneCycleLR |
| è®­ç»ƒä¸ç¨³å®š | Warmup + ä»»æ„è°ƒåº¦å™¨ |
| è‡ªåŠ¨è°ƒæ•´ | ReduceLROnPlateau |
| Transformer | Warmup + Inverse Square Root æˆ– Cosine |

### å¸¸è§é”™è¯¯

```python
# âŒ é”™è¯¯ï¼šå¿˜è®°åœ¨æ¯ä¸ª epoch è°ƒç”¨ scheduler.step()
for epoch in range(100):
    train()
    # scheduler.step()  # å¿˜è®°è°ƒç”¨ï¼

# âœ“ æ­£ç¡®
for epoch in range(100):
    train()
    scheduler.step()

# âŒ é”™è¯¯ï¼šReduceLROnPlateau å¿˜è®°ä¼ å…¥ç›‘æ§æŒ‡æ ‡
scheduler.step()  # ç¼ºå°‘å‚æ•°

# âœ“ æ­£ç¡®
scheduler.step(val_loss)

# âŒ é”™è¯¯ï¼šOneCycleLR åœ¨ epoch çº§åˆ«è°ƒç”¨
for epoch in range(100):
    train()
    scheduler.step()  # åº”è¯¥åœ¨ batch çº§åˆ«

# âœ“ æ­£ç¡®
for epoch in range(100):
    for batch in train_loader:
        train_step(batch)
        scheduler.step()  # æ¯ä¸ª batch åè°ƒç”¨
```

---

## ğŸ”¬ ç‰©ç†è§†è§’æ€»ç»“

### å­¦ä¹ ç‡è°ƒåº¦çš„ç‰©ç†å¯¹åº”

| è°ƒåº¦ç­–ç•¥ | ç‰©ç†è¿‡ç¨‹ |
|---------|---------|
| æ’å®šå­¦ä¹ ç‡ | æ’æ¸©åˆ†å­åŠ¨åŠ›å­¦ |
| é˜¶æ¢¯è¡°å‡ | é˜¶æ¢¯å¼é™æ¸© |
| æŒ‡æ•°è¡°å‡ | æŒ‡æ•°é™æ¸© |
| ä½™å¼¦é€€ç« | å‘¨æœŸæ€§æŒ¯è¡é™æ¸© |
| å‘¨æœŸæ€§å­¦ä¹ ç‡ | å‘¨æœŸæ€§å¤–åŠ›é©±åŠ¨ |
| é¢„çƒ­ | ç¼“æ…¢å‡æ¸©åå†é™æ¸© |

### æ¨¡æ‹Ÿé€€ç«çš„å¯ç¤º

æœ€ä¼˜çš„å­¦ä¹ ç‡è°ƒåº¦éµå¾ªæ¨¡æ‹Ÿé€€ç«çš„åŸåˆ™ï¼š

1. **åˆå§‹é«˜æ¸©**ï¼šæ¢ç´¢èƒ½é‡æ™¯è§‚
2. **ç¼“æ…¢é™æ¸©**ï¼šé€æ¸æ”¶æ•›åˆ°å±€éƒ¨æå°
3. **æœ€ç»ˆä½æ¸©**ï¼šç¨³å®šåœ¨æå°å€¼

é™æ¸©é€Ÿç‡ä¸èƒ½å¤ªå¿«ï¼ˆå¯èƒ½å›°åœ¨é«˜èƒ½æ€ï¼‰ä¹Ÿä¸èƒ½å¤ªæ…¢ï¼ˆæµªè´¹è®¡ç®—èµ„æºï¼‰ã€‚

---

## ğŸ“ ç»ƒä¹ 

1. å¯è§†åŒ–ä¸åŒå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å­¦ä¹ ç‡æ›²çº¿
2. ä½¿ç”¨å­¦ä¹ ç‡æŸ¥æ‰¾å™¨ä¸ºä½ çš„æ¨¡å‹æ‰¾åˆ°æœ€ä½³å­¦ä¹ ç‡
3. å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„æ¨¡æ‹Ÿé€€ç«è°ƒåº¦å™¨

---

## â­ï¸ ä¸‹ä¸€èŠ‚

ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†å­¦ä¹  [æ¨¡å‹ä¿å­˜ä¸åŠ è½½](./03_model_save_load.md)ï¼Œäº†è§£å¦‚ä½•æ­£ç¡®ä¿å­˜å’Œæ¢å¤è®­ç»ƒçŠ¶æ€ã€‚

