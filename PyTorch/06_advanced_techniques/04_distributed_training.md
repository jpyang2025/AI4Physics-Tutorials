# 6.4 åˆ†å¸ƒå¼è®­ç»ƒ

## ğŸ“– æ¦‚è¿°

éšç€æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é‡çš„å¢é•¿ï¼Œå• GPU è®­ç»ƒå¾€å¾€ä¸å¤Ÿé«˜æ•ˆã€‚åˆ†å¸ƒå¼è®­ç»ƒå…è®¸æˆ‘ä»¬åˆ©ç”¨å¤šä¸ª GPU æˆ–å¤šå°æœºå™¨æ¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£åˆ†å¸ƒå¼è®­ç»ƒçš„åŸºæœ¬æ¦‚å¿µ
- æŒæ¡ DataParallel å’Œ DistributedDataParallel
- äº†è§£æ··åˆç²¾åº¦è®­ç»ƒ
- å­¦ä¼šå¤„ç†å¤§è§„æ¨¡è®­ç»ƒçš„æŠ€å·§

---

## 6.4.1 åˆ†å¸ƒå¼è®­ç»ƒæ¦‚è¿°

### å¹¶è¡Œç­–ç•¥

```
åˆ†å¸ƒå¼è®­ç»ƒ
â”‚
â”œâ”€â”€ æ•°æ®å¹¶è¡Œï¼ˆData Parallelismï¼‰
â”‚   â””â”€â”€ æ¯ä¸ª GPU æœ‰å®Œæ•´æ¨¡å‹å‰¯æœ¬ï¼Œå¤„ç†ä¸åŒæ•°æ®
â”‚
â”œâ”€â”€ æ¨¡å‹å¹¶è¡Œï¼ˆModel Parallelismï¼‰
â”‚   â””â”€â”€ æ¨¡å‹å¤ªå¤§ï¼Œåˆ†å¸ƒåœ¨å¤šä¸ª GPU ä¸Š
â”‚
â””â”€â”€ æµæ°´çº¿å¹¶è¡Œï¼ˆPipeline Parallelismï¼‰
    â””â”€â”€ å°†æ¨¡å‹åˆ†æˆå¤šä¸ªé˜¶æ®µï¼Œåƒæµæ°´çº¿ä¸€æ ·å¤„ç†
```

### ç‰©ç†ç±»æ¯”

åˆ†å¸ƒå¼è®­ç»ƒç±»ä¼¼äº**å¹¶è¡Œè®¡ç®—**ä¸­çš„å¤šå¤„ç†å™¨æ¨¡æ‹Ÿï¼š

| æ¦‚å¿µ | ç‰©ç†æ¨¡æ‹Ÿç±»æ¯” |
|------|-------------|
| æ•°æ®å¹¶è¡Œ | ç©ºé—´åˆ†è§£æ³•ï¼ˆæ¯ä¸ªå¤„ç†å™¨è´Ÿè´£ä¸€éƒ¨åˆ†ç©ºé—´ï¼‰ |
| æ¨¡å‹å¹¶è¡Œ | ä»»åŠ¡åˆ†è§£æ³•ï¼ˆä¸åŒå¤„ç†å™¨è®¡ç®—ä¸åŒç‰©ç†é‡ï¼‰ |
| æ¢¯åº¦åŒæ­¥ | è¾¹ç•Œæ¡ä»¶äº¤æ¢ |

---

## 6.4.2 DataParallelï¼ˆå•æœºå¤šå¡ï¼‰

### åŸºæœ¬ç”¨æ³•

```python
import torch
import torch.nn as nn

# åˆ›å»ºæ¨¡å‹
model = YourModel()

# æ£€æŸ¥å¯ç”¨ GPU
if torch.cuda.device_count() > 1:
    print(f"ä½¿ç”¨ {torch.cuda.device_count()} ä¸ª GPU")
    model = nn.DataParallel(model)

model = model.cuda()

# è®­ç»ƒä»£ç ä¿æŒä¸å˜
for inputs, targets in train_loader:
    inputs = inputs.cuda()
    targets = targets.cuda()
    
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

### DataParallel çš„å·¥ä½œåŸç†

```
                         è¾“å…¥æ•°æ®
                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼               â–¼
               GPU 0 æ•°æ®      GPU 1 æ•°æ®
                    â”‚               â”‚
                    â–¼               â–¼
              æ¨¡å‹å‰¯æœ¬ 0       æ¨¡å‹å‰¯æœ¬ 1
                    â”‚               â”‚
                    â–¼               â–¼
                è¾“å‡º 0          è¾“å‡º 1
                    â”‚               â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                    åœ¨ GPU 0 ä¸Šæ±‡æ€»
                            â”‚
                            â–¼
                    è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
                            â”‚
                            â–¼
                    å¹¿æ’­æ¢¯åº¦åˆ°æ‰€æœ‰ GPU
                            â”‚
                            â–¼
                       æ›´æ–°å‚æ•°
```

### DataParallel çš„å±€é™æ€§

1. **GPU 0 æˆä¸ºç“¶é¢ˆ**ï¼šæ‰€æœ‰è¾“å‡ºéƒ½æ±‡é›†åˆ° GPU 0
2. **æ˜¾å­˜ä¸å‡è¡¡**ï¼šGPU 0 å ç”¨æ›´å¤šæ˜¾å­˜
3. **Python GIL é™åˆ¶**ï¼šå¤šçº¿ç¨‹æ•ˆç‡ä¸é«˜

---

## 6.4.3 DistributedDataParallelï¼ˆæ¨èï¼‰

DistributedDataParallel (DDP) æ˜¯æ›´é«˜æ•ˆçš„å¤š GPU è®­ç»ƒæ–¹å¼ã€‚

### åŸºæœ¬æ¦‚å¿µ

```python
# å…³é”®æ¦‚å¿µï¼š
# - world_size: æ€»è¿›ç¨‹æ•°ï¼ˆé€šå¸¸ç­‰äº GPU æ•°ï¼‰
# - rank: å½“å‰è¿›ç¨‹çš„å…¨å±€ ID
# - local_rank: å½“å‰è¿›ç¨‹åœ¨æœ¬æœºçš„ GPU ID
```

### å•æœºå¤šå¡ DDP

```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

def setup(rank, world_size):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group(
        backend='nccl',  # NVIDIA GPU ä½¿ç”¨ nccl
        rank=rank,
        world_size=world_size
    )
    
    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„ GPU
    torch.cuda.set_device(rank)

def cleanup():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    dist.destroy_process_group()

def train(rank, world_size, args):
    """è®­ç»ƒå‡½æ•°ï¼ˆåœ¨æ¯ä¸ªè¿›ç¨‹ä¸­è¿è¡Œï¼‰"""
    
    # åˆå§‹åŒ–
    setup(rank, world_size)
    
    # åˆ›å»ºæ¨¡å‹
    model = YourModel().cuda(rank)
    model = DDP(model, device_ids=[rank])
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨ï¼‰
    train_dataset = YourDataset()
    train_sampler = DistributedSampler(
        train_dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True
    )
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        sampler=train_sampler,
        num_workers=4,
        pin_memory=True
    )
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    criterion = nn.CrossEntropyLoss().cuda(rank)
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        train_sampler.set_epoch(epoch)  # é‡è¦ï¼šç¡®ä¿æ¯ä¸ª epoch çš„æ•°æ®é¡ºåºä¸åŒ
        
        model.train()
        for inputs, targets in train_loader:
            inputs = inputs.cuda(rank, non_blocking=True)
            targets = targets.cuda(rank, non_blocking=True)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
        
        # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°å’Œä¿å­˜
        if rank == 0:
            print(f"Epoch {epoch} completed")
            torch.save(model.module.state_dict(), 'model.pth')
    
    cleanup()

def main():
    """ä¸»å‡½æ•°"""
    world_size = torch.cuda.device_count()
    
    # å¯åŠ¨å¤šä¸ªè¿›ç¨‹
    mp.spawn(
        train,
        args=(world_size, args),
        nprocs=world_size,
        join=True
    )

if __name__ == '__main__':
    main()
```

### ä½¿ç”¨ torchrun å¯åŠ¨ï¼ˆæ¨èï¼‰

```python
# train_ddp.py
import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def main():
    # ä»ç¯å¢ƒå˜é‡è·å–åˆ†å¸ƒå¼ä¿¡æ¯
    local_rank = int(os.environ['LOCAL_RANK'])
    world_size = int(os.environ['WORLD_SIZE'])
    rank = int(os.environ['RANK'])
    
    # åˆå§‹åŒ–
    dist.init_process_group(backend='nccl')
    torch.cuda.set_device(local_rank)
    
    # åˆ›å»ºæ¨¡å‹
    model = YourModel().cuda()
    model = DDP(model, device_ids=[local_rank])
    
    # ... è®­ç»ƒä»£ç  ...
    
    dist.destroy_process_group()

if __name__ == '__main__':
    main()
```

```bash
# å¯åŠ¨å‘½ä»¤
# å•æœº 4 å¡
torchrun --nproc_per_node=4 train_ddp.py

# å¤šæœºï¼ˆ2 æœºå™¨ï¼Œæ¯æœº 4 å¡ï¼‰
# æœºå™¨ 1
torchrun --nnodes=2 --node_rank=0 --master_addr="192.168.1.1" \
         --master_port=12355 --nproc_per_node=4 train_ddp.py

# æœºå™¨ 2
torchrun --nnodes=2 --node_rank=1 --master_addr="192.168.1.1" \
         --master_port=12355 --nproc_per_node=4 train_ddp.py
```

---

## 6.4.4 æ··åˆç²¾åº¦è®­ç»ƒ

### ä½¿ç”¨ torch.cuda.amp

```python
from torch.cuda.amp import autocast, GradScaler

# åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨
scaler = GradScaler()

for inputs, targets in train_loader:
    inputs = inputs.cuda()
    targets = targets.cuda()
    
    optimizer.zero_grad()
    
    # è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
    with autocast():
        outputs = model(inputs)
        loss = criterion(outputs, targets)
    
    # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­
    scaler.scale(loss).backward()
    
    # æ›´æ–°å‚æ•°ï¼ˆè‡ªåŠ¨å¤„ç†æ¢¯åº¦ç¼©æ”¾ï¼‰
    scaler.step(optimizer)
    scaler.update()
```

### DDP + æ··åˆç²¾åº¦

```python
def train_ddp_amp(rank, world_size, args):
    """DDP + æ··åˆç²¾åº¦è®­ç»ƒ"""
    
    setup(rank, world_size)
    
    model = YourModel().cuda(rank)
    model = DDP(model, device_ids=[rank])
    
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    criterion = nn.CrossEntropyLoss().cuda(rank)
    scaler = GradScaler()
    
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=train_sampler)
    
    for epoch in range(args.epochs):
        train_sampler.set_epoch(epoch)
        
        for inputs, targets in train_loader:
            inputs = inputs.cuda(rank, non_blocking=True)
            targets = targets.cuda(rank, non_blocking=True)
            
            optimizer.zero_grad()
            
            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, targets)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
    
    cleanup()
```

### æ··åˆç²¾åº¦çš„ç‰©ç†æ„ä¹‰

æ··åˆç²¾åº¦è®­ç»ƒä½¿ç”¨ FP16ï¼ˆ16ä½æµ®ç‚¹ï¼‰è¿›è¡Œå‰å‘/åå‘ä¼ æ’­ï¼ŒFP32ï¼ˆ32ä½ï¼‰å­˜å‚¨å‚æ•°ï¼š

| ç²¾åº¦ | æ•°å€¼èŒƒå›´ | ç±»æ¯” |
|-----|---------|------|
| FP32 | ~1e-38 åˆ° ~1e38 | é«˜ç²¾åº¦ç§‘å­¦è®¡ç®— |
| FP16 | ~1e-8 åˆ° ~65504 | å·¥ç¨‹ä¼°ç®— |

å…³é”®åœ¨äº**åŠ¨æ€æŸå¤±ç¼©æ”¾**ï¼šé˜²æ­¢å°æ¢¯åº¦åœ¨ FP16 ä¸­ä¸‹æº¢ã€‚

---

## 6.4.5 æ¢¯åº¦ç´¯ç§¯

å½“ GPU æ˜¾å­˜ä¸è¶³ä»¥å®¹çº³å¤§æ‰¹é‡æ—¶ä½¿ç”¨ã€‚

```python
accumulation_steps = 4  # ç´¯ç§¯ 4 æ­¥ç­‰æ•ˆäº 4 å€ batch size
effective_batch_size = batch_size * accumulation_steps

optimizer.zero_grad()

for i, (inputs, targets) in enumerate(train_loader):
    inputs = inputs.cuda()
    targets = targets.cuda()
    
    # å‰å‘ä¼ æ’­
    with autocast():
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss = loss / accumulation_steps  # ç¼©æ”¾æŸå¤±
    
    # åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰
    scaler.scale(loss).backward()
    
    # æ¯ accumulation_steps æ­¥æ›´æ–°ä¸€æ¬¡
    if (i + 1) % accumulation_steps == 0:
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()
```

---

## 6.4.6 æ¢¯åº¦åŒæ­¥

### åŒæ­¥ BatchNorm

```python
# è·¨ GPU åŒæ­¥ BatchNorm ç»Ÿè®¡é‡
model = nn.SyncBatchNorm.convert_sync_batchnorm(model)
model = DDP(model, device_ids=[local_rank])
```

### æ‰‹åŠ¨æ¢¯åº¦åŒæ­¥

```python
# åœ¨ç‰¹å®šæ—¶åˆ»åŒæ­¥æ¢¯åº¦
dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
tensor /= world_size

# å¹¿æ’­å‚æ•°
dist.broadcast(tensor, src=0)  # ä» rank 0 å¹¿æ’­

# æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„å¼ é‡
gathered = [torch.zeros_like(tensor) for _ in range(world_size)]
dist.all_gather(gathered, tensor)
```

---

## 6.4.7 åˆ†å¸ƒå¼è®­ç»ƒå®ç”¨å·¥å…·

### åªåœ¨ä¸»è¿›ç¨‹æ‰§è¡Œ

```python
def is_main_process():
    """æ£€æŸ¥æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
    return not dist.is_initialized() or dist.get_rank() == 0

def save_on_master(state, path):
    """åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜"""
    if is_main_process():
        torch.save(state, path)

def print_on_master(*args, **kwargs):
    """åªåœ¨ä¸»è¿›ç¨‹æ‰“å°"""
    if is_main_process():
        print(*args, **kwargs)
```

### åˆ†å¸ƒå¼é‡‡æ ·å™¨

```python
from torch.utils.data.distributed import DistributedSampler

# åˆ›å»ºåˆ†å¸ƒå¼é‡‡æ ·å™¨
train_sampler = DistributedSampler(
    train_dataset,
    num_replicas=world_size,  # æ€»è¿›ç¨‹æ•°
    rank=rank,                 # å½“å‰è¿›ç¨‹
    shuffle=True,
    drop_last=True
)

train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    sampler=train_sampler,    # ä¸è¦è®¾ç½® shuffle=True
    num_workers=4,
    pin_memory=True
)

# æ¯ä¸ª epoch å¼€å§‹æ—¶è®¾ç½® epoch
for epoch in range(num_epochs):
    train_sampler.set_epoch(epoch)  # é‡è¦ï¼
    # ... è®­ç»ƒ ...
```

---

## 6.4.8 å®Œæ•´ DDP è®­ç»ƒè„šæœ¬

```python
#!/usr/bin/env python3
"""
å®Œæ•´çš„ DDP è®­ç»ƒè„šæœ¬

ä½¿ç”¨æ–¹æ³•ï¼š
    torchrun --nproc_per_node=4 train_ddp_complete.py
"""

import os
import argparse
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch.cuda.amp import autocast, GradScaler
import torchvision
import torchvision.transforms as transforms


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch-size', type=int, default=64)
    parser.add_argument('--lr', type=float, default=0.001)
    return parser.parse_args()


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if 'RANK' not in os.environ:
        # éåˆ†å¸ƒå¼æ¨¡å¼
        return 0, 1, 0
    
    rank = int(os.environ['RANK'])
    world_size = int(os.environ['WORLD_SIZE'])
    local_rank = int(os.environ['LOCAL_RANK'])
    
    dist.init_process_group(backend='nccl')
    torch.cuda.set_device(local_rank)
    
    return rank, world_size, local_rank


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def is_main_process():
    return not dist.is_initialized() or dist.get_rank() == 0


def main():
    args = parse_args()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼
    rank, world_size, local_rank = setup_distributed()
    device = torch.device(f'cuda:{local_rank}')
    
    if is_main_process():
        print(f"è®­ç»ƒé…ç½®: {world_size} GPUs, batch_size={args.batch_size}")
    
    # æ•°æ®
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    train_dataset = torchvision.datasets.MNIST(
        './data', train=True, download=True, transform=transform
    )
    
    # åˆ†å¸ƒå¼é‡‡æ ·å™¨
    if dist.is_initialized():
        train_sampler = DistributedSampler(train_dataset, shuffle=True)
    else:
        train_sampler = None
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        num_workers=4,
        pin_memory=True
    )
    
    # æ¨¡å‹
    model = nn.Sequential(
        nn.Flatten(),
        nn.Linear(784, 256),
        nn.ReLU(),
        nn.Linear(256, 10)
    ).to(device)
    
    # DDP åŒ…è£…
    if dist.is_initialized():
        model = DDP(model, device_ids=[local_rank])
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    criterion = nn.CrossEntropyLoss()
    scaler = GradScaler()
    
    # è®­ç»ƒ
    for epoch in range(args.epochs):
        if train_sampler:
            train_sampler.set_epoch(epoch)
        
        model.train()
        total_loss = 0.0
        
        for inputs, targets in train_loader:
            inputs = inputs.to(device, non_blocking=True)
            targets = targets.to(device, non_blocking=True)
            
            optimizer.zero_grad()
            
            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, targets)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            total_loss += loss.item()
        
        if is_main_process():
            avg_loss = total_loss / len(train_loader)
            print(f"Epoch {epoch}: Loss = {avg_loss:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    if is_main_process():
        state_dict = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()
        torch.save(state_dict, 'model_ddp.pth')
        print("æ¨¡å‹å·²ä¿å­˜")
    
    cleanup_distributed()


if __name__ == '__main__':
    main()
```

---

## 6.4.9 è°ƒè¯•åˆ†å¸ƒå¼è®­ç»ƒ

### å¸¸è§é—®é¢˜

```python
# é—®é¢˜1ï¼šNCCL è¶…æ—¶
# è§£å†³ï¼šå¢åŠ è¶…æ—¶æ—¶é—´
os.environ['NCCL_BLOCKING_WAIT'] = '1'
dist.init_process_group(backend='nccl', timeout=datetime.timedelta(hours=2))

# é—®é¢˜2ï¼šç«¯å£å ç”¨
# è§£å†³ï¼šæ›´æ¢ç«¯å£
os.environ['MASTER_PORT'] = '12356'

# é—®é¢˜3ï¼šæ˜¾å­˜ä¸å‡è¡¡
# è§£å†³ï¼šä½¿ç”¨ DistributedDataParallel è€Œé DataParallel

# é—®é¢˜4ï¼šæ­»é”
# æ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰è¿›ç¨‹æ‰§è¡Œç›¸åŒçš„é›†åˆæ“ä½œ
```

### è°ƒè¯•æŠ€å·§

```python
def debug_print(msg, rank=None):
    """å¸¦ rank ä¿¡æ¯çš„è°ƒè¯•æ‰“å°"""
    if rank is None:
        rank = dist.get_rank() if dist.is_initialized() else 0
    print(f"[Rank {rank}] {msg}")

# æ£€æŸ¥å¼ é‡æ˜¯å¦åŒæ­¥
def check_sync(tensor, name="tensor"):
    """æ£€æŸ¥å¼ é‡åœ¨æ‰€æœ‰è¿›ç¨‹æ˜¯å¦ç›¸åŒ"""
    if not dist.is_initialized():
        return True
    
    tensor_list = [torch.zeros_like(tensor) for _ in range(dist.get_world_size())]
    dist.all_gather(tensor_list, tensor)
    
    for i, t in enumerate(tensor_list[1:], 1):
        if not torch.allclose(tensor_list[0], t, atol=1e-6):
            print(f"è­¦å‘Šï¼š{name} åœ¨ rank 0 å’Œ rank {i} ä¸åŒæ­¥")
            return False
    return True
```

---

## ğŸ”¬ ç‰©ç†è§†è§’æ€»ç»“

### å¹¶è¡Œè®¡ç®—çš„ç‰©ç†ç±»æ¯”

| åˆ†å¸ƒå¼æ¦‚å¿µ | ç‰©ç†æ¨¡æ‹Ÿç±»æ¯” |
|-----------|-------------|
| æ•°æ®å¹¶è¡Œ | ç©ºé—´åŸŸåˆ†è§£ |
| æ¢¯åº¦åŒæ­¥ | è¾¹ç•Œæ¡ä»¶äº¤æ¢ |
| æ‰¹é‡å¤§å° | é‡‡æ ·æ•°/ç³»ç»¼å¤§å° |
| æ··åˆç²¾åº¦ | è‡ªé€‚åº”ç²¾åº¦ç§¯åˆ† |

### ç¼©æ”¾å®šå¾‹

ç†æƒ³æƒ…å†µä¸‹ï¼Œä½¿ç”¨ $N$ ä¸ª GPU åº”è¯¥è·å¾—æ¥è¿‘ $N$ å€çš„åŠ é€Ÿï¼š

$$T_N = \frac{T_1}{N} + T_{\text{comm}}$$

å…¶ä¸­ $T_{\text{comm}}$ æ˜¯é€šä¿¡å¼€é”€ã€‚

---

## ğŸ“ ç»ƒä¹ 

1. å°†ç°æœ‰çš„å• GPU è®­ç»ƒè„šæœ¬æ”¹å†™ä¸º DDP ç‰ˆæœ¬
2. å®ç°æ··åˆç²¾åº¦è®­ç»ƒå¹¶æ¯”è¾ƒé€Ÿåº¦æå‡
3. å°è¯•ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§æ‰¹é‡è®­ç»ƒ

---

## â­ï¸ ä¸‹ä¸€ç« é¢„å‘Š

æŒæ¡äº†è¿™äº›è¿›é˜¶æŠ€æœ¯åï¼Œç¬¬7ç« å°†å±•ç¤ºå¦‚ä½•å°†ç¥ç»ç½‘ç»œåº”ç”¨äºç‰©ç†å­¦é—®é¢˜ï¼ŒåŒ…æ‹¬æ±‚è§£å¾®åˆ†æ–¹ç¨‹ã€åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿç­‰ã€‚

