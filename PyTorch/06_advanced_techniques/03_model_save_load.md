# 6.3 æ¨¡å‹ä¿å­˜ä¸åŠ è½½

## ğŸ“– æ¦‚è¿°

æ­£ç¡®ä¿å­˜å’ŒåŠ è½½æ¨¡å‹æ˜¯æ·±åº¦å­¦ä¹ å·¥ä½œæµç¨‹ä¸­çš„é‡è¦ç¯èŠ‚ã€‚æœ¬èŠ‚ä»‹ç»å¦‚ä½•ä¿å­˜æ¨¡å‹æƒé‡ã€å®Œæ•´æ¨¡å‹ã€è®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œä»¥åŠå¦‚ä½•å¯¼å‡ºæ¨¡å‹ç”¨äºéƒ¨ç½²ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- æŒæ¡ä¿å­˜å’ŒåŠ è½½æ¨¡å‹çš„ä¸åŒæ–¹å¼
- ç†è§£ state_dict çš„ç»“æ„
- å®ç°å®Œæ•´çš„æ£€æŸ¥ç‚¹ç®¡ç†
- äº†è§£æ¨¡å‹å¯¼å‡ºï¼ˆONNXã€TorchScriptï¼‰

---

## 6.3.1 ä¿å­˜å’ŒåŠ è½½ state_dict

### ä»€ä¹ˆæ˜¯ state_dictï¼Ÿ

`state_dict` æ˜¯ä¸€ä¸ª Python å­—å…¸ï¼Œå°†æ¯ä¸ªå±‚æ˜ å°„åˆ°å…¶å‚æ•°å¼ é‡ã€‚

```python
import torch
import torch.nn as nn

# åˆ›å»ºç®€å•æ¨¡å‹
model = nn.Sequential(
    nn.Linear(10, 20),
    nn.ReLU(),
    nn.Linear(20, 5)
)

# æŸ¥çœ‹ state_dict
print("æ¨¡å‹çš„ state_dict:")
for name, param in model.state_dict().items():
    print(f"  {name}: {param.shape}")

# è¾“å‡ºï¼š
# æ¨¡å‹çš„ state_dict:
#   0.weight: torch.Size([20, 10])
#   0.bias: torch.Size([20])
#   2.weight: torch.Size([5, 20])
#   2.bias: torch.Size([5])
```

### æ¨èæ–¹å¼ï¼šåªä¿å­˜ state_dict

```python
# ä¿å­˜
torch.save(model.state_dict(), 'model_weights.pth')

# åŠ è½½
model = nn.Sequential(
    nn.Linear(10, 20),
    nn.ReLU(),
    nn.Linear(20, 5)
)
model.load_state_dict(torch.load('model_weights.pth'))
model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
```

### åŠ è½½åˆ°ä¸åŒè®¾å¤‡

```python
# ä¿å­˜ï¼ˆåœ¨ GPU ä¸Šè®­ç»ƒï¼‰
torch.save(model.state_dict(), 'model_weights.pth')

# åŠ è½½åˆ° CPU
model.load_state_dict(
    torch.load('model_weights.pth', map_location='cpu')
)

# åŠ è½½åˆ°æŒ‡å®š GPU
model.load_state_dict(
    torch.load('model_weights.pth', map_location='cuda:0')
)

# è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.load_state_dict(
    torch.load('model_weights.pth', map_location=device)
)
model.to(device)
```

---

## 6.3.2 ä¿å­˜å®Œæ•´æ¨¡å‹

### ä½¿ç”¨ pickle ä¿å­˜æ•´ä¸ªæ¨¡å‹

```python
# ä¿å­˜æ•´ä¸ªæ¨¡å‹ï¼ˆåŒ…æ‹¬ç»“æ„å’Œæƒé‡ï¼‰
torch.save(model, 'complete_model.pth')

# åŠ è½½
model = torch.load('complete_model.pth')
model.eval()
```

### âš ï¸ æ³¨æ„äº‹é¡¹

**ä¸æ¨èä¿å­˜å®Œæ•´æ¨¡å‹**ï¼Œå› ä¸ºï¼š

1. **ä¾èµ–åºåˆ—åŒ–**ï¼šæ¨¡å‹ç±»çš„å®šä¹‰å¿…é¡»å­˜åœ¨äºåŠ è½½ç¯å¢ƒä¸­
2. **å¯ç§»æ¤æ€§å·®**ï¼šä¸åŒ PyTorch ç‰ˆæœ¬å¯èƒ½ä¸å…¼å®¹
3. **æ–‡ä»¶æ›´å¤§**ï¼šåŒ…å«äº†æ¨¡å‹ç»“æ„ä¿¡æ¯

```python
# âŒ å¯èƒ½å‡ºé—®é¢˜çš„æƒ…å†µ
# ä¿å­˜æ—¶
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(10, 5)
    
    def forward(self, x):
        return self.fc(x)

model = MyModel()
torch.save(model, 'model.pth')

# åŠ è½½æ—¶ï¼Œå¦‚æœ MyModel ç±»çš„å®šä¹‰ä¸å­˜åœ¨æˆ–å·²æ›´æ”¹
model = torch.load('model.pth')  # å¯èƒ½å¤±è´¥ï¼
```

---

## 6.3.3 æ£€æŸ¥ç‚¹ï¼ˆCheckpointï¼‰

### å®Œæ•´æ£€æŸ¥ç‚¹åŒ…å«çš„å†…å®¹

```python
def save_checkpoint(model, optimizer, scheduler, epoch, loss, path):
    """ä¿å­˜å®Œæ•´æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'loss': loss,
        'rng_state': torch.get_rng_state(),
        'cuda_rng_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None
    }
    torch.save(checkpoint, path)
    print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {path}")


def load_checkpoint(path, model, optimizer, scheduler=None, device='cpu'):
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    checkpoint = torch.load(path, map_location=device)
    
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    if scheduler and checkpoint.get('scheduler_state_dict'):
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    # æ¢å¤éšæœºçŠ¶æ€ï¼ˆç¡®ä¿å¯é‡å¤æ€§ï¼‰
    if checkpoint.get('rng_state') is not None:
        torch.set_rng_state(checkpoint['rng_state'])
    if checkpoint.get('cuda_rng_state') is not None and torch.cuda.is_available():
        torch.cuda.set_rng_state(checkpoint['cuda_rng_state'])
    
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    
    print(f"ä» epoch {epoch} æ¢å¤è®­ç»ƒï¼ŒæŸå¤±: {loss:.4f}")
    
    return epoch, loss
```

### ä½¿ç”¨æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ

```python
import os

def train_with_checkpoint(model, train_loader, val_loader, criterion, 
                          optimizer, scheduler, num_epochs, 
                          checkpoint_dir='checkpoints', resume_from=None):
    """æ”¯æŒæ£€æŸ¥ç‚¹æ¢å¤çš„è®­ç»ƒ"""
    
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    start_epoch = 0
    best_val_loss = float('inf')
    
    # å°è¯•æ¢å¤
    if resume_from and os.path.exists(resume_from):
        start_epoch, _ = load_checkpoint(
            resume_from, model, optimizer, scheduler
        )
        start_epoch += 1  # ä»ä¸‹ä¸€ä¸ª epoch å¼€å§‹
    
    device = next(model.parameters()).device
    
    for epoch in range(start_epoch, num_epochs):
        # è®­ç»ƒ
        model.train()
        train_loss = 0.0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        train_loss /= len(train_loader)
        
        # éªŒè¯
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                val_loss += loss.item()
        val_loss /= len(val_loader)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if scheduler:
            scheduler.step()
        
        print(f"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_checkpoint(
                model, optimizer, scheduler, epoch, val_loss,
                os.path.join(checkpoint_dir, 'best_model.pth')
            )
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 10 == 0:
            save_checkpoint(
                model, optimizer, scheduler, epoch, val_loss,
                os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')
            )
```

---

## 6.3.4 å¤„ç†ä¸åŒ¹é…çš„ state_dict

### éƒ¨åˆ†åŠ è½½

```python
def load_partial_state_dict(model, state_dict, strict=False):
    """
    éƒ¨åˆ†åŠ è½½ state_dict
    
    ç”¨äºè¿ç§»å­¦ä¹ æˆ–æ¨¡å‹ç»“æ„å˜åŒ–çš„æƒ…å†µ
    """
    model_state = model.state_dict()
    
    # è¿‡æ»¤å‡ºåŒ¹é…çš„é”®
    pretrained_dict = {}
    for k, v in state_dict.items():
        if k in model_state:
            if v.shape == model_state[k].shape:
                pretrained_dict[k] = v
            else:
                print(f"è·³è¿‡ {k}: å½¢çŠ¶ä¸åŒ¹é… {v.shape} vs {model_state[k].shape}")
        else:
            print(f"è·³è¿‡ {k}: æ¨¡å‹ä¸­ä¸å­˜åœ¨")
    
    # æ›´æ–° state_dict
    model_state.update(pretrained_dict)
    model.load_state_dict(model_state)
    
    print(f"åŠ è½½äº† {len(pretrained_dict)}/{len(state_dict)} ä¸ªå‚æ•°")


# ä½¿ç”¨
pretrained_state = torch.load('pretrained_model.pth')
load_partial_state_dict(model, pretrained_state)
```

### é‡å‘½åé”®

```python
def rename_state_dict_keys(state_dict, key_mapping):
    """
    é‡å‘½å state_dict ä¸­çš„é”®
    
    Args:
        state_dict: åŸå§‹ state_dict
        key_mapping: {old_key: new_key} çš„å­—å…¸
    """
    new_state_dict = {}
    for k, v in state_dict.items():
        new_key = key_mapping.get(k, k)
        new_state_dict[new_key] = v
    return new_state_dict


# ç¤ºä¾‹ï¼šä»æ—§æ¨¡å‹è¿ç§»åˆ°æ–°æ¨¡å‹
key_mapping = {
    'fc1.weight': 'encoder.fc1.weight',
    'fc1.bias': 'encoder.fc1.bias',
    'fc2.weight': 'decoder.fc2.weight',
    'fc2.bias': 'decoder.fc2.bias',
}

old_state = torch.load('old_model.pth')
new_state = rename_state_dict_keys(old_state, key_mapping)
model.load_state_dict(new_state)
```

---

## 6.3.5 DataParallel æ¨¡å‹çš„ä¿å­˜ä¸åŠ è½½

```python
# DataParallel ä¼šç»™æ‰€æœ‰é”®æ·»åŠ  'module.' å‰ç¼€

# ä¿å­˜ DataParallel æ¨¡å‹
model = nn.DataParallel(model)
torch.save(model.module.state_dict(), 'model.pth')  # ä¿å­˜ .module

# æˆ–è€…ä¿å­˜æ•´ä¸ª state_dictï¼ŒåŠ è½½æ—¶å¤„ç†å‰ç¼€
torch.save(model.state_dict(), 'model_dp.pth')

# åŠ è½½æ—¶å»é™¤ 'module.' å‰ç¼€
def remove_module_prefix(state_dict):
    """ç§»é™¤ DataParallel çš„ 'module.' å‰ç¼€"""
    new_state_dict = {}
    for k, v in state_dict.items():
        if k.startswith('module.'):
            new_state_dict[k[7:]] = v  # ç§»é™¤å‰7ä¸ªå­—ç¬¦
        else:
            new_state_dict[k] = v
    return new_state_dict


state_dict = torch.load('model_dp.pth')
state_dict = remove_module_prefix(state_dict)
model.load_state_dict(state_dict)
```

---

## 6.3.6 TorchScript å¯¼å‡º

TorchScript å¯ä»¥å°† PyTorch æ¨¡å‹åºåˆ—åŒ–ä¸ºå¯ç‹¬ç«‹è¿è¡Œçš„æ ¼å¼ã€‚

### Tracing

```python
import torch.jit

# å‡†å¤‡ç¤ºä¾‹è¾“å…¥
example_input = torch.randn(1, 3, 224, 224)

# è·Ÿè¸ªæ¨¡å‹
model.eval()
traced_model = torch.jit.trace(model, example_input)

# ä¿å­˜
traced_model.save('model_traced.pt')

# åŠ è½½
loaded_model = torch.jit.load('model_traced.pt')
output = loaded_model(example_input)
```

### Scripting

```python
# å¯¹äºåŒ…å«æ§åˆ¶æµçš„æ¨¡å‹ï¼Œä½¿ç”¨ script
class ConditionalModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 20)
        self.fc2 = nn.Linear(20, 5)
    
    def forward(self, x, use_relu=True):
        x = self.fc1(x)
        if use_relu:
            x = torch.relu(x)
        return self.fc2(x)

model = ConditionalModel()
scripted_model = torch.jit.script(model)

scripted_model.save('model_scripted.pt')
```

### TorchScript çš„ä¼˜åŠ¿

1. **ç‹¬ç«‹äº Python**ï¼šå¯ä»¥åœ¨ C++ ç¯å¢ƒä¸­è¿è¡Œ
2. **ä¼˜åŒ–**ï¼šJIT ç¼–è¯‘å™¨å¯ä»¥è¿›è¡Œä¼˜åŒ–
3. **å¯ç§»æ¤**ï¼šæ— éœ€åŸå§‹æ¨¡å‹å®šä¹‰
4. **ç”Ÿäº§éƒ¨ç½²**ï¼šé€‚åˆæœåŠ¡ç«¯éƒ¨ç½²

---

## 6.3.7 ONNX å¯¼å‡º

ONNXï¼ˆOpen Neural Network Exchangeï¼‰æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æ¨¡å‹æ ¼å¼ã€‚

```python
import torch.onnx

# å‡†å¤‡æ¨¡å‹å’Œç¤ºä¾‹è¾“å…¥
model.eval()
example_input = torch.randn(1, 3, 224, 224)

# å¯¼å‡ºåˆ° ONNX
torch.onnx.export(
    model,                          # æ¨¡å‹
    example_input,                  # ç¤ºä¾‹è¾“å…¥
    'model.onnx',                   # è¾“å‡ºæ–‡ä»¶
    export_params=True,             # å¯¼å‡ºå‚æ•°
    opset_version=11,               # ONNX ç®—å­ç‰ˆæœ¬
    do_constant_folding=True,       # å¸¸é‡æŠ˜å ä¼˜åŒ–
    input_names=['input'],          # è¾“å…¥åç§°
    output_names=['output'],        # è¾“å‡ºåç§°
    dynamic_axes={                  # åŠ¨æ€è½´ï¼ˆæ”¯æŒå¯å˜ batch sizeï¼‰
        'input': {0: 'batch_size'},
        'output': {0: 'batch_size'}
    }
)

print("æ¨¡å‹å·²å¯¼å‡ºä¸º ONNX æ ¼å¼")
```

### éªŒè¯ ONNX æ¨¡å‹

```python
import onnx
import onnxruntime as ort
import numpy as np

# éªŒè¯æ¨¡å‹
onnx_model = onnx.load('model.onnx')
onnx.checker.check_model(onnx_model)
print("ONNX æ¨¡å‹éªŒè¯é€šè¿‡")

# ä½¿ç”¨ ONNX Runtime æ¨ç†
ort_session = ort.InferenceSession('model.onnx')

# å‡†å¤‡è¾“å…¥
input_name = ort_session.get_inputs()[0].name
input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)

# æ¨ç†
output = ort_session.run(None, {input_name: input_data})
print(f"è¾“å‡ºå½¢çŠ¶: {output[0].shape}")
```

---

## 6.3.8 æ£€æŸ¥ç‚¹ç®¡ç†å™¨

```python
import os
import glob
from datetime import datetime

class CheckpointManager:
    """æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, checkpoint_dir, max_to_keep=5):
        """
        Args:
            checkpoint_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
            max_to_keep: æœ€å¤šä¿ç•™çš„æ£€æŸ¥ç‚¹æ•°é‡
        """
        self.checkpoint_dir = checkpoint_dir
        self.max_to_keep = max_to_keep
        os.makedirs(checkpoint_dir, exist_ok=True)
    
    def save(self, model, optimizer, scheduler, epoch, metrics, 
             is_best=False, filename=None):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'metrics': metrics,
            'timestamp': datetime.now().isoformat()
        }
        
        # ç”Ÿæˆæ–‡ä»¶å
        if filename is None:
            filename = f'checkpoint_epoch_{epoch:04d}.pth'
        
        path = os.path.join(self.checkpoint_dir, filename)
        torch.save(checkpoint, path)
        print(f"ä¿å­˜æ£€æŸ¥ç‚¹: {path}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.checkpoint_dir, 'best_model.pth')
            torch.save(checkpoint, best_path)
            print(f"æ›´æ–°æœ€ä½³æ¨¡å‹: {best_path}")
        
        # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
        self._cleanup()
        
        return path
    
    def load(self, path, model, optimizer=None, scheduler=None, device='cpu'):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        
        checkpoint = torch.load(path, map_location=device)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        
        if optimizer and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if scheduler and checkpoint.get('scheduler_state_dict'):
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        print(f"åŠ è½½æ£€æŸ¥ç‚¹: {path}")
        print(f"  Epoch: {checkpoint['epoch']}")
        print(f"  Metrics: {checkpoint.get('metrics', 'N/A')}")
        
        return checkpoint
    
    def load_best(self, model, optimizer=None, scheduler=None, device='cpu'):
        """åŠ è½½æœ€ä½³æ¨¡å‹"""
        best_path = os.path.join(self.checkpoint_dir, 'best_model.pth')
        if os.path.exists(best_path):
            return self.load(best_path, model, optimizer, scheduler, device)
        else:
            raise FileNotFoundError("æœ€ä½³æ¨¡å‹ä¸å­˜åœ¨")
    
    def load_latest(self, model, optimizer=None, scheduler=None, device='cpu'):
        """åŠ è½½æœ€æ–°æ£€æŸ¥ç‚¹"""
        checkpoints = glob.glob(
            os.path.join(self.checkpoint_dir, 'checkpoint_epoch_*.pth')
        )
        if not checkpoints:
            raise FileNotFoundError("æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹")
        
        latest = max(checkpoints, key=os.path.getctime)
        return self.load(latest, model, optimizer, scheduler, device)
    
    def _cleanup(self):
        """æ¸…ç†æ—§æ£€æŸ¥ç‚¹ï¼Œåªä¿ç•™æœ€æ–°çš„ max_to_keep ä¸ª"""
        checkpoints = glob.glob(
            os.path.join(self.checkpoint_dir, 'checkpoint_epoch_*.pth')
        )
        
        if len(checkpoints) > self.max_to_keep:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
            checkpoints.sort(key=os.path.getctime)
            
            # åˆ é™¤æœ€æ—§çš„
            for ckpt in checkpoints[:-self.max_to_keep]:
                os.remove(ckpt)
                print(f"åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {ckpt}")
    
    def list_checkpoints(self):
        """åˆ—å‡ºæ‰€æœ‰æ£€æŸ¥ç‚¹"""
        checkpoints = glob.glob(
            os.path.join(self.checkpoint_dir, '*.pth')
        )
        for ckpt in sorted(checkpoints):
            info = torch.load(ckpt, map_location='cpu')
            print(f"{os.path.basename(ckpt)}:")
            print(f"  Epoch: {info.get('epoch', 'N/A')}")
            print(f"  Timestamp: {info.get('timestamp', 'N/A')}")


# ä½¿ç”¨ç¤ºä¾‹
manager = CheckpointManager('checkpoints', max_to_keep=5)

# è®­ç»ƒä¸­ä¿å­˜
for epoch in range(100):
    train_loss = train_one_epoch()
    val_loss = validate()
    
    is_best = val_loss < best_val_loss
    if is_best:
        best_val_loss = val_loss
    
    manager.save(
        model, optimizer, scheduler, epoch,
        metrics={'train_loss': train_loss, 'val_loss': val_loss},
        is_best=is_best
    )

# æ¢å¤è®­ç»ƒ
checkpoint = manager.load_latest(model, optimizer, scheduler)
start_epoch = checkpoint['epoch'] + 1
```

---

## 6.3.9 å®‰å…¨ä¿å­˜ï¼ˆé˜²æ­¢æŸåï¼‰

```python
import shutil

def safe_save(obj, path):
    """
    å®‰å…¨ä¿å­˜ï¼Œé˜²æ­¢åœ¨ä¿å­˜è¿‡ç¨‹ä¸­ä¸­æ–­å¯¼è‡´æ–‡ä»¶æŸå
    """
    temp_path = path + '.tmp'
    
    # å…ˆä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    torch.save(obj, temp_path)
    
    # å¦‚æœæˆåŠŸï¼Œæ›¿æ¢åŸæ–‡ä»¶
    shutil.move(temp_path, path)


def save_with_backup(obj, path):
    """
    ä¿å­˜å¹¶ä¿ç•™å¤‡ä»½
    """
    if os.path.exists(path):
        backup_path = path + '.backup'
        shutil.copy(path, backup_path)
    
    safe_save(obj, path)
```

---

## ğŸ”¬ ç‰©ç†è§†è§’æ€»ç»“

### æ£€æŸ¥ç‚¹çš„æ„ä¹‰

æ£€æŸ¥ç‚¹å¯ä»¥ç±»æ¯”äºç‰©ç†æ¨¡æ‹Ÿä¸­çš„**çŠ¶æ€å¿«ç…§**ï¼š

- ä¿å­˜ç³»ç»Ÿçš„å®Œæ•´çŠ¶æ€ï¼ˆä½ç½®ã€é€Ÿåº¦ç­‰ï¼‰
- å¯ä»¥ä»ä»»æ„æ—¶åˆ»æ¢å¤æ¨¡æ‹Ÿ
- ä¾¿äºåˆ†æä¸­é—´çŠ¶æ€

### æ¨¡å‹å¯¼å‡º

å°†æ¨¡å‹å¯¼å‡ºï¼ˆTorchScriptã€ONNXï¼‰ç±»ä¼¼äºï¼š

- å°†æ•°å€¼æ¨¡æ‹Ÿä»£ç ç¼–è¯‘ä¸ºå¯æ‰§è¡Œç¨‹åº
- å¯ä»¥åœ¨ä¸åŒå¹³å°è¿è¡Œ
- ä¼˜åŒ–åè¿è¡Œæ›´å¿«

---

## ğŸ“ ç»ƒä¹ 

1. å®ç°ä¸€ä¸ªå®Œæ•´çš„æ£€æŸ¥ç‚¹ç®¡ç†ç³»ç»Ÿ
2. å°†è®­ç»ƒå¥½çš„æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼å¹¶éªŒè¯
3. å®ç°ä»é¢„è®­ç»ƒæ¨¡å‹éƒ¨åˆ†åŠ è½½æƒé‡çš„åŠŸèƒ½

---

## â­ï¸ ä¸‹ä¸€èŠ‚

ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†å­¦ä¹  [åˆ†å¸ƒå¼è®­ç»ƒ](./04_distributed_training.md)ï¼Œäº†è§£å¦‚ä½•åœ¨å¤š GPU ä¸Šè®­ç»ƒæ¨¡å‹ã€‚

