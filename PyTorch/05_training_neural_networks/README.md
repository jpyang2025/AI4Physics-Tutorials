# 第5章：训练神经网络

## 📖 章节概述

前几章我们学习了如何构建神经网络，本章将介绍如何**训练**这些网络。训练是深度学习的核心环节，涉及数据加载、优化算法、训练循环等多个方面。

作为物理科研人员，你可以将训练过程理解为在高维参数空间中寻找能量最小点的过程。

## 🎯 学习目标

完成本章学习后，你将能够：

1. 使用 `Dataset` 和 `DataLoader` 高效加载数据
2. 理解并选择合适的优化器
3. 编写标准的训练循环
4. 实现模型验证和测试
5. 监控训练过程并进行调试

## 📚 章节内容

| 节 | 文件 | 主题 | 预计时间 |
|---|------|------|---------|
| 5.1 | [01_data_loading.md](./01_data_loading.md) | 数据加载 | 45 分钟 |
| 5.2 | [02_optimizers.md](./02_optimizers.md) | 优化器 | 40 分钟 |
| 5.3 | [03_training_loop.md](./03_training_loop.md) | 训练循环 | 50 分钟 |
| 5.4 | [04_validation_testing.md](./04_validation_testing.md) | 验证与测试 | 40 分钟 |

## 💻 示例代码

所有可运行的示例代码位于 `examples/` 目录：

- [`mnist_training.py`](./examples/mnist_training.py) - MNIST 分类完整训练流程
- [`regression_training.py`](./examples/regression_training.py) - 回归问题训练示例

## 🔬 物理视角：训练作为动力学过程

### 梯度下降的物理直觉

训练神经网络可以看作是在**参数空间**中的粒子动力学：

$$\frac{d\theta}{dt} = -\nabla_\theta L$$

这是一个在损失函数"势能面"上的过阻尼运动方程。

| 物理概念 | 机器学习对应 |
|---------|-------------|
| 位置 $\mathbf{r}$ | 参数 $\theta$ |
| 势能 $U(\mathbf{r})$ | 损失函数 $L(\theta)$ |
| 力 $-\nabla U$ | 负梯度 $-\nabla L$ |
| 时间步长 $\Delta t$ | 学习率 $\eta$ |
| 热涨落 | 随机梯度噪声 |

### 不同优化器的物理类比

| 优化器 | 物理系统 |
|--------|---------|
| SGD | 过阻尼朗之万动力学 |
| SGD + Momentum | 欠阻尼振子 |
| Adam | 自适应步长积分器 |
| 学习率退火 | 模拟退火 |

### 小批量训练的统计力学

使用小批量（mini-batch）而非全部数据相当于在损失函数中引入噪声：

$$\nabla L_{\text{batch}} = \nabla L_{\text{true}} + \xi$$

其中 $\xi$ 是采样噪声。这类似于有限温度下的朗之万动力学，有助于跳出局部极小值。

## 📋 前置要求

- 完成第1-4章的学习
- 理解损失函数和梯度的概念
- 了解基本的统计学概念

## 🚀 快速开始

```bash
# 运行示例
cd 05_training_neural_networks/examples
python mnist_training.py
python regression_training.py
```

## 🔄 训练流程概览

```
┌─────────────────────────────────────────────────────────┐
│                      训练流程                           │
├─────────────────────────────────────────────────────────┤
│                                                         │
│   数据准备                                              │
│   ├── Dataset: 定义数据源                               │
│   └── DataLoader: 批量化、打乱、并行加载                │
│                                                         │
│   模型与优化器                                          │
│   ├── Model: 定义网络结构                               │
│   ├── Loss: 定义损失函数                                │
│   └── Optimizer: 定义优化算法                           │
│                                                         │
│   训练循环 (每个 epoch)                                 │
│   ├── for batch in dataloader:                         │
│   │   ├── 前向传播: output = model(input)              │
│   │   ├── 计算损失: loss = criterion(output, target)   │
│   │   ├── 反向传播: loss.backward()                    │
│   │   └── 更新参数: optimizer.step()                   │
│   │                                                     │
│   └── 验证评估                                          │
│                                                         │
│   保存模型                                              │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

## ⏭️ 下一章预告

掌握了基本训练流程后，第6章将介绍进阶技术，包括正则化、学习率调度、模型保存与加载等。

---

*预计总学习时间：约 3 小时*

