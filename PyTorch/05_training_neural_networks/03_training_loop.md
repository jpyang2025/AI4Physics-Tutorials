# 5.3 è®­ç»ƒå¾ªç¯

## ğŸ“– æ¦‚è¿°

è®­ç»ƒå¾ªç¯æ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæµç¨‹ï¼Œå°†æ•°æ®åŠ è½½ã€å‰å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°æ•´åˆåœ¨ä¸€èµ·ã€‚æœ¬èŠ‚ä»‹ç»å¦‚ä½•ç¼–å†™é«˜æ•ˆã€å¯é çš„è®­ç»ƒå¾ªç¯ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- æŒæ¡æ ‡å‡†è®­ç»ƒå¾ªç¯çš„ç»“æ„
- ç†è§£è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®æ­¥éª¤
- å®ç°è®­ç»ƒæ—¥å¿—å’Œè¿›åº¦ç›‘æ§
- å¤„ç†å¸¸è§çš„è®­ç»ƒé—®é¢˜

---

## 5.3.1 åŸºæœ¬è®­ç»ƒå¾ªç¯

### æœ€ç®€è®­ç»ƒå¾ªç¯

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

def basic_training_loop(model, train_loader, criterion, optimizer, num_epochs):
    """
    æœ€åŸºæœ¬çš„è®­ç»ƒå¾ªç¯
    
    Args:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        num_epochs: è®­ç»ƒè½®æ•°
    """
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            # 1. æ¸…ç©ºæ¢¯åº¦
            optimizer.zero_grad()
            
            # 2. å‰å‘ä¼ æ’­
            outputs = model(inputs)
            
            # 3. è®¡ç®—æŸå¤±
            loss = criterion(outputs, targets)
            
            # 4. åå‘ä¼ æ’­
            loss.backward()
            
            # 5. æ›´æ–°å‚æ•°
            optimizer.step()
            
            epoch_loss += loss.item()
        
        avg_loss = epoch_loss / len(train_loader)
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}")
```

### è®­ç»ƒå¾ªç¯è¯¦è§£

```
è®­ç»ƒä¸€ä¸ª Epoch çš„æµç¨‹ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ for batch in dataloader:                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  â‘  optimizer.zero_grad()                                  â”‚
â”‚     â””â”€ æ¸…ç©ºä¸Šä¸€æ­¥çš„æ¢¯åº¦ï¼Œé˜²æ­¢ç´¯ç§¯                          â”‚
â”‚                                                            â”‚
â”‚  â‘¡ outputs = model(inputs)                                â”‚
â”‚     â””â”€ å‰å‘ä¼ æ’­ï¼šè®¡ç®—é¢„æµ‹å€¼                                â”‚
â”‚                                                            â”‚
â”‚  â‘¢ loss = criterion(outputs, targets)                     â”‚
â”‚     â””â”€ è®¡ç®—æŸå¤±ï¼šæ¯”è¾ƒé¢„æµ‹ä¸çœŸå®å€¼                          â”‚
â”‚                                                            â”‚
â”‚  â‘£ loss.backward()                                        â”‚
â”‚     â””â”€ åå‘ä¼ æ’­ï¼šè®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦                        â”‚
â”‚                                                            â”‚
â”‚  â‘¤ optimizer.step()                                       â”‚
â”‚     â””â”€ å‚æ•°æ›´æ–°ï¼šæ ¹æ®æ¢¯åº¦æ›´æ–°æƒé‡                          â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.3.2 å®Œæ•´è®­ç»ƒæµç¨‹

### åŒ…å«éªŒè¯çš„è®­ç»ƒå¾ªç¯

```python
def train_with_validation(model, train_loader, val_loader, 
                         criterion, optimizer, num_epochs, device='cpu'):
    """
    åŒ…å«éªŒè¯çš„å®Œæ•´è®­ç»ƒæµç¨‹
    """
    model = model.to(device)
    
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': []
    }
    
    best_val_loss = float('inf')
    
    for epoch in range(num_epochs):
        # ========== è®­ç»ƒé˜¶æ®µ ==========
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item() * inputs.size(0)
            _, predicted = outputs.max(1)
            train_total += targets.size(0)
            train_correct += predicted.eq(targets).sum().item()
        
        train_loss /= train_total
        train_acc = 100. * train_correct / train_total
        
        # ========== éªŒè¯é˜¶æ®µ ==========
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                
                val_loss += loss.item() * inputs.size(0)
                _, predicted = outputs.max(1)
                val_total += targets.size(0)
                val_correct += predicted.eq(targets).sum().item()
        
        val_loss /= val_total
        val_acc = 100. * val_correct / val_total
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'best_model.pth')
        
        print(f"Epoch [{epoch+1}/{num_epochs}]")
        print(f"  Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
        print(f"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")
    
    return history
```

---

## 5.3.3 GPU è®­ç»ƒ

### è®¾å¤‡ç®¡ç†

```python
# æ£€æµ‹å¯ç”¨è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# å¦‚æœæœ‰å¤šä¸ª GPU
if torch.cuda.is_available():
    print(f"å¯ç”¨ GPU æ•°é‡: {torch.cuda.device_count()}")
    print(f"å½“å‰ GPU: {torch.cuda.get_device_name(0)}")
```

### GPU è®­ç»ƒå¾ªç¯

```python
def gpu_training_loop(model, train_loader, criterion, optimizer, 
                      num_epochs, device):
    """GPU è®­ç»ƒå¾ªç¯"""
    
    # æ¨¡å‹ç§»åŠ¨åˆ° GPU
    model = model.to(device)
    
    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0.0
        
        for inputs, targets in train_loader:
            # æ•°æ®ç§»åŠ¨åˆ° GPU
            inputs = inputs.to(device)
            targets = targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        print(f"Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader):.4f}")
```

### æ··åˆç²¾åº¦è®­ç»ƒ

ä½¿ç”¨ AMPï¼ˆAutomatic Mixed Precisionï¼‰åŠ é€Ÿè®­ç»ƒå¹¶å‡å°‘æ˜¾å­˜å ç”¨ã€‚

```python
from torch.cuda.amp import autocast, GradScaler

def mixed_precision_training(model, train_loader, criterion, optimizer, 
                             num_epochs, device):
    """æ··åˆç²¾åº¦è®­ç»ƒ"""
    
    model = model.to(device)
    scaler = GradScaler()  # æ¢¯åº¦ç¼©æ”¾å™¨
    
    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0.0
        
        for inputs, targets in train_loader:
            inputs = inputs.to(device)
            targets = targets.to(device)
            
            optimizer.zero_grad()
            
            # è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
            with autocast():
                outputs = model(inputs)
                loss = criterion(outputs, targets)
            
            # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            
            # æ›´æ–°å‚æ•°ï¼ˆè‡ªåŠ¨å¤„ç†æ¢¯åº¦ç¼©æ”¾ï¼‰
            scaler.step(optimizer)
            scaler.update()
            
            epoch_loss += loss.item()
        
        print(f"Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader):.4f}")
```

---

## 5.3.4 è¿›åº¦ç›‘æ§

### ä½¿ç”¨ tqdm è¿›åº¦æ¡

```python
from tqdm import tqdm

def training_with_progress(model, train_loader, criterion, optimizer, 
                          num_epochs, device):
    """å¸¦è¿›åº¦æ¡çš„è®­ç»ƒ"""
    
    model = model.to(device)
    
    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0.0
        
        # åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')
        
        for inputs, targets in pbar:
            inputs = inputs.to(device)
            targets = targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            
            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_loss = epoch_loss / len(train_loader)
        print(f"Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")
```

### TensorBoard æ—¥å¿—

```python
from torch.utils.tensorboard import SummaryWriter

def training_with_tensorboard(model, train_loader, val_loader,
                              criterion, optimizer, num_epochs, device):
    """ä½¿ç”¨ TensorBoard è®°å½•è®­ç»ƒè¿‡ç¨‹"""
    
    writer = SummaryWriter('runs/experiment_1')
    model = model.to(device)
    
    global_step = 0
    
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        
        for inputs, targets in train_loader:
            inputs = inputs.to(device)
            targets = targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            
            # è®°å½•æ¯æ­¥æŸå¤±
            writer.add_scalar('Loss/train_step', loss.item(), global_step)
            global_step += 1
        
        # è®°å½•æ¯ä¸ª epoch çš„å¹³å‡æŸå¤±
        avg_train_loss = train_loss / len(train_loader)
        writer.add_scalar('Loss/train_epoch', avg_train_loss, epoch)
        
        # éªŒè¯
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs = inputs.to(device)
                targets = targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        writer.add_scalar('Loss/val_epoch', avg_val_loss, epoch)
        
        # è®°å½•å­¦ä¹ ç‡
        writer.add_scalar('Learning_rate', 
                         optimizer.param_groups[0]['lr'], epoch)
        
        # è®°å½•æ¨¡å‹å‚æ•°ç›´æ–¹å›¾
        for name, param in model.named_parameters():
            writer.add_histogram(f'Parameters/{name}', param, epoch)
            if param.grad is not None:
                writer.add_histogram(f'Gradients/{name}', param.grad, epoch)
        
        print(f"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, "
              f"Val Loss = {avg_val_loss:.4f}")
    
    writer.close()

# å¯åŠ¨ TensorBoard: tensorboard --logdir=runs
```

---

## 5.3.5 è®­ç»ƒæŠ€å·§

### æ—©åœï¼ˆEarly Stoppingï¼‰

```python
class EarlyStopping:
    """æ—©åœæœºåˆ¶ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ"""
    
    def __init__(self, patience=10, min_delta=0.0, mode='min'):
        """
        Args:
            patience: ç­‰å¾…æ”¹å–„çš„è½®æ•°
            min_delta: æœ€å°æ”¹å–„é‡
            mode: 'min' æˆ– 'max'
        """
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False
    
    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
        elif self._is_improvement(score):
            self.best_score = score
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        
        return self.early_stop
    
    def _is_improvement(self, score):
        if self.mode == 'min':
            return score < self.best_score - self.min_delta
        else:
            return score > self.best_score + self.min_delta


# ä½¿ç”¨æ—©åœ
early_stopping = EarlyStopping(patience=10)

for epoch in range(num_epochs):
    # è®­ç»ƒ...
    val_loss = validate(model, val_loader)
    
    if early_stopping(val_loss):
        print(f"æ—©åœäº Epoch {epoch+1}")
        break
```

### æ¨¡å‹æ£€æŸ¥ç‚¹

```python
class ModelCheckpoint:
    """æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä¿å­˜æœ€ä½³æ¨¡å‹"""
    
    def __init__(self, filepath, monitor='val_loss', mode='min'):
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.best_score = float('inf') if mode == 'min' else float('-inf')
    
    def __call__(self, score, model, optimizer=None, epoch=None):
        if self._is_improvement(score):
            self.best_score = score
            self._save_checkpoint(model, optimizer, epoch, score)
            return True
        return False
    
    def _is_improvement(self, score):
        if self.mode == 'min':
            return score < self.best_score
        return score > self.best_score
    
    def _save_checkpoint(self, model, optimizer, epoch, score):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'score': score
        }
        if optimizer is not None:
            checkpoint['optimizer_state_dict'] = optimizer.state_dict()
        
        torch.save(checkpoint, self.filepath)
        print(f"âœ“ ä¿å­˜æ£€æŸ¥ç‚¹: {self.monitor}={score:.4f}")


# ä½¿ç”¨æ£€æŸ¥ç‚¹
checkpoint = ModelCheckpoint('best_model.pth', monitor='val_loss')

for epoch in range(num_epochs):
    train_loss = train_one_epoch(model, train_loader)
    val_loss = validate(model, val_loader)
    
    checkpoint(val_loss, model, optimizer, epoch)
```

### æ¢¯åº¦ç›‘æ§

```python
def monitor_gradients(model, log_interval=100):
    """æ¢¯åº¦ç›‘æ§è£…é¥°å™¨"""
    
    gradient_norms = []
    
    def hook(grad):
        gradient_norms.append(grad.norm().item())
    
    # æ³¨å†Œé’©å­
    hooks = []
    for param in model.parameters():
        if param.requires_grad:
            hooks.append(param.register_hook(hook))
    
    return hooks, gradient_norms


# è®­ç»ƒä¸­ç›‘æ§æ¢¯åº¦
def train_with_gradient_monitoring(model, train_loader, criterion, 
                                   optimizer, device):
    """å¸¦æ¢¯åº¦ç›‘æ§çš„è®­ç»ƒ"""
    
    for batch_idx, (inputs, targets) in enumerate(train_loader):
        inputs = inputs.to(device)
        targets = targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        total_norm = 0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** 0.5
        
        if total_norm > 100:
            print(f"âš ï¸ è­¦å‘Š: æ¢¯åº¦èŒƒæ•°è¿‡å¤§ ({total_norm:.2f})")
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
```

---

## 5.3.6 å®Œæ•´è®­ç»ƒå™¨ç±»

```python
class Trainer:
    """é€šç”¨è®­ç»ƒå™¨ç±»"""
    
    def __init__(self, model, criterion, optimizer, device='cpu',
                 scheduler=None, early_stopping=None, checkpoint=None):
        self.model = model.to(device)
        self.criterion = criterion
        self.optimizer = optimizer
        self.device = device
        self.scheduler = scheduler
        self.early_stopping = early_stopping
        self.checkpoint = checkpoint
        
        self.history = {
            'train_loss': [], 'val_loss': [],
            'train_acc': [], 'val_acc': []
        }
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ª epoch"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for inputs, targets in train_loader:
            inputs = inputs.to(self.device)
            targets = targets.to(self.device)
            
            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item() * inputs.size(0)
            
            # è®¡ç®—å‡†ç¡®ç‡ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
            if outputs.dim() > 1 and outputs.size(1) > 1:
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        avg_loss = total_loss / len(train_loader.dataset)
        accuracy = 100. * correct / total if total > 0 else 0
        
        return avg_loss, accuracy
    
    @torch.no_grad()
    def validate(self, val_loader):
        """éªŒè¯"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for inputs, targets in val_loader:
            inputs = inputs.to(self.device)
            targets = targets.to(self.device)
            
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)
            
            total_loss += loss.item() * inputs.size(0)
            
            if outputs.dim() > 1 and outputs.size(1) > 1:
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        avg_loss = total_loss / len(val_loader.dataset)
        accuracy = 100. * correct / total if total > 0 else 0
        
        return avg_loss, accuracy
    
    def fit(self, train_loader, val_loader, num_epochs):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        
        for epoch in range(num_epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc = self.validate(val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler is not None:
                if isinstance(self.scheduler, 
                             torch.optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()
            
            # è®°å½•å†å²
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_acc'].append(val_acc)
            
            # æ‰“å°è¿›åº¦
            lr = self.optimizer.param_groups[0]['lr']
            print(f"Epoch [{epoch+1}/{num_epochs}] "
                  f"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | "
                  f"Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}% | "
                  f"LR: {lr:.2e}")
            
            # æ£€æŸ¥ç‚¹
            if self.checkpoint is not None:
                self.checkpoint(val_loss, self.model, self.optimizer, epoch)
            
            # æ—©åœ
            if self.early_stopping is not None:
                if self.early_stopping(val_loss):
                    print(f"æ—©åœäº Epoch {epoch+1}")
                    break
        
        return self.history
    
    def plot_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))
        
        # æŸå¤±æ›²çº¿
        axes[0].plot(self.history['train_loss'], label='Train')
        axes[0].plot(self.history['val_loss'], label='Val')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].legend()
        axes[0].set_title('Loss Curve')
        
        # å‡†ç¡®ç‡æ›²çº¿
        axes[1].plot(self.history['train_acc'], label='Train')
        axes[1].plot(self.history['val_acc'], label='Val')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Accuracy (%)')
        axes[1].legend()
        axes[1].set_title('Accuracy Curve')
        
        plt.tight_layout()
        plt.show()


# ä½¿ç”¨ç¤ºä¾‹
trainer = Trainer(
    model=model,
    criterion=nn.CrossEntropyLoss(),
    optimizer=optim.Adam(model.parameters(), lr=0.001),
    device=device,
    early_stopping=EarlyStopping(patience=10),
    checkpoint=ModelCheckpoint('best_model.pth')
)

history = trainer.fit(train_loader, val_loader, num_epochs=100)
trainer.plot_history()
```

---

## 5.3.7 ç‰©ç†é—®é¢˜è®­ç»ƒç¤ºä¾‹

### è®­ç»ƒç¥ç»ç½‘ç»œæ±‚è§£å¾®åˆ†æ–¹ç¨‹

```python
def train_physics_network():
    """
    è®­ç»ƒç¥ç»ç½‘ç»œæ±‚è§£å¸¸å¾®åˆ†æ–¹ç¨‹
    
    é—®é¢˜ï¼šy'' + y = 0
    è¾¹ç•Œæ¡ä»¶ï¼šy(0) = 0, y(Ï€) = 0
    è§£æè§£ï¼šy(x) = A*sin(x)
    """
    
    class PhysicsNet(nn.Module):
        """æ»¡è¶³è¾¹ç•Œæ¡ä»¶çš„ç½‘ç»œ"""
        def __init__(self):
            super().__init__()
            self.net = nn.Sequential(
                nn.Linear(1, 64),
                nn.Tanh(),
                nn.Linear(64, 64),
                nn.Tanh(),
                nn.Linear(64, 1)
            )
        
        def forward(self, x):
            # è‡ªåŠ¨æ»¡è¶³è¾¹ç•Œæ¡ä»¶ y(0)=0, y(Ï€)=0
            # y = x(Ï€-x) * NN(x)
            return x * (torch.pi - x) * self.net(x)
    
    model = PhysicsNet()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    # å†…éƒ¨é…ç‚¹
    x_interior = torch.linspace(0.01, torch.pi - 0.01, 100, 
                                 requires_grad=True).reshape(-1, 1)
    
    for epoch in range(5000):
        optimizer.zero_grad()
        
        # è®¡ç®— y, y', y''
        y = model(x_interior)
        
        y_x = torch.autograd.grad(
            y, x_interior, 
            grad_outputs=torch.ones_like(y),
            create_graph=True
        )[0]
        
        y_xx = torch.autograd.grad(
            y_x, x_interior,
            grad_outputs=torch.ones_like(y_x),
            create_graph=True
        )[0]
        
        # æ®‹å·®ï¼šy'' + y = 0
        residual = y_xx + y
        loss = (residual ** 2).mean()
        
        loss.backward()
        optimizer.step()
        
        if epoch % 1000 == 0:
            print(f"Epoch {epoch}, Residual Loss: {loss.item():.6f}")
    
    return model


# è®­ç»ƒ
model = train_physics_network()

# éªŒè¯
x_test = torch.linspace(0, torch.pi, 100).reshape(-1, 1)
with torch.no_grad():
    y_pred = model(x_test)

# å½’ä¸€åŒ–åæ¯”è¾ƒ
y_pred_normalized = y_pred / y_pred.max()
y_exact = torch.sin(x_test)
print(f"æœ€å¤§è¯¯å·®: {(y_pred_normalized - y_exact).abs().max().item():.6f}")
```

---

## ğŸ”¬ ç‰©ç†è§†è§’æ€»ç»“

### è®­ç»ƒä½œä¸ºåŠ¨åŠ›å­¦æ¼”åŒ–

è®­ç»ƒè¿‡ç¨‹å¯ä»¥çœ‹ä½œå‚æ•°ç©ºé—´ä¸­çš„åŠ¨åŠ›å­¦æ¼”åŒ–ï¼š

$$\theta(t+\Delta t) = \theta(t) - \eta \nabla L(\theta(t))$$

è¿™æ˜¯ç¦»æ•£åŒ–çš„æ¢¯åº¦æµæ–¹ç¨‹ã€‚

### Epoch çš„ç‰©ç†æ„ä¹‰

- **ä¸€ä¸ª Batch**ï¼šä¸€æ¬¡åŠ›çš„æµ‹é‡ï¼ˆæœ‰å™ªå£°ï¼‰
- **ä¸€ä¸ª Epoch**ï¼šéå†æ•´ä¸ªç›¸ç©ºé—´
- **å¤šä¸ª Epoch**ï¼šç³»ç»Ÿå‘å¹³è¡¡æ€å¼›è±«

### æ”¶æ•›åˆ¤æ®

ç±»ä¼¼äºç‰©ç†ç³»ç»Ÿè¾¾åˆ°å¹³è¡¡çš„åˆ¤æ®ï¼š

| ç‰©ç†ç³»ç»Ÿ | ç¥ç»ç½‘ç»œè®­ç»ƒ |
|---------|-------------|
| èƒ½é‡ä¸å†ä¸‹é™ | æŸå¤±ä¸å†ä¸‹é™ |
| æ¶¨è½å˜å° | æ¢¯åº¦èŒƒæ•°å˜å° |
| æ¸©åº¦é™ä½ | å­¦ä¹ ç‡è¡°å‡ |

---

## ğŸ“ ç»ƒä¹ 

1. å®ç°ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼ŒåŒ…å«æ—©åœå’Œæ£€æŸ¥ç‚¹ä¿å­˜
2. ä½¿ç”¨ TensorBoard è®°å½•è®­ç»ƒè¿‡ç¨‹
3. å®ç°æ··åˆç²¾åº¦è®­ç»ƒå¹¶æ¯”è¾ƒé€Ÿåº¦

---

## â­ï¸ ä¸‹ä¸€èŠ‚

ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†å­¦ä¹  [éªŒè¯ä¸æµ‹è¯•](./04_validation_testing.md)ï¼Œäº†è§£å¦‚ä½•è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

