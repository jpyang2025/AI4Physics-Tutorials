# 5.1 æ•°æ®åŠ è½½

## ğŸ“– æ¦‚è¿°

é«˜æ•ˆçš„æ•°æ®åŠ è½½æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„åŸºç¡€ã€‚PyTorch æä¾›äº† `Dataset` å’Œ `DataLoader` ä¸¤ä¸ªæ ¸å¿ƒç±»ï¼Œå®ç°äº†æ•°æ®è®¿é—®ä¸æ‰¹é‡å¤„ç†çš„è§£è€¦ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£ `Dataset` æŠ½è±¡åŸºç±»
- ä½¿ç”¨ `DataLoader` è¿›è¡Œæ‰¹é‡æ•°æ®åŠ è½½
- å®ç°è‡ªå®šä¹‰æ•°æ®é›†
- æŒæ¡æ•°æ®å¢å¼ºæŠ€æœ¯

---

## 5.1.1 Dataset åŸºç¡€

### Dataset æŠ½è±¡ç±»

`torch.utils.data.Dataset` æ˜¯æ‰€æœ‰æ•°æ®é›†çš„æŠ½è±¡åŸºç±»ã€‚è‡ªå®šä¹‰æ•°æ®é›†éœ€è¦å®ç°ä¸¤ä¸ªæ–¹æ³•ï¼š

```python
import torch
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    """è‡ªå®šä¹‰æ•°æ®é›†æ¨¡æ¿"""
    
    def __init__(self, data, labels):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data: è¾“å…¥æ•°æ®
            labels: æ ‡ç­¾æ•°æ®
        """
        self.data = data
        self.labels = labels
    
    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.data)
    
    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ ·æœ¬
        
        Args:
            idx: æ ·æœ¬ç´¢å¼•
            
        Returns:
            å…ƒç»„ (æ•°æ®, æ ‡ç­¾)
        """
        return self.data[idx], self.labels[idx]
```

### ç‰©ç†æ•°æ®é›†ç¤ºä¾‹ï¼šè°æŒ¯å­è½¨è¿¹

```python
import torch
import numpy as np
from torch.utils.data import Dataset

class HarmonicOscillatorDataset(Dataset):
    """
    è°æŒ¯å­è½¨è¿¹æ•°æ®é›†
    
    ç”Ÿæˆç®€è°è¿åŠ¨çš„ (t, x) æ•°æ®å¯¹
    x(t) = A * cos(Ï‰t + Ï†)
    """
    
    def __init__(self, n_trajectories=1000, n_points=100, 
                 omega_range=(0.5, 2.0), noise_level=0.1):
        """
        Args:
            n_trajectories: è½¨è¿¹æ•°é‡
            n_points: æ¯æ¡è½¨è¿¹çš„æ—¶é—´ç‚¹æ•°
            omega_range: è§’é¢‘ç‡èŒƒå›´
            noise_level: å™ªå£°æ°´å¹³
        """
        self.n_trajectories = n_trajectories
        self.n_points = n_points
        
        # æ—¶é—´ç½‘æ ¼
        self.t = torch.linspace(0, 10, n_points)
        
        # ç”Ÿæˆæ•°æ®
        self.trajectories = []
        self.parameters = []  # (A, omega, phi)
        
        for _ in range(n_trajectories):
            # éšæœºå‚æ•°
            A = np.random.uniform(0.5, 2.0)
            omega = np.random.uniform(*omega_range)
            phi = np.random.uniform(0, 2 * np.pi)
            
            # ç”Ÿæˆè½¨è¿¹
            x = A * torch.cos(omega * self.t + phi)
            
            # æ·»åŠ å™ªå£°
            x = x + noise_level * torch.randn_like(x)
            
            self.trajectories.append(x)
            self.parameters.append(torch.tensor([A, omega, phi]))
        
        self.trajectories = torch.stack(self.trajectories)
        self.parameters = torch.stack(self.parameters)
    
    def __len__(self):
        return self.n_trajectories
    
    def __getitem__(self, idx):
        """è¿”å› (è½¨è¿¹, å‚æ•°)"""
        return self.trajectories[idx], self.parameters[idx]


# ä½¿ç”¨ç¤ºä¾‹
dataset = HarmonicOscillatorDataset(n_trajectories=1000)
print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")

# è·å–ä¸€ä¸ªæ ·æœ¬
trajectory, params = dataset[0]
print(f"è½¨è¿¹å½¢çŠ¶: {trajectory.shape}")
print(f"å‚æ•° (A, Ï‰, Ï†): {params}")
```

---

## 5.1.2 DataLoader

`DataLoader` å°† `Dataset` åŒ…è£…ä¸ºå¯è¿­ä»£å¯¹è±¡ï¼Œæä¾›æ‰¹é‡åŒ–ã€æ‰“ä¹±ã€å¹¶è¡ŒåŠ è½½ç­‰åŠŸèƒ½ã€‚

### åŸºæœ¬ç”¨æ³•

```python
from torch.utils.data import DataLoader

# åˆ›å»ºæ•°æ®é›†
dataset = HarmonicOscillatorDataset(n_trajectories=1000)

# åˆ›å»º DataLoader
dataloader = DataLoader(
    dataset,
    batch_size=32,      # æ‰¹é‡å¤§å°
    shuffle=True,       # æ‰“ä¹±æ•°æ®ï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
    num_workers=4,      # å¹¶è¡ŒåŠ è½½çš„å·¥ä½œè¿›ç¨‹æ•°
    pin_memory=True,    # ä½¿ç”¨é”é¡µå†…å­˜ï¼ˆGPU è®­ç»ƒæ—¶åŠ é€Ÿï¼‰
    drop_last=True      # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„æ‰¹æ¬¡
)

# è¿­ä»£æ•°æ®
for batch_trajectories, batch_params in dataloader:
    print(f"æ‰¹é‡è½¨è¿¹å½¢çŠ¶: {batch_trajectories.shape}")  # [32, 100]
    print(f"æ‰¹é‡å‚æ•°å½¢çŠ¶: {batch_params.shape}")        # [32, 3]
    break
```

### DataLoader å…³é”®å‚æ•°

| å‚æ•° | è¯´æ˜ | å…¸å‹å€¼ |
|------|------|-------|
| `batch_size` | æ¯æ‰¹æ ·æœ¬æ•° | 32, 64, 128, 256 |
| `shuffle` | æ˜¯å¦æ‰“ä¹± | è®­ç»ƒ Trueï¼Œæµ‹è¯• False |
| `num_workers` | å¹¶è¡Œè¿›ç¨‹æ•° | 0-8ï¼ˆæ ¹æ® CPU æ ¸å¿ƒæ•°ï¼‰ |
| `pin_memory` | é”é¡µå†…å­˜ | GPU è®­ç»ƒæ—¶ True |
| `drop_last` | ä¸¢å¼ƒä¸å®Œæ•´æ‰¹æ¬¡ | æ‰¹å½’ä¸€åŒ–æ—¶ True |

### æ‰¹é‡å¤§å°çš„ç‰©ç†æ„ä¹‰

æ‰¹é‡å¤§å°å½±å“æ¢¯åº¦ä¼°è®¡çš„æ–¹å·®ï¼š

$$\text{Var}[\nabla L_{\text{batch}}] \propto \frac{\sigma^2}{B}$$

å…¶ä¸­ $B$ æ˜¯æ‰¹é‡å¤§å°ã€‚

| æ‰¹é‡å¤§å° | ç‰¹ç‚¹ |
|---------|------|
| å°æ‰¹é‡ï¼ˆ8-32ï¼‰ | å™ªå£°å¤§ï¼Œæœ‰åŠ©äºè·³å‡ºå±€éƒ¨æå°ï¼Œä½†è®­ç»ƒä¸ç¨³å®š |
| ä¸­ç­‰æ‰¹é‡ï¼ˆ64-256ï¼‰ | å¹³è¡¡å™ªå£°å’Œç¨³å®šæ€§ï¼Œæœ€å¸¸ç”¨ |
| å¤§æ‰¹é‡ï¼ˆ512+ï¼‰ | æ¢¯åº¦ä¼°è®¡å‡†ç¡®ï¼Œéœ€è¦æ›´å¤§å­¦ä¹ ç‡ |

---

## 5.1.3 å†…ç½®æ•°æ®é›†

PyTorch æä¾›äº†å¤šç§é¢„ç½®æ•°æ®é›†ï¼Œæ–¹ä¾¿å¿«é€Ÿå®éªŒã€‚

### torchvision æ•°æ®é›†

```python
import torchvision
import torchvision.transforms as transforms

# å®šä¹‰å˜æ¢
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # å½’ä¸€åŒ–åˆ° [-1, 1]
])

# ä¸‹è½½å¹¶åŠ è½½ MNIST
train_dataset = torchvision.datasets.MNIST(
    root='./data',          # æ•°æ®å­˜æ”¾è·¯å¾„
    train=True,             # è®­ç»ƒé›†
    transform=transform,    # åº”ç”¨å˜æ¢
    download=True           # è‡ªåŠ¨ä¸‹è½½
)

test_dataset = torchvision.datasets.MNIST(
    root='./data',
    train=False,
    transform=transform,
    download=True
)

# åˆ›å»º DataLoader
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# æŸ¥çœ‹æ•°æ®
images, labels = next(iter(train_loader))
print(f"å›¾åƒå½¢çŠ¶: {images.shape}")  # [64, 1, 28, 28]
print(f"æ ‡ç­¾å½¢çŠ¶: {labels.shape}")  # [64]
```

### å¸¸ç”¨æ•°æ®é›†ä¸€è§ˆ

```python
# å›¾åƒåˆ†ç±»
torchvision.datasets.MNIST       # æ‰‹å†™æ•°å­—
torchvision.datasets.CIFAR10     # 10ç±»è‡ªç„¶å›¾åƒ
torchvision.datasets.CIFAR100    # 100ç±»è‡ªç„¶å›¾åƒ
torchvision.datasets.ImageNet    # å¤§è§„æ¨¡å›¾åƒåˆ†ç±»
torchvision.datasets.FashionMNIST  # æœè£…å›¾åƒ

# ç›®æ ‡æ£€æµ‹
torchvision.datasets.CocoDetection  # COCO æ•°æ®é›†

# è¯­ä¹‰åˆ†å‰²
torchvision.datasets.VOCSegmentation  # Pascal VOC
```

---

## 5.1.4 è‡ªå®šä¹‰æ•°æ®é›†

### ä»æ–‡ä»¶åŠ è½½æ•°æ®

```python
import os
import numpy as np
from torch.utils.data import Dataset

class ExperimentalDataset(Dataset):
    """
    ä» .npy æ–‡ä»¶åŠ è½½å®éªŒæ•°æ®
    
    é€‚ç”¨äºç‰©ç†å®éªŒæ•°æ®ï¼Œå¦‚ï¼š
    - å…‰è°±æ•°æ®
    - æ•£å°„æ•°æ®
    - æ—¶é—´åºåˆ—æµ‹é‡
    """
    
    def __init__(self, data_dir, transform=None):
        """
        Args:
            data_dir: æ•°æ®ç›®å½•ï¼ŒåŒ…å« data.npy å’Œ labels.npy
            transform: å¯é€‰çš„æ•°æ®å˜æ¢
        """
        self.data_dir = data_dir
        self.transform = transform
        
        # åŠ è½½æ•°æ®
        self.data = np.load(os.path.join(data_dir, 'data.npy'))
        self.labels = np.load(os.path.join(data_dir, 'labels.npy'))
        
        # è½¬æ¢ä¸ºå¼ é‡
        self.data = torch.from_numpy(self.data).float()
        self.labels = torch.from_numpy(self.labels).float()
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        x = self.data[idx]
        y = self.labels[idx]
        
        if self.transform:
            x = self.transform(x)
        
        return x, y


class LazyLoadDataset(Dataset):
    """
    æ‡’åŠ è½½æ•°æ®é›† - é€‚ç”¨äºå¤§å‹æ•°æ®
    
    ä¸å°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°å†…å­˜ï¼Œè€Œæ˜¯åœ¨éœ€è¦æ—¶è¯»å–
    """
    
    def __init__(self, file_list, load_func):
        """
        Args:
            file_list: æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            load_func: åŠ è½½å•ä¸ªæ–‡ä»¶çš„å‡½æ•°
        """
        self.file_list = file_list
        self.load_func = load_func
    
    def __len__(self):
        return len(self.file_list)
    
    def __getitem__(self, idx):
        # ä»…åœ¨éœ€è¦æ—¶åŠ è½½æ•°æ®
        return self.load_func(self.file_list[idx])
```

### æ˜ å°„å¼ä¸å¯è¿­ä»£å¼æ•°æ®é›†

```python
from torch.utils.data import Dataset, IterableDataset

# æ˜ å°„å¼æ•°æ®é›†ï¼ˆæ”¯æŒéšæœºè®¿é—®ï¼‰
class MapDataset(Dataset):
    """é€šè¿‡ç´¢å¼•è®¿é—®"""
    def __getitem__(self, idx):
        return self.data[idx]

# å¯è¿­ä»£å¼æ•°æ®é›†ï¼ˆæµå¼è®¿é—®ï¼‰
class StreamDataset(IterableDataset):
    """
    é€‚ç”¨äºï¼š
    - å®æ—¶æ•°æ®æµï¼ˆä¼ æ„Ÿå™¨æ•°æ®ï¼‰
    - è¶…å¤§å‹æ•°æ®é›†
    - åœ¨çº¿ç”Ÿæˆçš„æ•°æ®
    """
    
    def __init__(self, generator_func):
        self.generator_func = generator_func
    
    def __iter__(self):
        return self.generator_func()


# ç¤ºä¾‹ï¼šç”Ÿæˆç²’å­æ•£å°„äº‹ä»¶
def particle_event_generator():
    """æ¨¡æ‹Ÿç²’å­æ•£å°„äº‹ä»¶ç”Ÿæˆå™¨"""
    while True:
        # æ¨¡æ‹Ÿå…¥å°„ç²’å­
        energy = np.random.exponential(10.0)  # GeV
        theta = np.random.uniform(0, np.pi)
        phi = np.random.uniform(0, 2 * np.pi)
        
        # æ¨¡æ‹Ÿå‡ºå°„ç²’å­
        n_particles = np.random.poisson(5)
        particles = {
            'energy': energy,
            'theta': theta,
            'phi': phi,
            'n_out': n_particles
        }
        
        yield particles


stream_dataset = StreamDataset(particle_event_generator)
stream_loader = DataLoader(stream_dataset, batch_size=32)
```

---

## 5.1.5 æ•°æ®å˜æ¢ä¸å¢å¼º

### åŸºæœ¬å˜æ¢

```python
import torchvision.transforms as T

# å›¾åƒå˜æ¢ç®¡é“
image_transform = T.Compose([
    T.Resize(256),              # è°ƒæ•´å¤§å°
    T.CenterCrop(224),          # ä¸­å¿ƒè£å‰ª
    T.ToTensor(),               # è½¬æ¢ä¸ºå¼ é‡ [0, 1]
    T.Normalize(                # æ ‡å‡†åŒ–
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])
```

### æ•°æ®å¢å¼ºï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰

æ•°æ®å¢å¼ºå¯ä»¥æœ‰æ•ˆæ‰©å……æ•°æ®é›†ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚

```python
train_transform = T.Compose([
    T.RandomResizedCrop(224),           # éšæœºè£å‰ª
    T.RandomHorizontalFlip(p=0.5),      # éšæœºæ°´å¹³ç¿»è½¬
    T.RandomRotation(15),               # éšæœºæ—‹è½¬
    T.ColorJitter(                       # é¢œè‰²æŠ–åŠ¨
        brightness=0.2,
        contrast=0.2,
        saturation=0.2,
        hue=0.1
    ),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225])
])

# æµ‹è¯•æ—¶ä¸ä½¿ç”¨æ•°æ®å¢å¼º
test_transform = T.Compose([
    T.Resize(256),
    T.CenterCrop(224),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225])
])
```

### ç‰©ç†æ•°æ®å¢å¼º

```python
class PhysicsDataAugmentation:
    """ç‰©ç†æ•°æ®å¢å¼ºå™¨"""
    
    def __init__(self, noise_level=0.05, shift_range=0.1):
        self.noise_level = noise_level
        self.shift_range = shift_range
    
    def add_noise(self, x):
        """æ·»åŠ é«˜æ–¯å™ªå£° - æ¨¡æ‹Ÿæµ‹é‡è¯¯å·®"""
        noise = torch.randn_like(x) * self.noise_level * x.abs().mean()
        return x + noise
    
    def time_shift(self, x):
        """æ—¶é—´å¹³ç§» - å¯¹äºæ—¶é—´åºåˆ—"""
        shift = int(np.random.uniform(-self.shift_range, self.shift_range) * len(x))
        return torch.roll(x, shifts=shift)
    
    def scale(self, x, scale_range=(0.8, 1.2)):
        """ç¼©æ”¾ - æ¨¡æ‹Ÿå¢ç›Šå˜åŒ–"""
        scale = np.random.uniform(*scale_range)
        return x * scale
    
    def __call__(self, x):
        """éšæœºåº”ç”¨å¢å¼º"""
        if np.random.random() > 0.5:
            x = self.add_noise(x)
        if np.random.random() > 0.5:
            x = self.time_shift(x)
        if np.random.random() > 0.5:
            x = self.scale(x)
        return x
```

---

## 5.1.6 æ•°æ®åˆ’åˆ†

### è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ’åˆ†

```python
from torch.utils.data import random_split

# å‡è®¾æœ‰ä¸€ä¸ªå®Œæ•´æ•°æ®é›†
full_dataset = HarmonicOscillatorDataset(n_trajectories=10000)

# åˆ’åˆ†æ•°æ®é›†
train_size = int(0.8 * len(full_dataset))  # 80% è®­ç»ƒ
val_size = int(0.1 * len(full_dataset))    # 10% éªŒè¯
test_size = len(full_dataset) - train_size - val_size  # 10% æµ‹è¯•

train_dataset, val_dataset, test_dataset = random_split(
    full_dataset, 
    [train_size, val_size, test_size],
    generator=torch.Generator().manual_seed(42)  # å›ºå®šéšæœºç§å­
)

print(f"è®­ç»ƒé›†: {len(train_dataset)}")
print(f"éªŒè¯é›†: {len(val_dataset)}")
print(f"æµ‹è¯•é›†: {len(test_dataset)}")

# åˆ›å»º DataLoader
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
```

### K æŠ˜äº¤å‰éªŒè¯

```python
from sklearn.model_selection import KFold

def create_kfold_loaders(dataset, k=5, batch_size=32):
    """åˆ›å»º K æŠ˜äº¤å‰éªŒè¯çš„æ•°æ®åŠ è½½å™¨"""
    kfold = KFold(n_splits=k, shuffle=True, random_state=42)
    indices = list(range(len(dataset)))
    
    fold_loaders = []
    for fold, (train_idx, val_idx) in enumerate(kfold.split(indices)):
        # åˆ›å»ºå­é›†
        train_subset = torch.utils.data.Subset(dataset, train_idx)
        val_subset = torch.utils.data.Subset(dataset, val_idx)
        
        # åˆ›å»º DataLoader
        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)
        
        fold_loaders.append((train_loader, val_loader))
        print(f"Fold {fold+1}: Train {len(train_idx)}, Val {len(val_idx)}")
    
    return fold_loaders
```

---

## 5.1.7 è‡ªå®šä¹‰é‡‡æ ·å™¨

### é‡‡æ ·å™¨ç±»å‹

```python
from torch.utils.data import Sampler, SequentialSampler, RandomSampler, WeightedRandomSampler

# é¡ºåºé‡‡æ ·
seq_sampler = SequentialSampler(dataset)

# éšæœºé‡‡æ ·
rand_sampler = RandomSampler(dataset)

# å¸¦æƒé‡éšæœºé‡‡æ · - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
# å‡è®¾æœ‰ä¸¤ä¸ªç±»åˆ«ï¼Œ0ç±»æœ‰900ä¸ªæ ·æœ¬ï¼Œ1ç±»æœ‰100ä¸ªæ ·æœ¬
class_counts = [900, 100]
weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)
sample_weights = weights[labels]  # labels æ˜¯æ¯ä¸ªæ ·æœ¬çš„ç±»åˆ«

weighted_sampler = WeightedRandomSampler(
    weights=sample_weights,
    num_samples=len(sample_weights),
    replacement=True
)

loader = DataLoader(dataset, batch_size=32, sampler=weighted_sampler)
```

### ç‰©ç†åº”ç”¨ï¼šç¨€æœ‰äº‹ä»¶é‡‡æ ·

```python
class RareEventSampler(Sampler):
    """
    ç¨€æœ‰äº‹ä»¶é‡‡æ ·å™¨
    
    åœ¨ç‰©ç†å®éªŒä¸­ï¼ŒæŸäº›äº‹ä»¶ï¼ˆå¦‚å¸Œæ ¼æ–¯ç»è‰²å­è¡°å˜ï¼‰éå¸¸ç¨€æœ‰ã€‚
    è¯¥é‡‡æ ·å™¨å¢åŠ å¯¹ç¨€æœ‰äº‹ä»¶çš„é‡‡æ ·æ¦‚ç‡ã€‚
    """
    
    def __init__(self, event_types, rare_event_ids, rare_boost=10.0):
        """
        Args:
            event_types: äº‹ä»¶ç±»å‹æ•°ç»„
            rare_event_ids: ç¨€æœ‰äº‹ä»¶çš„ç±»å‹ ID åˆ—è¡¨
            rare_boost: ç¨€æœ‰äº‹ä»¶é‡‡æ ·æƒé‡æå‡å€æ•°
        """
        self.n_samples = len(event_types)
        
        # è®¡ç®—é‡‡æ ·æƒé‡
        self.weights = torch.ones(self.n_samples)
        for rare_id in rare_event_ids:
            rare_mask = (event_types == rare_id)
            self.weights[rare_mask] = rare_boost
        
        # å½’ä¸€åŒ–
        self.weights = self.weights / self.weights.sum()
    
    def __iter__(self):
        indices = torch.multinomial(
            self.weights, 
            self.n_samples, 
            replacement=True
        )
        return iter(indices.tolist())
    
    def __len__(self):
        return self.n_samples
```

---

## 5.1.8 collate_fn è‡ªå®šä¹‰æ‰¹å¤„ç†

å½“æ•°æ®æ ·æœ¬å¤§å°ä¸ä¸€è‡´æ—¶ï¼Œéœ€è¦è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°ã€‚

```python
def custom_collate_fn(batch):
    """
    è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°
    
    å¤„ç†å˜é•¿åºåˆ—çš„æ‰¹å¤„ç†
    """
    # batch æ˜¯ [(data, label), (data, label), ...] çš„åˆ—è¡¨
    data_list, label_list = zip(*batch)
    
    # è·å–æœ€é•¿åºåˆ—é•¿åº¦
    max_len = max(d.shape[0] for d in data_list)
    
    # å¡«å……åˆ°ç›¸åŒé•¿åº¦
    padded_data = []
    lengths = []
    for d in data_list:
        length = d.shape[0]
        lengths.append(length)
        
        if length < max_len:
            padding = torch.zeros(max_len - length, *d.shape[1:])
            d = torch.cat([d, padding], dim=0)
        padded_data.append(d)
    
    return (
        torch.stack(padded_data),       # [B, max_len, ...]
        torch.tensor(lengths),           # [B]
        torch.stack(label_list)          # [B, ...]
    )


# ä½¿ç”¨è‡ªå®šä¹‰ collate_fn
loader = DataLoader(
    dataset,
    batch_size=32,
    collate_fn=custom_collate_fn
)
```

### å˜é•¿ç²’å­äº‹ä»¶å¤„ç†

```python
def particle_event_collate(batch):
    """
    ç²’å­ç‰©ç†äº‹ä»¶æ‰¹å¤„ç†
    
    æ¯ä¸ªäº‹ä»¶æœ‰ä¸åŒæ•°é‡çš„ç²’å­
    """
    events = []
    event_lengths = []
    targets = []
    
    for event_particles, target in batch:
        events.append(event_particles)
        event_lengths.append(len(event_particles))
        targets.append(target)
    
    # æ‰¾åˆ°æœ€å¤§ç²’å­æ•°
    max_particles = max(event_lengths)
    particle_dim = events[0].shape[-1]  # æ¯ä¸ªç²’å­çš„ç‰¹å¾ç»´åº¦
    
    # å¡«å……
    padded_events = torch.zeros(len(batch), max_particles, particle_dim)
    mask = torch.zeros(len(batch), max_particles, dtype=torch.bool)
    
    for i, (event, length) in enumerate(zip(events, event_lengths)):
        padded_events[i, :length] = event
        mask[i, :length] = True
    
    return {
        'particles': padded_events,     # [B, max_n, D]
        'mask': mask,                    # [B, max_n]
        'n_particles': torch.tensor(event_lengths),  # [B]
        'target': torch.stack(targets)  # [B, ...]
    }
```

---

## 5.1.9 å¤š GPU æ•°æ®åŠ è½½

```python
from torch.utils.data import DistributedSampler

# åˆ†å¸ƒå¼è®­ç»ƒæ—¶ä½¿ç”¨
distributed_sampler = DistributedSampler(
    dataset,
    num_replicas=world_size,  # GPU æ•°é‡
    rank=rank,                # å½“å‰ GPU ç¼–å·
    shuffle=True
)

loader = DataLoader(
    dataset,
    batch_size=32,
    sampler=distributed_sampler,  # ä½¿ç”¨åˆ†å¸ƒå¼é‡‡æ ·å™¨
    num_workers=4
)

# æ¯ä¸ª epoch å¼€å§‹æ—¶éœ€è¦è®¾ç½® epoch
for epoch in range(num_epochs):
    distributed_sampler.set_epoch(epoch)
    for batch in loader:
        # è®­ç»ƒ...
        pass
```

---

## ğŸ”¬ ç‰©ç†è§†è§’æ€»ç»“

### æ•°æ®åŠ è½½çš„ç‰©ç†ç±»æ¯”

| æ•°æ®åŠ è½½æ¦‚å¿µ | ç‰©ç†ç±»æ¯” |
|-------------|---------|
| Dataset | ç›¸ç©ºé—´ä¸­çš„æ ·æœ¬é›†åˆ |
| DataLoader | éå†ç›¸ç©ºé—´çš„ç®—æ³• |
| Batch | ç³»ç»¼ä¸­çš„å­ç³»ç»Ÿ |
| Shuffle | éå†æ€§å‡è®¾ |
| æ•°æ®å¢å¼º | å¯¹ç§°æ€§çº¦æŸä¸‹çš„å˜æ¢ |
| é‡‡æ ·æƒé‡ | ç»å°”å…¹æ›¼å› å­ |

### æ‰¹é‡å¤§å°ä¸æ¢¯åº¦ä¼°è®¡

ä»ç»Ÿè®¡åŠ›å­¦è§’åº¦ï¼Œå°æ‰¹é‡æ¢¯åº¦å¯ä»¥çœ‹ä½œæœ‰é™æ¸©åº¦ä¸‹çš„åŠ›ï¼š

$$F = -\nabla U + \sqrt{2T}\eta$$

å…¶ä¸­æ¸©åº¦ $T \propto 1/B$ï¼ˆB æ˜¯æ‰¹é‡å¤§å°ï¼‰ï¼Œ$\eta$ æ˜¯éšæœºå™ªå£°ã€‚

---

## ğŸ“ ç»ƒä¹ 

1. ä¸ºä½ çš„å®éªŒæ•°æ®åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰ Dataset
2. å®ç°ä¸€ä¸ªæ•°æ®å¢å¼ºç®¡é“ï¼ŒåŒ…å«å™ªå£°æ·»åŠ å’Œç¼©æ”¾
3. ä½¿ç”¨ WeightedRandomSampler å¤„ç†ç±»åˆ«ä¸å¹³è¡¡æ•°æ®

---

## â­ï¸ ä¸‹ä¸€èŠ‚

ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†å­¦ä¹  [ä¼˜åŒ–å™¨](./02_optimizers.md)ï¼Œäº†è§£å¦‚ä½•æ›´æ–°ç½‘ç»œå‚æ•°ã€‚

