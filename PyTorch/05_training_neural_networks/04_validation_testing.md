# 5.4 éªŒè¯ä¸æµ‹è¯•

## ğŸ“– æ¦‚è¿°

éªŒè¯å’Œæµ‹è¯•æ˜¯æ¨¡å‹å¼€å‘ä¸­ä¸å¯æˆ–ç¼ºçš„ç¯èŠ‚ã€‚éªŒè¯ç”¨äºè°ƒæ•´è¶…å‚æ•°ï¼Œæµ‹è¯•ç”¨äºè¯„ä¼°æœ€ç»ˆæ€§èƒ½ã€‚æœ¬èŠ‚ä»‹ç»å¦‚ä½•æ­£ç¡®è¯„ä¼°æ¨¡å‹æ€§èƒ½å¹¶é¿å…å¸¸è§é™·é˜±ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£éªŒè¯é›†å’Œæµ‹è¯•é›†çš„åŒºåˆ«
- æŒæ¡å¸¸ç”¨è¯„ä¼°æŒ‡æ ‡
- å®ç°æ­£ç¡®çš„è¯„ä¼°æµç¨‹
- é¿å…æ•°æ®æ³„éœ²

---

## 5.4.1 æ•°æ®åˆ’åˆ†ç­–ç•¥

### è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ’åˆ†

```
å…¨éƒ¨æ•°æ®
â”œâ”€â”€ è®­ç»ƒé›† (60-80%)
â”‚   â””â”€â”€ ç”¨äºè®­ç»ƒæ¨¡å‹å‚æ•°
â”œâ”€â”€ éªŒè¯é›† (10-20%)
â”‚   â””â”€â”€ ç”¨äºè°ƒæ•´è¶…å‚æ•°å’Œæ—©åœ
â””â”€â”€ æµ‹è¯•é›† (10-20%)
    â””â”€â”€ ç”¨äºæœ€ç»ˆæ€§èƒ½è¯„ä¼°ï¼ˆåªä½¿ç”¨ä¸€æ¬¡ï¼ï¼‰
```

```python
from torch.utils.data import random_split

def split_dataset(dataset, train_ratio=0.8, val_ratio=0.1, seed=42):
    """
    åˆ’åˆ†æ•°æ®é›†
    
    Args:
        dataset: å®Œæ•´æ•°æ®é›†
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        seed: éšæœºç§å­
    
    Returns:
        train_set, val_set, test_set
    """
    total = len(dataset)
    train_size = int(total * train_ratio)
    val_size = int(total * val_ratio)
    test_size = total - train_size - val_size
    
    generator = torch.Generator().manual_seed(seed)
    
    train_set, val_set, test_set = random_split(
        dataset, 
        [train_size, val_size, test_size],
        generator=generator
    )
    
    print(f"æ•°æ®é›†åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(train_set)}")
    print(f"  éªŒè¯é›†: {len(val_set)}")
    print(f"  æµ‹è¯•é›†: {len(test_set)}")
    
    return train_set, val_set, test_set
```

### äº¤å‰éªŒè¯

å½“æ•°æ®é‡æœ‰é™æ—¶ï¼Œä½¿ç”¨ K æŠ˜äº¤å‰éªŒè¯ã€‚

```python
from sklearn.model_selection import KFold
import numpy as np

def k_fold_cross_validation(dataset, model_fn, train_fn, k=5, seed=42):
    """
    K æŠ˜äº¤å‰éªŒè¯
    
    Args:
        dataset: å®Œæ•´æ•°æ®é›†
        model_fn: åˆ›å»ºæ¨¡å‹çš„å‡½æ•°
        train_fn: è®­ç»ƒå‡½æ•°
        k: æŠ˜æ•°
        seed: éšæœºç§å­
    
    Returns:
        æ¯æŠ˜çš„éªŒè¯ç»“æœ
    """
    kfold = KFold(n_splits=k, shuffle=True, random_state=seed)
    indices = np.arange(len(dataset))
    
    fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(indices)):
        print(f"\n===== Fold {fold + 1}/{k} =====")
        
        # åˆ›å»ºå­é›†
        train_subset = torch.utils.data.Subset(dataset, train_idx)
        val_subset = torch.utils.data.Subset(dataset, val_idx)
        
        # åˆ›å»ºæ–°æ¨¡å‹
        model = model_fn()
        
        # è®­ç»ƒå¹¶è¯„ä¼°
        result = train_fn(model, train_subset, val_subset)
        fold_results.append(result)
        
        print(f"Fold {fold + 1} éªŒè¯ç»“æœ: {result}")
    
    # ç»Ÿè®¡ç»“æœ
    mean_result = np.mean(fold_results)
    std_result = np.std(fold_results)
    print(f"\näº¤å‰éªŒè¯ç»“æœ: {mean_result:.4f} Â± {std_result:.4f}")
    
    return fold_results
```

### æ—¶é—´åºåˆ—æ•°æ®åˆ’åˆ†

å¯¹äºæ—¶é—´åºåˆ—æ•°æ®ï¼Œä¸èƒ½éšæœºåˆ’åˆ†ï¼

```python
def time_series_split(dataset, n_splits=5):
    """
    æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
    
    ä¿æŒæ—¶é—´é¡ºåºï¼šæ€»æ˜¯ç”¨è¿‡å»é¢„æµ‹æœªæ¥
    """
    from sklearn.model_selection import TimeSeriesSplit
    
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    splits = []
    for train_idx, val_idx in tscv.split(range(len(dataset))):
        train_subset = torch.utils.data.Subset(dataset, train_idx)
        val_subset = torch.utils.data.Subset(dataset, val_idx)
        splits.append((train_subset, val_subset))
        
        print(f"Train: {min(train_idx)}-{max(train_idx)}, "
              f"Val: {min(val_idx)}-{max(val_idx)}")
    
    return splits
```

---

## 5.4.2 è¯„ä¼°æ¨¡å¼

### train() vs eval()

```python
# è®­ç»ƒæ¨¡å¼
model.train()
# - Dropout å±‚æ¿€æ´»ï¼Œéšæœºä¸¢å¼ƒç¥ç»å…ƒ
# - BatchNorm ä½¿ç”¨å½“å‰æ‰¹æ¬¡çš„å‡å€¼å’Œæ–¹å·®

# è¯„ä¼°æ¨¡å¼
model.eval()
# - Dropout å±‚å…³é—­ï¼Œä½¿ç”¨æ‰€æœ‰ç¥ç»å…ƒ
# - BatchNorm ä½¿ç”¨è®­ç»ƒæ—¶ç´¯ç§¯çš„å‡å€¼å’Œæ–¹å·®

# æ­£ç¡®çš„è¯„ä¼°æµç¨‹
def evaluate(model, test_loader, criterion, device):
    model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    
    total_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—
        for inputs, targets in test_loader:
            inputs = inputs.to(device)
            targets = targets.to(device)
            
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            total_loss += loss.item() * inputs.size(0)
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    avg_loss = total_loss / total
    accuracy = 100. * correct / total
    
    return avg_loss, accuracy
```

### torch.no_grad() vs torch.inference_mode()

```python
# no_grad: å…³é—­æ¢¯åº¦è¿½è¸ª
with torch.no_grad():
    output = model(input)
    # ä»ç„¶å¯ä»¥å¯¹ output è¿›è¡Œéœ€è¦æ¢¯åº¦çš„æ“ä½œï¼ˆåœ¨ with å—å¤–ï¼‰

# inference_mode: æ›´å½»åº•çš„æ¨ç†æ¨¡å¼ï¼ˆPyTorch 1.9+ï¼‰
with torch.inference_mode():
    output = model(input)
    # å®Œå…¨ç¦ç”¨ autogradï¼Œæ›´å¿«æ›´çœå†…å­˜
    # output å¼ é‡ä¸èƒ½ç”¨äºåç»­çš„æ¢¯åº¦è®¡ç®—
```

---

## 5.4.3 åˆ†ç±»è¯„ä¼°æŒ‡æ ‡

### åŸºæœ¬æŒ‡æ ‡

```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)

def compute_classification_metrics(y_true, y_pred, num_classes):
    """
    è®¡ç®—åˆ†ç±»æŒ‡æ ‡
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹æ ‡ç­¾
        num_classes: ç±»åˆ«æ•°
    """
    metrics = {}
    
    # å‡†ç¡®ç‡
    metrics['accuracy'] = accuracy_score(y_true, y_pred)
    
    # ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1
    if num_classes == 2:
        metrics['precision'] = precision_score(y_true, y_pred)
        metrics['recall'] = recall_score(y_true, y_pred)
        metrics['f1'] = f1_score(y_true, y_pred)
    else:
        # å¤šåˆ†ç±»ä½¿ç”¨å®å¹³å‡æˆ–åŠ æƒå¹³å‡
        metrics['precision'] = precision_score(y_true, y_pred, average='macro')
        metrics['recall'] = recall_score(y_true, y_pred, average='macro')
        metrics['f1'] = f1_score(y_true, y_pred, average='macro')
    
    # æ··æ·†çŸ©é˜µ
    metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred)
    
    return metrics
```

### æ··æ·†çŸ©é˜µå¯è§†åŒ–

```python
import matplotlib.pyplot as plt
import seaborn as sns

def plot_confusion_matrix(cm, class_names=None, figsize=(8, 6)):
    """
    ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    
    Args:
        cm: æ··æ·†çŸ©é˜µ
        class_names: ç±»åˆ«åç§°
        figsize: å›¾åƒå¤§å°
    """
    plt.figure(figsize=figsize)
    
    if class_names is None:
        class_names = [str(i) for i in range(len(cm))]
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.tight_layout()
    plt.show()
```

### ROC æ›²çº¿å’Œ AUC

```python
from sklearn.metrics import roc_curve, auc, roc_auc_score

def compute_roc_auc(y_true, y_scores, num_classes):
    """
    è®¡ç®— ROC æ›²çº¿å’Œ AUC
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_scores: é¢„æµ‹æ¦‚ç‡/åˆ†æ•°
        num_classes: ç±»åˆ«æ•°
    """
    if num_classes == 2:
        # äºŒåˆ†ç±»
        fpr, tpr, thresholds = roc_curve(y_true, y_scores[:, 1])
        roc_auc = auc(fpr, tpr)
        return {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}
    else:
        # å¤šåˆ†ç±»ï¼šOne-vs-Rest
        from sklearn.preprocessing import label_binarize
        
        y_true_bin = label_binarize(y_true, classes=range(num_classes))
        
        fpr = {}
        tpr = {}
        roc_auc = {}
        
        for i in range(num_classes):
            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_scores[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
        
        return {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}


def plot_roc_curve(roc_data, num_classes):
    """ç»˜åˆ¶ ROC æ›²çº¿"""
    plt.figure(figsize=(8, 6))
    
    if num_classes == 2:
        plt.plot(roc_data['fpr'], roc_data['tpr'], 
                label=f"AUC = {roc_data['auc']:.3f}")
    else:
        for i in range(num_classes):
            plt.plot(roc_data['fpr'][i], roc_data['tpr'][i],
                    label=f"Class {i} (AUC = {roc_data['auc'][i]:.3f})")
    
    plt.plot([0, 1], [0, 1], 'k--', label='Random')
    plt.xlabel('å‡é˜³æ€§ç‡ (FPR)')
    plt.ylabel('çœŸé˜³æ€§ç‡ (TPR)')
    plt.title('ROC æ›²çº¿')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()
```

---

## 5.4.4 å›å½’è¯„ä¼°æŒ‡æ ‡

```python
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def compute_regression_metrics(y_true, y_pred):
    """
    è®¡ç®—å›å½’æŒ‡æ ‡
    
    Args:
        y_true: çœŸå®å€¼
        y_pred: é¢„æµ‹å€¼
    """
    metrics = {}
    
    # MSE (å‡æ–¹è¯¯å·®)
    metrics['mse'] = mean_squared_error(y_true, y_pred)
    
    # RMSE (å‡æ–¹æ ¹è¯¯å·®)
    metrics['rmse'] = np.sqrt(metrics['mse'])
    
    # MAE (å¹³å‡ç»å¯¹è¯¯å·®)
    metrics['mae'] = mean_absolute_error(y_true, y_pred)
    
    # RÂ² (å†³å®šç³»æ•°)
    metrics['r2'] = r2_score(y_true, y_pred)
    
    # ç›¸å¯¹è¯¯å·®
    relative_error = np.abs(y_true - y_pred) / (np.abs(y_true) + 1e-8)
    metrics['mape'] = np.mean(relative_error) * 100  # å¹³å‡ç™¾åˆ†æ¯”è¯¯å·®
    
    return metrics


def plot_regression_results(y_true, y_pred, title='å›å½’ç»“æœ'):
    """
    ç»˜åˆ¶å›å½’ç»“æœ
    """
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # é¢„æµ‹å€¼ vs çœŸå®å€¼
    axes[0].scatter(y_true, y_pred, alpha=0.5)
    axes[0].plot([y_true.min(), y_true.max()], 
                 [y_true.min(), y_true.max()], 'r--', label='ç†æƒ³')
    axes[0].set_xlabel('çœŸå®å€¼')
    axes[0].set_ylabel('é¢„æµ‹å€¼')
    axes[0].set_title('é¢„æµ‹å€¼ vs çœŸå®å€¼')
    axes[0].legend()
    
    # æ®‹å·®åˆ†å¸ƒ
    residuals = y_pred - y_true
    axes[1].hist(residuals, bins=50, edgecolor='black')
    axes[1].axvline(x=0, color='r', linestyle='--')
    axes[1].set_xlabel('æ®‹å·®')
    axes[1].set_ylabel('é¢‘æ•°')
    axes[1].set_title('æ®‹å·®åˆ†å¸ƒ')
    
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()
```

---

## 5.4.5 ç‰©ç†é—®é¢˜è¯„ä¼°æŒ‡æ ‡

### å®ˆæ’é‡è¯¯å·®

```python
def evaluate_conservation_laws(model, test_loader, device):
    """
    è¯„ä¼°ç‰©ç†å®ˆæ’å¾‹
    
    æ£€éªŒæ¨¡å‹é¢„æµ‹æ˜¯å¦æ»¡è¶³èƒ½é‡å®ˆæ’ã€åŠ¨é‡å®ˆæ’ç­‰
    """
    model.eval()
    
    energy_errors = []
    momentum_errors = []
    
    with torch.no_grad():
        for initial_state, final_state in test_loader:
            initial_state = initial_state.to(device)
            
            # æ¨¡å‹é¢„æµ‹æœ€ç»ˆçŠ¶æ€
            predicted_final = model(initial_state)
            
            # è®¡ç®—èƒ½é‡ï¼ˆå‡è®¾ state = [q, p]ï¼ŒH = pÂ²/2 + V(q)ï¼‰
            q_init, p_init = initial_state[:, :3], initial_state[:, 3:]
            q_pred, p_pred = predicted_final[:, :3], predicted_final[:, 3:]
            
            E_init = 0.5 * (p_init**2).sum(dim=1)  # åŠ¨èƒ½
            E_pred = 0.5 * (p_pred**2).sum(dim=1)
            
            energy_error = (E_pred - E_init).abs()
            energy_errors.extend(energy_error.cpu().numpy())
            
            # è®¡ç®—åŠ¨é‡
            p_total_init = p_init.sum(dim=1)
            p_total_pred = p_pred.sum(dim=1)
            
            momentum_error = (p_total_pred - p_total_init).abs()
            momentum_errors.extend(momentum_error.cpu().numpy())
    
    metrics = {
        'energy_error_mean': np.mean(energy_errors),
        'energy_error_std': np.std(energy_errors),
        'momentum_error_mean': np.mean(momentum_errors),
        'momentum_error_std': np.std(momentum_errors)
    }
    
    return metrics
```

### å¾®åˆ†æ–¹ç¨‹æ®‹å·®

```python
def evaluate_pde_residual(model, domain_points, boundary_points, 
                          pde_residual_fn, boundary_fn, device):
    """
    è¯„ä¼° PDE æ±‚è§£å™¨çš„æ®‹å·®
    
    Args:
        model: PINN æ¨¡å‹
        domain_points: å†…éƒ¨ç‚¹
        boundary_points: è¾¹ç•Œç‚¹
        pde_residual_fn: PDE æ®‹å·®å‡½æ•°
        boundary_fn: è¾¹ç•Œæ¡ä»¶å‡½æ•°
    """
    model.eval()
    
    # å†…éƒ¨æ®‹å·®
    domain_points = domain_points.to(device)
    domain_points.requires_grad = True
    
    u = model(domain_points)
    residual = pde_residual_fn(u, domain_points)
    
    pde_residual = (residual**2).mean().item()
    
    # è¾¹ç•Œæ®‹å·®
    boundary_points = boundary_points.to(device)
    u_boundary = model(boundary_points)
    u_exact = boundary_fn(boundary_points)
    
    boundary_residual = ((u_boundary - u_exact)**2).mean().item()
    
    return {
        'pde_residual': pde_residual,
        'boundary_residual': boundary_residual,
        'total_residual': pde_residual + boundary_residual
    }
```

---

## 5.4.6 å®Œæ•´æµ‹è¯•æµç¨‹

```python
class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, model, device='cpu'):
        self.model = model.to(device)
        self.device = device
    
    def predict(self, dataloader):
        """è·å–é¢„æµ‹ç»“æœ"""
        self.model.eval()
        
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for inputs, targets in dataloader:
                inputs = inputs.to(self.device)
                
                outputs = self.model(inputs)
                
                if outputs.dim() > 1 and outputs.size(1) > 1:
                    probs = torch.softmax(outputs, dim=1)
                    preds = outputs.argmax(dim=1)
                    all_probs.append(probs.cpu())
                else:
                    preds = outputs
                    all_probs = None
                
                all_preds.append(preds.cpu())
                all_targets.append(targets)
        
        result = {
            'predictions': torch.cat(all_preds).numpy(),
            'targets': torch.cat(all_targets).numpy()
        }
        
        if all_probs is not None:
            result['probabilities'] = torch.cat(all_probs).numpy()
        
        return result
    
    def evaluate_classification(self, dataloader, class_names=None):
        """åˆ†ç±»è¯„ä¼°"""
        result = self.predict(dataloader)
        
        y_true = result['targets']
        y_pred = result['predictions']
        
        num_classes = len(np.unique(y_true))
        
        # åŸºæœ¬æŒ‡æ ‡
        metrics = compute_classification_metrics(y_true, y_pred, num_classes)
        
        # ROC-AUCï¼ˆå¦‚æœæœ‰æ¦‚ç‡è¾“å‡ºï¼‰
        if 'probabilities' in result:
            roc_data = compute_roc_auc(y_true, result['probabilities'], num_classes)
            metrics['roc_auc'] = roc_data
        
        # æ‰“å°æŠ¥å‘Š
        print("\nåˆ†ç±»è¯„ä¼°æŠ¥å‘Š")
        print("=" * 50)
        print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
        print(f"ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
        print(f"å¬å›ç‡: {metrics['recall']:.4f}")
        print(f"F1 åˆ†æ•°: {metrics['f1']:.4f}")
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        plot_confusion_matrix(metrics['confusion_matrix'], class_names)
        
        # ç»˜åˆ¶ ROC æ›²çº¿
        if 'roc_auc' in metrics:
            plot_roc_curve(metrics['roc_auc'], num_classes)
        
        return metrics
    
    def evaluate_regression(self, dataloader):
        """å›å½’è¯„ä¼°"""
        result = self.predict(dataloader)
        
        y_true = result['targets'].flatten()
        y_pred = result['predictions'].flatten()
        
        metrics = compute_regression_metrics(y_true, y_pred)
        
        # æ‰“å°æŠ¥å‘Š
        print("\nå›å½’è¯„ä¼°æŠ¥å‘Š")
        print("=" * 50)
        print(f"MSE: {metrics['mse']:.6f}")
        print(f"RMSE: {metrics['rmse']:.6f}")
        print(f"MAE: {metrics['mae']:.6f}")
        print(f"RÂ²: {metrics['r2']:.4f}")
        print(f"MAPE: {metrics['mape']:.2f}%")
        
        # ç»˜åˆ¶ç»“æœ
        plot_regression_results(y_true, y_pred)
        
        return metrics


# ä½¿ç”¨ç¤ºä¾‹
evaluator = ModelEvaluator(model, device)

# åˆ†ç±»è¯„ä¼°
class_names = ['ç±»åˆ«0', 'ç±»åˆ«1', 'ç±»åˆ«2']
metrics = evaluator.evaluate_classification(test_loader, class_names)

# å›å½’è¯„ä¼°
# metrics = evaluator.evaluate_regression(test_loader)
```

---

## 5.4.7 é¿å…å¸¸è§é™·é˜±

### æ•°æ®æ³„éœ²

```python
# âŒ é”™è¯¯ï¼šåœ¨åˆ’åˆ†å‰è¿›è¡Œæ ‡å‡†åŒ–
scaler = StandardScaler()
X_normalized = scaler.fit_transform(X)  # ä½¿ç”¨äº†å…¨éƒ¨æ•°æ®çš„ä¿¡æ¯
X_train, X_test = train_test_split(X_normalized)

# âœ“ æ­£ç¡®ï¼šåªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆæ ‡å‡†åŒ–
X_train, X_test = train_test_split(X)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)  # åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ
X_test = scaler.transform(X_test)        # ç”¨è®­ç»ƒé›†çš„å‚æ•°å˜æ¢æµ‹è¯•é›†
```

### è¿‡åº¦è°ƒä¼˜

```python
# âŒ é”™è¯¯ï¼šåå¤åœ¨æµ‹è¯•é›†ä¸Šè°ƒæ•´æ¨¡å‹
for _ in range(100):
    train(model)
    test_acc = evaluate(model, test_loader)  # æµ‹è¯•é›†ä¿¡æ¯æ³„éœ²ï¼
    if test_acc > best:
        adjust_hyperparameters()

# âœ“ æ­£ç¡®ï¼šç”¨éªŒè¯é›†è°ƒä¼˜ï¼Œæµ‹è¯•é›†åªç”¨ä¸€æ¬¡
for _ in range(100):
    train(model)
    val_acc = evaluate(model, val_loader)  # ç”¨éªŒè¯é›†
    if val_acc > best:
        adjust_hyperparameters()

# æœ€ç»ˆè¯„ä¼°
final_test_acc = evaluate(best_model, test_loader)  # æµ‹è¯•é›†åªç”¨ä¸€æ¬¡
```

### å¿˜è®° eval() æ¨¡å¼

```python
# âŒ é”™è¯¯ï¼šè¯„ä¼°æ—¶å¿˜è®°åˆ‡æ¢æ¨¡å¼
def evaluate_wrong(model, test_loader):
    # model.eval() ç¼ºå¤±ï¼
    # Dropout ä»ç„¶æ¿€æ´»ï¼ŒBatchNorm ä½¿ç”¨é”™è¯¯çš„ç»Ÿè®¡é‡
    for inputs, targets in test_loader:
        outputs = model(inputs)
        # ...

# âœ“ æ­£ç¡®
def evaluate_correct(model, test_loader):
    model.eval()  # å¿…é¡»ï¼
    with torch.no_grad():  # æ¨è
        for inputs, targets in test_loader:
            outputs = model(inputs)
            # ...
```

---

## ğŸ”¬ ç‰©ç†è§†è§’æ€»ç»“

### è¯„ä¼°çš„ç»Ÿè®¡åŠ›å­¦æ„ä¹‰

æ¨¡å‹è¯„ä¼°å¯ä»¥ç±»æ¯”äºç‰©ç†æµ‹é‡ï¼š

| è¯„ä¼°æ¦‚å¿µ | ç‰©ç†ç±»æ¯” |
|---------|---------|
| è®­ç»ƒé›† | ç”¨äºç¡®å®šç³»ç»Ÿå‚æ•°çš„æµ‹é‡ |
| éªŒè¯é›† | ç”¨äºè°ƒæ•´å®éªŒæ¡ä»¶çš„æµ‹é‡ |
| æµ‹è¯•é›† | ç‹¬ç«‹éªŒè¯å®éªŒ |
| è¿‡æ‹Ÿåˆ | è¿‡åº¦æ‹Ÿåˆå™ªå£° |
| æ³›åŒ–è¯¯å·® | ç³»ç»Ÿè¯¯å·® |

### ä¸ç¡®å®šæ€§é‡åŒ–

```python
def uncertainty_estimation(model, test_loader, n_samples=30, device='cpu'):
    """
    ä½¿ç”¨ MC Dropout ä¼°è®¡é¢„æµ‹ä¸ç¡®å®šæ€§
    
    ç±»ä¼¼äºç‰©ç†æµ‹é‡ä¸­çš„è¯¯å·®ä¼°è®¡
    """
    model.train()  # ä¿æŒ Dropout æ¿€æ´»
    
    all_predictions = []
    
    for _ in range(n_samples):
        predictions = []
        with torch.no_grad():
            for inputs, _ in test_loader:
                inputs = inputs.to(device)
                outputs = model(inputs)
                predictions.append(outputs.cpu())
        all_predictions.append(torch.cat(predictions))
    
    predictions = torch.stack(all_predictions)  # [n_samples, n_test, ...]
    
    # å‡å€¼å’Œæ ‡å‡†å·®
    mean = predictions.mean(dim=0)
    std = predictions.std(dim=0)
    
    return mean, std
```

---

## ğŸ“ ç»ƒä¹ 

1. å®ç°ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹è¯„ä¼°æµç¨‹ï¼ŒåŒ…å«åˆ†ç±»æŒ‡æ ‡å’Œå¯è§†åŒ–
2. ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹æ€§èƒ½
3. å®ç° MC Dropout ä¸ç¡®å®šæ€§ä¼°è®¡

---

## â­ï¸ ä¸‹ä¸€ç« é¢„å‘Š

æŒæ¡äº†åŸºæœ¬è®­ç»ƒæµç¨‹åï¼Œç¬¬6ç« å°†ä»‹ç»è¿›é˜¶æŠ€æœ¯ï¼ŒåŒ…æ‹¬æ­£åˆ™åŒ–ã€å­¦ä¹ ç‡è°ƒåº¦ã€åˆ†å¸ƒå¼è®­ç»ƒç­‰ã€‚

