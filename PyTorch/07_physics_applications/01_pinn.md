# 7.1 ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPINNï¼‰

## ğŸ“– æ¦‚è¿°

ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œï¼ˆPhysics-Informed Neural Networks, PINNï¼‰æ˜¯ä¸€ç§å°†ç‰©ç†å®šå¾‹åµŒå…¥ç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼Œç”¨äºæ±‚è§£æ­£é—®é¢˜ï¼ˆç»™å®šæ–¹ç¨‹æ±‚è§£ï¼‰å’Œé€†é—®é¢˜ï¼ˆä»æ•°æ®æ¨æ–­å‚æ•°ï¼‰ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£ PINN çš„æ ¸å¿ƒæ€æƒ³
- æŒæ¡ PINN æ±‚è§£ ODE å’Œ PDE çš„æ–¹æ³•
- å®ç°è¾¹ç•Œæ¡ä»¶å’Œåˆå§‹æ¡ä»¶çš„å¤„ç†
- åº”ç”¨ PINN è§£å†³å®é™…ç‰©ç†é—®é¢˜

---

## 7.1.1 PINN åŸºæœ¬åŸç†

### æ ¸å¿ƒæ€æƒ³

PINN åˆ©ç”¨ç¥ç»ç½‘ç»œä½œä¸ºå¾®åˆ†æ–¹ç¨‹è§£çš„è¿‘ä¼¼ï¼Œå¹¶é€šè¿‡**è‡ªåŠ¨å¾®åˆ†**è®¡ç®—å¯¼æ•°ï¼Œå°†**PDE æ®‹å·®**ä½œä¸ºæŸå¤±å‡½æ•°çš„ä¸€éƒ¨åˆ†ã€‚

```
è¾“å…¥ (x, t) â†’ ç¥ç»ç½‘ç»œ â†’ è¾“å‡º u(x,t)
                â†“
        è‡ªåŠ¨å¾®åˆ†è®¡ç®— âˆ‚u/âˆ‚t, âˆ‚u/âˆ‚x, âˆ‚Â²u/âˆ‚xÂ², ...
                â†“
        PDE æ®‹å·® = PDE(u, âˆ‚u/âˆ‚t, âˆ‚Â²u/âˆ‚xÂ², ...) 
                â†“
        æœ€å°åŒ–æ®‹å·® â†’ å¾—åˆ°æ»¡è¶³ PDE çš„è§£
```

### æŸå¤±å‡½æ•°ç»“æ„

$$\mathcal{L} = \mathcal{L}_{\text{PDE}} + \lambda_{\text{BC}} \mathcal{L}_{\text{BC}} + \lambda_{\text{IC}} \mathcal{L}_{\text{IC}} + \lambda_{\text{data}} \mathcal{L}_{\text{data}}$$

- $\mathcal{L}_{\text{PDE}}$ï¼šPDE æ®‹å·®ï¼ˆåœ¨å†…éƒ¨é…ç‚¹ä¸Šï¼‰
- $\mathcal{L}_{\text{BC}}$ï¼šè¾¹ç•Œæ¡ä»¶æ®‹å·®
- $\mathcal{L}_{\text{IC}}$ï¼šåˆå§‹æ¡ä»¶æ®‹å·®
- $\mathcal{L}_{\text{data}}$ï¼šè§‚æµ‹æ•°æ®æ‹Ÿåˆï¼ˆå¯é€‰ï¼‰

---

## 7.1.2 ç®€å• ODE æ±‚è§£

### ä¾‹1ï¼šä¸€é˜¶ ODE

$$\frac{dy}{dx} = -y, \quad y(0) = 1$$

è§£æè§£ï¼š$y(x) = e^{-x}$

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

class PINN_ODE(nn.Module):
    """æ±‚è§£ä¸€é˜¶ ODE çš„ PINN"""
    
    def __init__(self, hidden_dim=32):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(1, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, x):
        return self.net(x)


def solve_first_order_ode():
    """æ±‚è§£ dy/dx = -y, y(0) = 1"""
    
    model = PINN_ODE()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    
    # å†…éƒ¨é…ç‚¹
    x_interior = torch.linspace(0, 5, 100, requires_grad=True).reshape(-1, 1)
    
    # è¾¹ç•Œç‚¹
    x_bc = torch.zeros(1, 1)
    y_bc = torch.ones(1, 1)  # y(0) = 1
    
    for epoch in range(3000):
        optimizer.zero_grad()
        
        # ODE æ®‹å·®
        y = model(x_interior)
        dy_dx = torch.autograd.grad(
            y, x_interior, 
            grad_outputs=torch.ones_like(y),
            create_graph=True
        )[0]
        
        residual = dy_dx + y  # dy/dx + y = 0
        loss_ode = (residual ** 2).mean()
        
        # è¾¹ç•Œæ¡ä»¶
        y_pred_bc = model(x_bc)
        loss_bc = (y_pred_bc - y_bc) ** 2
        
        # æ€»æŸå¤±
        loss = loss_ode + 10 * loss_bc
        
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 500 == 0:
            print(f"Epoch {epoch+1}: Loss = {loss.item():.6f}")
    
    # éªŒè¯
    x_test = torch.linspace(0, 5, 100).reshape(-1, 1)
    with torch.no_grad():
        y_pred = model(x_test)
    y_exact = torch.exp(-x_test)
    
    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    plt.plot(x_test.numpy(), y_pred.numpy(), 'r-', label='PINN', linewidth=2)
    plt.plot(x_test.numpy(), y_exact.numpy(), 'b--', label='è§£æè§£', linewidth=2)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.title('ODE è§£')
    
    plt.subplot(1, 2, 2)
    error = (y_pred - y_exact).abs()
    plt.plot(x_test.numpy(), error.numpy())
    plt.xlabel('x')
    plt.ylabel('|è¯¯å·®|')
    plt.title('ç»å¯¹è¯¯å·®')
    
    plt.tight_layout()
    plt.show()

solve_first_order_ode()
```

### ä¾‹2ï¼šäºŒé˜¶ ODEï¼ˆç®€è°æŒ¯å­ï¼‰

$$\frac{d^2 y}{dx^2} + \omega^2 y = 0, \quad y(0) = 1, \quad y'(0) = 0$$

è§£æè§£ï¼š$y(x) = \cos(\omega x)$

```python
class PINN_SHO(nn.Module):
    """ç®€è°æŒ¯å­ PINN"""
    
    def __init__(self, hidden_dim=64):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(1, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, x):
        return self.net(x)


def solve_harmonic_oscillator(omega=2.0):
    """æ±‚è§£ y'' + Ï‰Â²y = 0"""
    
    model = PINN_SHO()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # å†…éƒ¨é…ç‚¹
    x_interior = torch.linspace(0.01, 2*torch.pi, 200, requires_grad=True).reshape(-1, 1)
    
    # åˆå§‹æ¡ä»¶ç‚¹
    x_ic = torch.zeros(1, 1, requires_grad=True)
    
    for epoch in range(5000):
        optimizer.zero_grad()
        
        # ODE æ®‹å·®
        y = model(x_interior)
        dy_dx = torch.autograd.grad(
            y, x_interior,
            grad_outputs=torch.ones_like(y),
            create_graph=True
        )[0]
        d2y_dx2 = torch.autograd.grad(
            dy_dx, x_interior,
            grad_outputs=torch.ones_like(dy_dx),
            create_graph=True
        )[0]
        
        residual = d2y_dx2 + omega**2 * y
        loss_ode = (residual ** 2).mean()
        
        # åˆå§‹æ¡ä»¶ï¼šy(0) = 1
        y_ic = model(x_ic)
        loss_ic1 = (y_ic - 1.0) ** 2
        
        # åˆå§‹æ¡ä»¶ï¼šy'(0) = 0
        dy_ic = torch.autograd.grad(
            y_ic, x_ic,
            grad_outputs=torch.ones_like(y_ic),
            create_graph=True
        )[0]
        loss_ic2 = dy_ic ** 2
        
        # æ€»æŸå¤±
        loss = loss_ode + 100 * (loss_ic1 + loss_ic2)
        
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 1000 == 0:
            print(f"Epoch {epoch+1}: Loss = {loss.item():.6f}")
    
    return model
```

---

## 7.1.3 åå¾®åˆ†æ–¹ç¨‹æ±‚è§£

### çƒ­ä¼ å¯¼æ–¹ç¨‹

$$\frac{\partial u}{\partial t} = \alpha \frac{\partial^2 u}{\partial x^2}$$

åˆå§‹æ¡ä»¶ï¼š$u(x, 0) = \sin(\pi x)$
è¾¹ç•Œæ¡ä»¶ï¼š$u(0, t) = u(1, t) = 0$

```python
class PINN_Heat(nn.Module):
    """çƒ­ä¼ å¯¼æ–¹ç¨‹ PINN"""
    
    def __init__(self, hidden_dim=64, num_layers=4):
        super().__init__()
        
        layers = [nn.Linear(2, hidden_dim), nn.Tanh()]
        for _ in range(num_layers - 1):
            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])
        layers.append(nn.Linear(hidden_dim, 1))
        
        self.net = nn.Sequential(*layers)
    
    def forward(self, x, t):
        inputs = torch.cat([x, t], dim=1)
        return self.net(inputs)


def solve_heat_equation(alpha=0.1):
    """
    æ±‚è§£çƒ­ä¼ å¯¼æ–¹ç¨‹
    
    âˆ‚u/âˆ‚t = Î± âˆ‚Â²u/âˆ‚xÂ²
    """
    
    model = PINN_Heat()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # é…ç‚¹
    n_interior = 2000
    n_bc = 100
    n_ic = 100
    
    for epoch in range(10000):
        optimizer.zero_grad()
        
        # å†…éƒ¨é…ç‚¹ï¼ˆéšæœºé‡‡æ ·ï¼‰
        x_int = torch.rand(n_interior, 1, requires_grad=True)
        t_int = torch.rand(n_interior, 1, requires_grad=True) * 0.5
        
        u = model(x_int, t_int)
        
        # è®¡ç®—åå¯¼æ•°
        u_t = torch.autograd.grad(
            u, t_int, grad_outputs=torch.ones_like(u),
            create_graph=True
        )[0]
        
        u_x = torch.autograd.grad(
            u, x_int, grad_outputs=torch.ones_like(u),
            create_graph=True
        )[0]
        
        u_xx = torch.autograd.grad(
            u_x, x_int, grad_outputs=torch.ones_like(u_x),
            create_graph=True
        )[0]
        
        # PDE æ®‹å·®
        residual = u_t - alpha * u_xx
        loss_pde = (residual ** 2).mean()
        
        # åˆå§‹æ¡ä»¶ï¼šu(x, 0) = sin(Ï€x)
        x_ic = torch.rand(n_ic, 1)
        t_ic = torch.zeros(n_ic, 1)
        u_ic_pred = model(x_ic, t_ic)
        u_ic_true = torch.sin(torch.pi * x_ic)
        loss_ic = ((u_ic_pred - u_ic_true) ** 2).mean()
        
        # è¾¹ç•Œæ¡ä»¶ï¼šu(0,t) = u(1,t) = 0
        t_bc = torch.rand(n_bc, 1) * 0.5
        
        x_bc_left = torch.zeros(n_bc, 1)
        u_bc_left = model(x_bc_left, t_bc)
        
        x_bc_right = torch.ones(n_bc, 1)
        u_bc_right = model(x_bc_right, t_bc)
        
        loss_bc = (u_bc_left ** 2).mean() + (u_bc_right ** 2).mean()
        
        # æ€»æŸå¤±
        loss = loss_pde + 10 * loss_ic + 10 * loss_bc
        
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 2000 == 0:
            print(f"Epoch {epoch+1}: PDE={loss_pde.item():.6f}, "
                  f"IC={loss_ic.item():.6f}, BC={loss_bc.item():.6f}")
    
    return model
```

### æ³¢åŠ¨æ–¹ç¨‹

$$\frac{\partial^2 u}{\partial t^2} = c^2 \frac{\partial^2 u}{\partial x^2}$$

```python
class PINN_Wave(nn.Module):
    """æ³¢åŠ¨æ–¹ç¨‹ PINN"""
    
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(2, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, 1)
        )
    
    def forward(self, x, t):
        inputs = torch.cat([x, t], dim=1)
        return self.net(inputs)


def solve_wave_equation(c=1.0):
    """
    æ±‚è§£æ³¢åŠ¨æ–¹ç¨‹
    
    âˆ‚Â²u/âˆ‚tÂ² = cÂ² âˆ‚Â²u/âˆ‚xÂ²
    åˆå§‹æ¡ä»¶ï¼šu(x,0) = sin(Ï€x), âˆ‚u/âˆ‚t(x,0) = 0
    è¾¹ç•Œæ¡ä»¶ï¼šu(0,t) = u(1,t) = 0
    """
    
    model = PINN_Wave()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    for epoch in range(15000):
        optimizer.zero_grad()
        
        # å†…éƒ¨é…ç‚¹
        x_int = torch.rand(1000, 1, requires_grad=True)
        t_int = torch.rand(1000, 1, requires_grad=True) * 2.0
        
        u = model(x_int, t_int)
        
        # è®¡ç®—äºŒé˜¶åå¯¼
        u_t = torch.autograd.grad(u, t_int, torch.ones_like(u), create_graph=True)[0]
        u_tt = torch.autograd.grad(u_t, t_int, torch.ones_like(u_t), create_graph=True)[0]
        
        u_x = torch.autograd.grad(u, x_int, torch.ones_like(u), create_graph=True)[0]
        u_xx = torch.autograd.grad(u_x, x_int, torch.ones_like(u_x), create_graph=True)[0]
        
        # PDE æ®‹å·®
        residual = u_tt - c**2 * u_xx
        loss_pde = (residual ** 2).mean()
        
        # åˆå§‹æ¡ä»¶
        x_ic = torch.rand(100, 1, requires_grad=True)
        t_ic = torch.zeros(100, 1, requires_grad=True)
        
        u_ic = model(x_ic, t_ic)
        u_ic_true = torch.sin(torch.pi * x_ic)
        loss_ic1 = ((u_ic - u_ic_true) ** 2).mean()
        
        # âˆ‚u/âˆ‚t(x,0) = 0
        u_t_ic = torch.autograd.grad(u_ic, t_ic, torch.ones_like(u_ic), create_graph=True)[0]
        loss_ic2 = (u_t_ic ** 2).mean()
        
        # è¾¹ç•Œæ¡ä»¶
        t_bc = torch.rand(100, 1) * 2.0
        u_left = model(torch.zeros(100, 1), t_bc)
        u_right = model(torch.ones(100, 1), t_bc)
        loss_bc = (u_left ** 2).mean() + (u_right ** 2).mean()
        
        loss = loss_pde + 10 * (loss_ic1 + loss_ic2) + 10 * loss_bc
        
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 3000 == 0:
            print(f"Epoch {epoch+1}: Loss = {loss.item():.6f}")
    
    return model
```

---

## 7.1.4 ç¡¬çº¦æŸè¾¹ç•Œæ¡ä»¶

### æ„é€ è‡ªåŠ¨æ»¡è¶³è¾¹ç•Œæ¡ä»¶çš„è§£

é€šè¿‡å·§å¦™çš„ç½‘ç»œè¾“å‡ºå˜æ¢ï¼Œå¯ä»¥è®©è§£**è‡ªåŠ¨æ»¡è¶³è¾¹ç•Œæ¡ä»¶**ã€‚

```python
class PINN_HardBC(nn.Module):
    """
    ç¡¬çº¦æŸè¾¹ç•Œæ¡ä»¶çš„ PINN
    
    å¯¹äº u(0) = a, u(1) = b çš„ Dirichlet è¾¹ç•Œæ¡ä»¶ï¼Œ
    æ„é€ ï¼šu(x) = a(1-x) + bx + x(1-x)Â·NN(x)
    """
    
    def __init__(self, a=0, b=0):
        super().__init__()
        self.a = a
        self.b = b
        self.net = nn.Sequential(
            nn.Linear(1, 32),
            nn.Tanh(),
            nn.Linear(32, 32),
            nn.Tanh(),
            nn.Linear(32, 1)
        )
    
    def forward(self, x):
        # åŸºç¡€å‡½æ•°æ»¡è¶³è¾¹ç•Œæ¡ä»¶
        base = self.a * (1 - x) + self.b * x
        # ä¿®æ­£é¡¹åœ¨è¾¹ç•Œä¸ºé›¶
        correction = x * (1 - x) * self.net(x)
        return base + correction


class PINN_TimeDependent_HardBC(nn.Module):
    """
    æ—¶é—´ç›¸å…³é—®é¢˜çš„ç¡¬çº¦æŸ
    
    æ»¡è¶³ï¼š
    - u(x, 0) = f(x) (åˆå§‹æ¡ä»¶)
    - u(0, t) = g0(t), u(1, t) = g1(t) (è¾¹ç•Œæ¡ä»¶)
    """
    
    def __init__(self, f_init, g0_bc, g1_bc):
        super().__init__()
        self.f_init = f_init    # åˆå§‹æ¡ä»¶å‡½æ•°
        self.g0_bc = g0_bc      # å·¦è¾¹ç•Œå‡½æ•°
        self.g1_bc = g1_bc      # å³è¾¹ç•Œå‡½æ•°
        
        self.net = nn.Sequential(
            nn.Linear(2, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, 1)
        )
    
    def forward(self, x, t):
        inputs = torch.cat([x, t], dim=1)
        nn_out = self.net(inputs)
        
        # æ»¡è¶³åˆå§‹æ¡ä»¶çš„é¡¹
        u_init = self.f_init(x)
        
        # æ»¡è¶³è¾¹ç•Œæ¡ä»¶çš„æ’å€¼
        u_bc = (1 - x) * self.g0_bc(t) + x * self.g1_bc(t)
        
        # ç»„åˆï¼ˆt=0 æ—¶ç­‰äºåˆå§‹æ¡ä»¶ï¼Œx=0,1 æ—¶ç­‰äºè¾¹ç•Œæ¡ä»¶ï¼‰
        # ä¿®æ­£é¡¹åœ¨ t=0 å’Œ x=0,1 å¤„ä¸ºé›¶
        correction = t * x * (1 - x) * nn_out
        
        # éœ€è¦æ›´å¤æ‚çš„æ„é€ æ¥åŒæ—¶æ»¡è¶³ IC å’Œ BC
        # è¿™é‡Œç®€åŒ–å¤„ç†
        return u_init * torch.exp(-t) + (1 - torch.exp(-t)) * u_bc + correction
```

---

## 7.1.5 é€†é—®é¢˜ï¼šå‚æ•°ä¼°è®¡

### ä»æ•°æ®æ¨æ–­æœªçŸ¥å‚æ•°

```python
class PINN_Inverse(nn.Module):
    """
    é€†é—®é¢˜ PINNï¼šä»æ•°æ®æ¨æ–­æœªçŸ¥å‚æ•°
    
    ä¾‹å¦‚ï¼šä»çƒ­ä¼ å¯¼æ•°æ®æ¨æ–­çƒ­æ‰©æ•£ç³»æ•° Î±
    """
    
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(2, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, 1)
        )
        # æœªçŸ¥å‚æ•°ï¼ˆå¯å­¦ä¹ ï¼‰
        self.alpha = nn.Parameter(torch.tensor(0.5))
    
    def forward(self, x, t):
        inputs = torch.cat([x, t], dim=1)
        return self.net(inputs)


def solve_inverse_problem():
    """
    ä»è§‚æµ‹æ•°æ®æ¨æ–­çƒ­æ‰©æ•£ç³»æ•°
    
    çœŸå® Î± = 0.1
    """
    alpha_true = 0.1
    
    # ç”Ÿæˆ"è§‚æµ‹"æ•°æ®ï¼ˆè§£æè§£ï¼‰
    def analytical_solution(x, t, alpha):
        return torch.sin(torch.pi * x) * torch.exp(-alpha * torch.pi**2 * t)
    
    # è§‚æµ‹ç‚¹
    n_obs = 50
    x_obs = torch.rand(n_obs, 1)
    t_obs = torch.rand(n_obs, 1) * 0.5
    u_obs = analytical_solution(x_obs, t_obs, alpha_true)
    u_obs = u_obs + 0.01 * torch.randn_like(u_obs)  # æ·»åŠ å™ªå£°
    
    model = PINN_Inverse()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    alpha_history = []
    
    for epoch in range(10000):
        optimizer.zero_grad()
        
        # æ•°æ®æ‹ŸåˆæŸå¤±
        u_pred_obs = model(x_obs, t_obs)
        loss_data = ((u_pred_obs - u_obs) ** 2).mean()
        
        # PDE æ®‹å·®
        x_int = torch.rand(500, 1, requires_grad=True)
        t_int = torch.rand(500, 1, requires_grad=True) * 0.5
        
        u = model(x_int, t_int)
        u_t = torch.autograd.grad(u, t_int, torch.ones_like(u), create_graph=True)[0]
        u_x = torch.autograd.grad(u, x_int, torch.ones_like(u), create_graph=True)[0]
        u_xx = torch.autograd.grad(u_x, x_int, torch.ones_like(u_x), create_graph=True)[0]
        
        residual = u_t - model.alpha * u_xx
        loss_pde = (residual ** 2).mean()
        
        loss = loss_data + 0.1 * loss_pde
        
        loss.backward()
        optimizer.step()
        
        # ç¡®ä¿ Î± > 0
        with torch.no_grad():
            model.alpha.clamp_(min=0.001)
        
        alpha_history.append(model.alpha.item())
        
        if (epoch + 1) % 2000 == 0:
            print(f"Epoch {epoch+1}: Î± = {model.alpha.item():.4f} "
                  f"(çœŸå®å€¼: {alpha_true})")
    
    print(f"\næœ€ç»ˆä¼°è®¡: Î± = {model.alpha.item():.4f}")
    print(f"ç›¸å¯¹è¯¯å·®: {abs(model.alpha.item() - alpha_true) / alpha_true * 100:.2f}%")
    
    return model, alpha_history
```

---

## 7.1.6 Burgers æ–¹ç¨‹

éçº¿æ€§ PDE çš„ç»å…¸ä¾‹å­ï¼š

$$\frac{\partial u}{\partial t} + u \frac{\partial u}{\partial x} = \nu \frac{\partial^2 u}{\partial x^2}$$

```python
class PINN_Burgers(nn.Module):
    """Burgers æ–¹ç¨‹ PINN"""
    
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(2, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, 1)
        )
    
    def forward(self, x, t):
        inputs = torch.cat([x, t], dim=1)
        return self.net(inputs)


def solve_burgers_equation(nu=0.01):
    """
    æ±‚è§£ Burgers æ–¹ç¨‹
    
    âˆ‚u/âˆ‚t + uÂ·âˆ‚u/âˆ‚x = Î½Â·âˆ‚Â²u/âˆ‚xÂ²
    
    åˆå§‹æ¡ä»¶ï¼šu(x,0) = -sin(Ï€x)
    è¾¹ç•Œæ¡ä»¶ï¼šu(-1,t) = u(1,t) = 0
    """
    
    model = PINN_Burgers()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    for epoch in range(20000):
        optimizer.zero_grad()
        
        # å†…éƒ¨é…ç‚¹
        x_int = torch.rand(2000, 1, requires_grad=True) * 2 - 1  # [-1, 1]
        t_int = torch.rand(2000, 1, requires_grad=True)
        
        u = model(x_int, t_int)
        
        u_t = torch.autograd.grad(u, t_int, torch.ones_like(u), create_graph=True)[0]
        u_x = torch.autograd.grad(u, x_int, torch.ones_like(u), create_graph=True)[0]
        u_xx = torch.autograd.grad(u_x, x_int, torch.ones_like(u_x), create_graph=True)[0]
        
        # Burgers æ–¹ç¨‹æ®‹å·®
        residual = u_t + u * u_x - nu * u_xx
        loss_pde = (residual ** 2).mean()
        
        # åˆå§‹æ¡ä»¶
        x_ic = torch.rand(200, 1) * 2 - 1
        t_ic = torch.zeros(200, 1)
        u_ic = model(x_ic, t_ic)
        u_ic_true = -torch.sin(torch.pi * x_ic)
        loss_ic = ((u_ic - u_ic_true) ** 2).mean()
        
        # è¾¹ç•Œæ¡ä»¶
        t_bc = torch.rand(100, 1)
        u_left = model(-torch.ones(100, 1), t_bc)
        u_right = model(torch.ones(100, 1), t_bc)
        loss_bc = (u_left ** 2).mean() + (u_right ** 2).mean()
        
        loss = loss_pde + 10 * loss_ic + 10 * loss_bc
        
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 4000 == 0:
            print(f"Epoch {epoch+1}: Loss = {loss.item():.6f}")
    
    return model
```

---

## 7.1.7 Navier-Stokes æ–¹ç¨‹

æµä½“åŠ›å­¦çš„åŸºæœ¬æ–¹ç¨‹ï¼š

```python
class PINN_NavierStokes(nn.Module):
    """
    2D ä¸å¯å‹ç¼© Navier-Stokes æ–¹ç¨‹
    
    âˆ‚u/âˆ‚t + uÂ·âˆ‡u = -âˆ‡p/Ï + Î½âˆ‡Â²u
    âˆ‡Â·u = 0
    """
    
    def __init__(self):
        super().__init__()
        # å…±äº«çš„ç‰¹å¾æå–å™¨
        self.shared = nn.Sequential(
            nn.Linear(3, 64),  # (x, y, t)
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh()
        )
        # åˆ†åˆ«è¾“å‡º u, v, p
        self.u_head = nn.Sequential(nn.Linear(64, 32), nn.Tanh(), nn.Linear(32, 1))
        self.v_head = nn.Sequential(nn.Linear(64, 32), nn.Tanh(), nn.Linear(32, 1))
        self.p_head = nn.Sequential(nn.Linear(64, 32), nn.Tanh(), nn.Linear(32, 1))
    
    def forward(self, x, y, t):
        inputs = torch.cat([x, y, t], dim=1)
        features = self.shared(inputs)
        
        u = self.u_head(features)
        v = self.v_head(features)
        p = self.p_head(features)
        
        return u, v, p


def navier_stokes_residual(model, x, y, t, nu=0.01, rho=1.0):
    """è®¡ç®— Navier-Stokes æ®‹å·®"""
    
    u, v, p = model(x, y, t)
    
    # ä¸€é˜¶å¯¼æ•°
    u_t = torch.autograd.grad(u, t, torch.ones_like(u), create_graph=True)[0]
    u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]
    u_y = torch.autograd.grad(u, y, torch.ones_like(u), create_graph=True)[0]
    
    v_t = torch.autograd.grad(v, t, torch.ones_like(v), create_graph=True)[0]
    v_x = torch.autograd.grad(v, x, torch.ones_like(v), create_graph=True)[0]
    v_y = torch.autograd.grad(v, y, torch.ones_like(v), create_graph=True)[0]
    
    p_x = torch.autograd.grad(p, x, torch.ones_like(p), create_graph=True)[0]
    p_y = torch.autograd.grad(p, y, torch.ones_like(p), create_graph=True)[0]
    
    # äºŒé˜¶å¯¼æ•°
    u_xx = torch.autograd.grad(u_x, x, torch.ones_like(u_x), create_graph=True)[0]
    u_yy = torch.autograd.grad(u_y, y, torch.ones_like(u_y), create_graph=True)[0]
    v_xx = torch.autograd.grad(v_x, x, torch.ones_like(v_x), create_graph=True)[0]
    v_yy = torch.autograd.grad(v_y, y, torch.ones_like(v_y), create_graph=True)[0]
    
    # åŠ¨é‡æ–¹ç¨‹æ®‹å·®
    res_u = u_t + u * u_x + v * u_y + p_x / rho - nu * (u_xx + u_yy)
    res_v = v_t + u * v_x + v * v_y + p_y / rho - nu * (v_xx + v_yy)
    
    # è¿ç»­æ€§æ–¹ç¨‹æ®‹å·®
    res_cont = u_x + v_y
    
    return res_u, res_v, res_cont
```

---

## 7.1.8 è®­ç»ƒæŠ€å·§

### è‡ªé€‚åº”æƒé‡

```python
class AdaptiveLossWeights:
    """è‡ªé€‚åº”æŸå¤±æƒé‡"""
    
    def __init__(self, n_losses, learning_rate=0.01):
        self.log_weights = torch.zeros(n_losses, requires_grad=True)
        self.optimizer = torch.optim.Adam([self.log_weights], lr=learning_rate)
    
    def get_weights(self):
        return torch.exp(-self.log_weights)
    
    def update(self, losses):
        """æ ¹æ®æŸå¤±æ›´æ–°æƒé‡"""
        self.optimizer.zero_grad()
        
        # æŸå¤±ï¼š-log(w) + w * L
        total = 0
        for log_w, loss in zip(self.log_weights, losses):
            total += -log_w + torch.exp(-log_w) * loss.detach()
        
        total.backward()
        self.optimizer.step()
```

### é‡‡æ ·ç­–ç•¥

```python
def residual_based_sampling(model, x_range, t_range, n_samples, 
                             pde_residual_fn):
    """
    åŸºäºæ®‹å·®çš„è‡ªé€‚åº”é‡‡æ ·
    
    åœ¨æ®‹å·®å¤§çš„åŒºåŸŸé‡‡æ ·æ›´å¤šç‚¹
    """
    # ç²—é‡‡æ ·è¯„ä¼°æ®‹å·®
    n_coarse = 1000
    x_coarse = torch.rand(n_coarse, 1) * (x_range[1] - x_range[0]) + x_range[0]
    t_coarse = torch.rand(n_coarse, 1) * (t_range[1] - t_range[0]) + t_range[0]
    x_coarse.requires_grad = True
    t_coarse.requires_grad = True
    
    with torch.no_grad():
        residual = pde_residual_fn(model, x_coarse, t_coarse)
        weights = residual.abs().squeeze()
        weights = weights / weights.sum()
    
    # æ ¹æ®æƒé‡é‡‡æ ·
    indices = torch.multinomial(weights, n_samples, replacement=True)
    
    x_refined = x_coarse[indices].clone().detach().requires_grad_(True)
    t_refined = t_coarse[indices].clone().detach().requires_grad_(True)
    
    return x_refined, t_refined
```

---

## ğŸ”¬ ç‰©ç†è§†è§’æ€»ç»“

### PINN çš„ä¼˜åŠ¿

| æ–¹é¢ | ä¼ ç»Ÿæ•°å€¼æ–¹æ³• | PINN |
|------|-------------|------|
| ç½‘æ ¼ | éœ€è¦ç¦»æ•£åŒ– | æ— ç½‘æ ¼ |
| é«˜ç»´é—®é¢˜ | ç»´åº¦ç¾éš¾ | ç›¸å¯¹å®¹æ˜“ |
| é€†é—®é¢˜ | éœ€è¦ç‰¹æ®Šå¤„ç† | è‡ªç„¶èåˆ |
| å™ªå£°æ•°æ® | æ•æ„Ÿ | æœ‰æ­£åˆ™åŒ–æ•ˆæœ |

### å±€é™æ€§

- è®­ç»ƒå¯èƒ½å›°éš¾ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜
- å¯¹äºå¼ºéçº¿æ€§é—®é¢˜ç²¾åº¦å¯èƒ½ä¸è¶³
- è¶…å‚æ•°ï¼ˆæƒé‡ï¼‰è°ƒèŠ‚éœ€è¦ç»éªŒ

---

## ğŸ“ ç»ƒä¹ 

1. ç”¨ PINN æ±‚è§£æ³Šæ¾æ–¹ç¨‹ $\nabla^2 u = f(x,y)$
2. å®ç°ä¸€ä¸ª PINN æ±‚è§£è–›å®šè°”æ–¹ç¨‹çš„æ—¶é—´æ¼”åŒ–
3. å°è¯•ä»å«å™ªå£°æ•°æ®ä¸­æ¢å¤ PDE çš„æœªçŸ¥å‚æ•°

---

## â­ï¸ ä¸‹ä¸€èŠ‚

ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†å­¦ä¹  [åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿ](./02_molecular_dynamics.md)ï¼Œäº†è§£å¦‚ä½•ç”¨ç¥ç»ç½‘ç»œæ„å»ºåŠ¿å‡½æ•°ã€‚

